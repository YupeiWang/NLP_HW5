{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from pprint import pprint\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 uninstall tensorflow-gpu==2.0.0-beta0\n",
    "!pip3 uninstall -y tensorflow\n",
    "!pip3 uninstall keras\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorboard~=2.6->tensorflow) (2.24.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorboard~=2.6->tensorflow) (3.3.4)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorboard~=2.6->tensorflow) (1.32.1)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorboard~=2.6->tensorflow) (1.0.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorboard~=2.6->tensorflow) (1.8.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorboard~=2.6->tensorflow) (0.4.4)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorboard~=2.6->tensorflow) (50.3.1.post20201107)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (2020.6.20)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (1.25.11)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow) (4.2.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in c:\\users\\user\\anaconda3\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow) (4.7.2)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow) (1.3.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\user\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow) (3.1.1)\n",
      "Installing collected packages: keras, clang, grpcio, tensorboard, tensorflow-estimator, tensorflow\n",
      "  Attempting uninstall: grpcio\n",
      "    Found existing installation: grpcio 1.34.1\n",
      "    Uninstalling grpcio-1.34.1:\n",
      "      Successfully uninstalled grpcio-1.34.1\n",
      "  Attempting uninstall: tensorboard\n",
      "    Found existing installation: tensorboard 2.5.0\n",
      "    Uninstalling tensorboard-2.5.0:\n",
      "      Successfully uninstalled tensorboard-2.5.0\n",
      "  Attempting uninstall: tensorflow-estimator\n",
      "    Found existing installation: tensorflow-estimator 2.5.0\n",
      "    Uninstalling tensorflow-estimator-2.5.0:\n",
      "      Successfully uninstalled tensorflow-estimator-2.5.0\n",
      "Successfully installed clang-5.0 grpcio-1.39.0 keras-2.6.0 tensorboard-2.6.0 tensorflow-2.6.0 tensorflow-estimator-2.6.0\n"
     ]
    }
   ],
   "source": [
    "!pip3 uninstall tensorflow-gpu==2.0.0-beta0\n",
    "!pip3 install tensorflow-gpu==2.0.0-beta0\n",
    "!pip3 uninstall tensorflow\n",
    "!pip3 install tensorflow\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requirement already satisfied: tensorflow-metadata in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-datasets) (1.2.0)\n",
      "Requirement already satisfied: protobuf>=3.12.2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-datasets) (3.17.3)\n",
      "Requirement already satisfied: six in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-datasets) (1.15.0)\n",
      "Requirement already satisfied: termcolor in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-datasets) (1.1.0)\n",
      "Requirement already satisfied: promise in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-datasets) (2.3)\n",
      "Requirement already satisfied: attrs>=18.1.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-datasets) (20.3.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-datasets) (2.24.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-datasets) (1.19.2)\n",
      "Requirement already satisfied: tqdm in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-datasets) (4.50.2)\n",
      "Requirement already satisfied: dill in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-datasets) (0.3.4)\n",
      "Requirement already satisfied: zipp>=3.1.0; python_version < \"3.10\" in c:\\users\\user\\anaconda3\\lib\\site-packages (from importlib-resources; python_version < \"3.9\"->tensorflow-datasets) (3.4.0)\n",
      "Requirement already satisfied: googleapis-common-protos<2,>=1.52.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-metadata->tensorflow-datasets) (1.53.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests>=2.19.0->tensorflow-datasets) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests>=2.19.0->tensorflow-datasets) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests>=2.19.0->tensorflow-datasets) (1.25.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests>=2.19.0->tensorflow-datasets) (2020.6.20)\n"
     ]
    }
   ],
   "source": [
    "!pip3 uninstall tensorflow-datasets\n",
    "!pip3 install tensorflow-datasets\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install --ignore-installed --upgrade --ignore-installed tensorflow\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0\n"
     ]
    }
   ],
   "source": [
    "!pip3 install tensorflow-gpu==2.0.0-beta0\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "clear_output()\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(level=\"ERROR\")\n",
    "\n",
    "np.set_printoptions(suppress=True) #讓 numpy 不要顯示科學記號"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"nmt\"\n",
    "en_vocab_file = os.path.join(output_dir, \"en_vocab\")\n",
    "zh_vocab_file = os.path.join(output_dir, \"zh_vocab\")\n",
    "checkpoint_path = os.path.join(output_dir, \"checkpoints\")\n",
    "log_dir = os.path.join(output_dir, 'logs')\n",
    "download_dir = \"tensorflow-datasets/downloads\"\n",
    "\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{Split('train'): ['newscommentary_v14',\n",
      "                  'wikititles_v1',\n",
      "                  'uncorpus_v1',\n",
      "                  'casia2015',\n",
      "                  'casict2011',\n",
      "                  'casict2015',\n",
      "                  'datum2015',\n",
      "                  'datum2017',\n",
      "                  'neu2017'],\n",
      " Split('validation'): ['newstest2018']}\n"
     ]
    }
   ],
   "source": [
    "tmp_builder = tfds.builder(\"wmt19_translate/zh-en\")\n",
    "pprint(tmp_builder.subsets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = tfds.translate.wmt.WmtConfig(\n",
    "    version=\"0.0.3\",\n",
    "    language_pair=(\"zh\", \"en\"),\n",
    "    subsets={\n",
    "        tfds.Split.TRAIN: [\"newscommentary_v14\"]\n",
    "    },\n",
    ")\n",
    "builder = tfds.builder(\"wmt_translate\", config=config)\n",
    "builder.download_and_prepare(download_dir=download_dir)\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<PrefetchDataset shapes: ((), ()), types: (tf.string, tf.string)>\n",
      "<PrefetchDataset shapes: ((), ()), types: (tf.string, tf.string)>\n"
     ]
    }
   ],
   "source": [
    "examples = builder.as_dataset(split=['train[:20%]','train[20%:21%]','train[21%:]'], as_supervised=True)\n",
    "\n",
    "train_examples, val_examples, _ = examples\n",
    "print(train_examples)\n",
    "print(val_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b'The fear is real and visceral, and politicians ignore it at their peril.', shape=(), dtype=string)\n",
      "tf.Tensor(b'\\xe8\\xbf\\x99\\xe7\\xa7\\x8d\\xe6\\x81\\x90\\xe6\\x83\\xa7\\xe6\\x98\\xaf\\xe7\\x9c\\x9f\\xe5\\xae\\x9e\\xe8\\x80\\x8c\\xe5\\x86\\x85\\xe5\\x9c\\xa8\\xe7\\x9a\\x84\\xe3\\x80\\x82 \\xe5\\xbf\\xbd\\xe8\\xa7\\x86\\xe5\\xae\\x83\\xe7\\x9a\\x84\\xe6\\x94\\xbf\\xe6\\xb2\\xbb\\xe5\\xae\\xb6\\xe4\\xbb\\xac\\xe5\\x89\\x8d\\xe9\\x80\\x94\\xe5\\xa0\\xaa\\xe5\\xbf\\xa7\\xe3\\x80\\x82', shape=(), dtype=string)\n",
      "----------\n",
      "tf.Tensor(b'In fact, the German political landscape needs nothing more than a truly liberal party, in the US sense of the word \\xe2\\x80\\x9cliberal\\xe2\\x80\\x9d \\xe2\\x80\\x93 a champion of the cause of individual freedom.', shape=(), dtype=string)\n",
      "tf.Tensor(b'\\xe4\\xba\\x8b\\xe5\\xae\\x9e\\xe4\\xb8\\x8a\\xef\\xbc\\x8c\\xe5\\xbe\\xb7\\xe5\\x9b\\xbd\\xe6\\x94\\xbf\\xe6\\xb2\\xbb\\xe5\\xb1\\x80\\xe5\\x8a\\xbf\\xe9\\x9c\\x80\\xe8\\xa6\\x81\\xe7\\x9a\\x84\\xe4\\xb8\\x8d\\xe8\\xbf\\x87\\xe6\\x98\\xaf\\xe4\\xb8\\x80\\xe4\\xb8\\xaa\\xe7\\xac\\xa6\\xe5\\x90\\x88\\xe7\\xbe\\x8e\\xe5\\x9b\\xbd\\xe6\\x89\\x80\\xe8\\xb0\\x93\\xe2\\x80\\x9c\\xe8\\x87\\xaa\\xe7\\x94\\xb1\\xe2\\x80\\x9d\\xe5\\xae\\x9a\\xe4\\xb9\\x89\\xe7\\x9a\\x84\\xe7\\x9c\\x9f\\xe6\\xad\\xa3\\xe7\\x9a\\x84\\xe8\\x87\\xaa\\xe7\\x94\\xb1\\xe5\\x85\\x9a\\xe6\\xb4\\xbe\\xef\\xbc\\x8c\\xe4\\xb9\\x9f\\xe5\\xb0\\xb1\\xe6\\x98\\xaf\\xe4\\xb8\\xaa\\xe4\\xba\\xba\\xe8\\x87\\xaa\\xe7\\x94\\xb1\\xe4\\xba\\x8b\\xe4\\xb8\\x9a\\xe7\\x9a\\x84\\xe5\\x80\\xa1\\xe5\\xaf\\xbc\\xe8\\x80\\x85\\xe3\\x80\\x82', shape=(), dtype=string)\n",
      "----------\n",
      "tf.Tensor(b'Shifting to renewable-energy sources will require enormous effort and major infrastructure investment.', shape=(), dtype=string)\n",
      "tf.Tensor(b'\\xe5\\xbf\\x85\\xe9\\xa1\\xbb\\xe4\\xbb\\x98\\xe5\\x87\\xba\\xe5\\xb7\\xa8\\xe5\\xa4\\xa7\\xe7\\x9a\\x84\\xe5\\x8a\\xaa\\xe5\\x8a\\x9b\\xe5\\x92\\x8c\\xe5\\x9f\\xba\\xe7\\xa1\\x80\\xe8\\xae\\xbe\\xe6\\x96\\xbd\\xe6\\x8a\\x95\\xe8\\xb5\\x84\\xe6\\x89\\x8d\\xe8\\x83\\xbd\\xe5\\xae\\x8c\\xe6\\x88\\x90\\xe5\\x90\\x91\\xe5\\x8f\\xaf\\xe5\\x86\\x8d\\xe7\\x94\\x9f\\xe8\\x83\\xbd\\xe6\\xba\\x90\\xe7\\x9a\\x84\\xe8\\xbf\\x87\\xe6\\xb8\\xa1\\xe3\\x80\\x82', shape=(), dtype=string)\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "for en, zh in train_examples.take(3):\n",
    "    print(en)\n",
    "    print(zh)\n",
    "    print('-' * 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The fear is real and visceral, and politicians ignore it at their peril.\n",
      "这种恐惧是真实而内在的。 忽视它的政治家们前途堪忧。\n",
      "----------\n",
      "In fact, the German political landscape needs nothing more than a truly liberal party, in the US sense of the word “liberal” – a champion of the cause of individual freedom.\n",
      "事实上，德国政治局势需要的不过是一个符合美国所谓“自由”定义的真正的自由党派，也就是个人自由事业的倡导者。\n",
      "----------\n",
      "Shifting to renewable-energy sources will require enormous effort and major infrastructure investment.\n",
      "必须付出巨大的努力和基础设施投资才能完成向可再生能源的过渡。\n",
      "----------\n",
      "In this sense, it is critical to recognize the fundamental difference between “urban villages” and their rural counterparts.\n",
      "在这方面，关键在于认识到“城市村落”和农村村落之间的根本区别。\n",
      "----------\n",
      "A strong European voice, such as Nicolas Sarkozy’s during the French presidency of the EU, may make a difference, but only for six months, and at the cost of reinforcing other European countries’ nationalist feelings in reaction to the expression of “Gallic pride.”\n",
      "法国担任轮值主席国期间尼古拉·萨科奇统一的欧洲声音可能让人耳目一新，但这种声音却只持续了短短六个月，而且付出了让其他欧洲国家在面对“高卢人的骄傲”时民族主义情感进一步被激发的代价。\n",
      "----------\n",
      "Most of Japan’s bondholders are nationals (if not the central bank) and have an interest in political stability.\n",
      "日本债券持有人大多为本国国民（甚至中央银行 ） ， 政治稳定符合他们的利益。\n",
      "----------\n",
      "Paul Romer, one of the originators of new growth theory, has accused some leading names, including the Nobel laureate Robert Lucas, of what he calls “mathiness” – using math to obfuscate rather than clarify.\n",
      "新增长理论创始人之一的保罗·罗默（Paul Romer）也批评一些著名经济学家，包括诺贝尔奖获得者罗伯特·卢卡斯（Robert Lucas）在内，说他们“数学性 ” （ 罗默的用语）太重，结果是让问题变得更加模糊而不是更加清晰。\n",
      "----------\n",
      "It is, in fact, a capsule depiction of the United States Federal Reserve and the European Central Bank.\n",
      "事实上，这就是对美联储和欧洲央行的简略描述。\n",
      "----------\n",
      "Given these variables, the degree to which migration is affected by asylum-seekers will not be easy to predict or control.\n",
      "考虑到这些变量，移民受寻求庇护者的影响程度很难预测或控制。\n",
      "----------\n",
      "WASHINGTON, DC – In the 2016 American presidential election, Hillary Clinton and Donald Trump agreed that the US economy is suffering from dilapidated infrastructure, and both called for greater investment in renovating and upgrading the country’s public capital stock.\n",
      "华盛顿—在2016年美国总统选举中，希拉里·克林顿和唐纳德·特朗普都认为美国经济饱受基础设施陈旧的拖累，两人都要求加大投资用于修缮和升级美国公共资本存量。\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sample_examples = []\n",
    "num_samples = 10\n",
    "\n",
    "for en_t, zh_t in train_examples.take(num_samples):\n",
    "    en = en_t.numpy().decode(\"utf-8\")\n",
    "    zh = zh_t.numpy().decode(\"utf-8\")\n",
    "\n",
    "    print(en)\n",
    "    print(zh)\n",
    "    print('-' * 10)\n",
    "\n",
    "    # 之後用來簡單評估模型的訓練情況\n",
    "    sample_examples.append((en, zh))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "沒有已建立的字典，從頭建立。\n",
      "字典大小：8113\n",
      "前 10 個 subwords：[', ', 'the_', 'of_', 'to_', 'and_', 's_', 'in_', 'a_', 'is_', 'that_']\n",
      "\n",
      "Wall time: 1min 51s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "## load_from_file 函式嘗試讀取之前已經建好的字典檔案\n",
    "try:\n",
    "    subword_encoder_en = tfds.deprecated.text.SubwordTextEncoder.load_from_file(en_vocab_file)\n",
    "    print(f\"載入已建立的字典： {en_vocab_file}\")\n",
    "except:\n",
    "    print(\"沒有已建立的字典，從頭建立。\")\n",
    "    subword_encoder_en = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(\n",
    "      (en.numpy() for en, _ in train_examples), \n",
    "      target_vocab_size=2**13) # 有需要可以調整字典大小\n",
    "  \n",
    "    # 將字典檔案存下以方便下次 warmstart\n",
    "    subword_encoder_en.save_to_file(en_vocab_file)\n",
    "  \n",
    "\n",
    "print(f\"字典大小：{subword_encoder_en.vocab_size}\")\n",
    "print(f\"前 10 個 subwords：{subword_encoder_en.subwords[:10]}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3461, 7889, 9, 3502, 4379, 1134, 7903]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_string = 'Taiwan is beautiful.'\n",
    "indices = subword_encoder_en.encode(sample_string)\n",
    "indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index     Subword\n",
      "---------------\n",
      " 3461     Taiwan\n",
      " 7889      \n",
      "    9     is \n",
      " 3502     bea\n",
      " 4379     uti\n",
      " 1134     ful\n",
      " 7903     .\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"{0:10}{1:6}\".format(\"Index\", \"Subword\"))\n",
    "print(\"-\" * 15)\n",
    "for idx in indices:\n",
    "    subword = subword_encoder_en.decode([idx])\n",
    "    print('{0:5}{1:6}'.format(idx, ' ' * 5 + subword))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Taiwan is beautiful.', 'Taiwan is beautiful.')\n"
     ]
    }
   ],
   "source": [
    "sample_string = 'Taiwan is beautiful.'\n",
    "indices = subword_encoder_en.encode(sample_string)\n",
    "decoded_string = subword_encoder_en.decode(indices)\n",
    "assert decoded_string == sample_string\n",
    "pprint((sample_string, decoded_string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#為中文建立一個字典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "沒有已建立的字典，從頭建立。\n",
      "字典大小：4205\n",
      "前 10 個 subwords：['的', '，', '。', '国', '在', '是', '一', '和', '不', '这']\n",
      "\n",
      "Wall time: 11min 15s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#https://www.tensorflow.org/datasets/api_docs/python/tfds/deprecated/text/SubwordTextEncoder\n",
    "try:\n",
    "    subword_encoder_zh = tfds.deprecated.text.SubwordTextEncoder.load_from_file(zh_vocab_file)\n",
    "    print(f\"載入已建立的字典： {zh_vocab_file}\")\n",
    "except:\n",
    "    print(\"沒有已建立的字典，從頭建立。\")\n",
    "    subword_encoder_zh = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(\n",
    "      (zh.numpy() for _, zh in train_examples), \n",
    "      target_vocab_size=2**13, # 有需要可以調整字典大小\n",
    "      max_subword_length=1) # 每一個中文字就是字典裡的一個單位\n",
    "\n",
    "    # 將字典檔案存下以方便下次 warmstart \n",
    "    subword_encoder_zh.save_to_file(zh_vocab_file)\n",
    "\n",
    "print(f\"字典大小：{subword_encoder_zh.vocab_size}\")\n",
    "print(f\"前 10 個 subwords：{subword_encoder_zh.subwords[:10]}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "这种恐惧是真实而内在的。 忽视它的政治家们前途堪忧。\n",
      "[10, 151, 574, 1298, 6, 374, 55, 29, 193, 5, 1, 3, 3981, 931, 431, 125, 1, 17, 124, 33, 20, 97, 1089, 1247, 861, 3]\n"
     ]
    }
   ],
   "source": [
    "sample_string = sample_examples[0][1]\n",
    "indices = subword_encoder_zh.encode(sample_string)\n",
    "print(sample_string)\n",
    "print(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[英中原文]（轉換前）\n",
      "The eurozone’s collapse forces a major realignment of European politics.\n",
      "欧元区的瓦解强迫欧洲政治进行一次重大改组。\n",
      "\n",
      "--------------------\n",
      "\n",
      "[英中序列]（轉換後）\n",
      "[16, 900, 11, 6, 1527, 874, 8, 230, 2259, 2728, 239, 3, 89, 1236, 7903]\n",
      "[44, 202, 168, 1, 852, 201, 231, 592, 44, 87, 17, 124, 106, 38, 7, 279, 86, 18, 212, 265, 3]\n"
     ]
    }
   ],
   "source": [
    "en = \"The eurozone’s collapse forces a major realignment of European politics.\"\n",
    "zh = \"欧元区的瓦解强迫欧洲政治进行一次重大改组。\"\n",
    "\n",
    "# 將文字轉成為 subword indices\n",
    "en_indices = subword_encoder_en.encode(en)\n",
    "zh_indices = subword_encoder_zh.encode(zh)\n",
    "\n",
    "print(\"[英中原文]（轉換前）\")\n",
    "print(en)\n",
    "print(zh)\n",
    "print()\n",
    "print('-' * 20)\n",
    "print()\n",
    "print(\"[英中序列]（轉換後）\")\n",
    "print(en_indices)\n",
    "print(zh_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def encode(en_t, zh_t):\n",
    "  # 因為字典的索引從 0 開始，\n",
    "  # 我們可以使用 subword_encoder_en.vocab_size 這個值作為 BOS 的索引值\n",
    "  # 用 subword_encoder_en.vocab_size + 1 作為 EOS 的索引值\n",
    "  en_indices = [subword_encoder_en.vocab_size] + subword_encoder_en.encode(\n",
    "      en_t.numpy()) + [subword_encoder_en.vocab_size + 1]\n",
    "  # 同理，不過是使用中文字典的最後一個索引 + 1\n",
    "  zh_indices = [subword_encoder_zh.vocab_size] + subword_encoder_zh.encode(\n",
    "      zh_t.numpy()) + [subword_encoder_zh.vocab_size + 1]\n",
    "  \n",
    "  return en_indices, zh_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "英文 BOS 的 index： 8113\n",
      "英文 EOS 的 index： 8114\n",
      "中文 BOS 的 index： 4205\n",
      "中文 EOS 的 index： 4206\n",
      "\n",
      "輸入為 2 個 Tensors：\n",
      "(<tf.Tensor: shape=(), dtype=string, numpy=b'The fear is real and visceral, and politicians ignore it at their peril.'>,\n",
      " <tf.Tensor: shape=(), dtype=string, numpy=b'\\xe8\\xbf\\x99\\xe7\\xa7\\x8d\\xe6\\x81\\x90\\xe6\\x83\\xa7\\xe6\\x98\\xaf\\xe7\\x9c\\x9f\\xe5\\xae\\x9e\\xe8\\x80\\x8c\\xe5\\x86\\x85\\xe5\\x9c\\xa8\\xe7\\x9a\\x84\\xe3\\x80\\x82 \\xe5\\xbf\\xbd\\xe8\\xa7\\x86\\xe5\\xae\\x83\\xe7\\x9a\\x84\\xe6\\x94\\xbf\\xe6\\xb2\\xbb\\xe5\\xae\\xb6\\xe4\\xbb\\xac\\xe5\\x89\\x8d\\xe9\\x80\\x94\\xe5\\xa0\\xaa\\xe5\\xbf\\xa7\\xe3\\x80\\x82'>)\n",
      "---------------\n",
      "輸出為 2 個索引序列：\n",
      "([8113,\n",
      "  16,\n",
      "  1284,\n",
      "  9,\n",
      "  243,\n",
      "  5,\n",
      "  1275,\n",
      "  1756,\n",
      "  156,\n",
      "  1,\n",
      "  5,\n",
      "  1016,\n",
      "  5566,\n",
      "  21,\n",
      "  38,\n",
      "  33,\n",
      "  2982,\n",
      "  7965,\n",
      "  7903,\n",
      "  8114],\n",
      " [4205,\n",
      "  10,\n",
      "  151,\n",
      "  574,\n",
      "  1298,\n",
      "  6,\n",
      "  374,\n",
      "  55,\n",
      "  29,\n",
      "  193,\n",
      "  5,\n",
      "  1,\n",
      "  3,\n",
      "  3981,\n",
      "  931,\n",
      "  431,\n",
      "  125,\n",
      "  1,\n",
      "  17,\n",
      "  124,\n",
      "  33,\n",
      "  20,\n",
      "  97,\n",
      "  1089,\n",
      "  1247,\n",
      "  861,\n",
      "  3,\n",
      "  4206])\n"
     ]
    }
   ],
   "source": [
    "en_t, zh_t = next(iter(train_examples))\n",
    "en_indices, zh_indices = encode(en_t, zh_t)\n",
    "print('英文 BOS 的 index：', subword_encoder_en.vocab_size)\n",
    "print('英文 EOS 的 index：', subword_encoder_en.vocab_size + 1)\n",
    "print('中文 BOS 的 index：', subword_encoder_zh.vocab_size)\n",
    "print('中文 EOS 的 index：', subword_encoder_zh.vocab_size + 1)\n",
    "\n",
    "print('\\n輸入為 2 個 Tensors：')\n",
    "pprint((en_t, zh_t))\n",
    "print('-' * 15)\n",
    "print('輸出為 2 個索引序列：')\n",
    "pprint((en_indices, zh_indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[8113   16 1284    9  243    5 1275 1756  156    1    5 1016 5566   21\n",
      "   38   33 2982 7965 7903 8114], shape=(20,), dtype=int64)\n",
      "tf.Tensor(\n",
      "[4205   10  151  574 1298    6  374   55   29  193    5    1    3 3981\n",
      "  931  431  125    1   17  124   33   20   97 1089 1247  861    3 4206], shape=(28,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "def tf_encode(en_t, zh_t):\n",
    "  # 在 `tf_encode` 函式裡頭的 `en_t` 與 `zh_t` 都不是 Eager Tensors\n",
    "  # 要到 `tf.py_funtion` 裡頭才是\n",
    "  # 另外因為索引都是整數，所以使用 `tf.int64`\n",
    "  return tf.py_function(encode, [en_t, zh_t], [tf.int64, tf.int64])\n",
    "\n",
    "# `tmp_dataset` 為說明用資料集，說明完所有重要的 func，\n",
    "# 我們會從頭建立一個正式的 `train_dataset`\n",
    "tmp_dataset = train_examples.map(tf_encode)\n",
    "en_indices, zh_indices = next(iter(tmp_dataset))\n",
    "print(en_indices)\n",
    "print(zh_indices)\n",
    "## tmp_dataset 的輸出已經是兩個索引序列，而非原文字串"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 40\n",
    "\n",
    "def filter_max_length(en, zh, max_length=MAX_LENGTH):\n",
    "  # en, zh 分別代表英文與中文的索引序列\n",
    "  return tf.logical_and(tf.size(en) <= max_length,\n",
    "                        tf.size(zh) <= max_length)\n",
    "\n",
    "# tf.data.Dataset.filter(func) 只會回傳 func 為真的例子\n",
    "tmp_dataset = tmp_dataset.filter(filter_max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "所有英文與中文序列長度都不超過 40 個 tokens\n",
      "訓練資料集裡總共有 29784 筆數據\n"
     ]
    }
   ],
   "source": [
    "# 因為我們數據量小可以這樣 count\n",
    "num_examples = 0\n",
    "for en_indices, zh_indices in tmp_dataset:\n",
    "    cond1 = len(en_indices) <= MAX_LENGTH\n",
    "    cond2 = len(zh_indices) <= MAX_LENGTH\n",
    "    assert cond1 and cond2\n",
    "    num_examples += 1\n",
    "\n",
    "print(f\"所有英文與中文序列長度都不超過 {MAX_LENGTH} 個 tokens\")\n",
    "print(f\"訓練資料集裡總共有 {num_examples} 筆數據\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "英文索引序列的 batch\n",
      "tf.Tensor(\n",
      "[[8113   16 1284 ...    0    0    0]\n",
      " [8113 1894 1302 ...    0    0    0]\n",
      " [8113   44   40 ...    0    0    0]\n",
      " ...\n",
      " [8113  122  506 ...    0    0    0]\n",
      " [8113   16  215 ...    0    0    0]\n",
      " [8113 7443 7889 ...    0    0    0]], shape=(64, 39), dtype=int64)\n",
      "--------------------\n",
      "中文索引序列的 batch\n",
      "tf.Tensor(\n",
      "[[4205   10  151 ...    0    0    0]\n",
      " [4205  206  275 ...    0    0    0]\n",
      " [4205    5   10 ...    0    0    0]\n",
      " ...\n",
      " [4205   34    6 ...    0    0    0]\n",
      " [4205  317  256 ...    0    0    0]\n",
      " [4205  167  326 ...    0    0    0]], shape=(64, 40), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 64\n",
    "# 將 batch 裡的所有序列都 pad 到同樣長度\n",
    "tmp_dataset = tmp_dataset.padded_batch(BATCH_SIZE, padded_shapes=([-1], [-1]))\n",
    "en_batch, zh_batch = next(iter(tmp_dataset))\n",
    "print(\"英文索引序列的 batch\")\n",
    "print(en_batch)\n",
    "print('-' * 20)\n",
    "print(\"中文索引序列的 batch\")\n",
    "print(zh_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 40\n",
    "BATCH_SIZE = 128\n",
    "BUFFER_SIZE = 15000\n",
    "\n",
    "# 訓練集\n",
    "train_dataset = (train_examples  # 輸出：(英文句子, 中文句子)\n",
    "                 .map(tf_encode) # 輸出：(英文索引序列, 中文索引序列)\n",
    "                 .filter(filter_max_length) # 同上，且序列長度都不超過 40\n",
    "                 .cache() # 加快讀取數據\n",
    "                 .shuffle(BUFFER_SIZE) # 將例子洗牌確保隨機性\n",
    "                 .padded_batch(BATCH_SIZE, # 將 batch 裡的序列都 pad 到一樣長度\n",
    "                               padded_shapes=([-1], [-1]))\n",
    "                 .prefetch(tf.data.experimental.AUTOTUNE)) # 加速\n",
    "# 驗證集\n",
    "val_dataset = (val_examples\n",
    "               .map(tf_encode)\n",
    "               .filter(filter_max_length)\n",
    "               .padded_batch(BATCH_SIZE, \n",
    "                             padded_shapes=([-1], [-1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "英文索引序列的 batch\n",
      "tf.Tensor(\n",
      "[[8113   87 5599 ...    0    0    0]\n",
      " [8113  998    5 ...    0    0    0]\n",
      " [8113 6266 1606 ...    0    0    0]\n",
      " ...\n",
      " [8113   16 6445 ...    0    0    0]\n",
      " [8113 5363   39 ...    0    0    0]\n",
      " [8113 5150 1662 ...    0    0    0]], shape=(128, 40), dtype=int64)\n",
      "--------------------\n",
      "中文索引序列的 batch\n",
      "tf.Tensor(\n",
      "[[4205   10  126 ...    0    0    0]\n",
      " [4205    4   33 ...    0    0    0]\n",
      " [4205   52   11 ...    0    0    0]\n",
      " ...\n",
      " [4205  526  538 ...    0    0    0]\n",
      " [4205  266 1380 ...    0    0    0]\n",
      " [4205   45  116 ...    0    0    0]], shape=(128, 40), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "en_batch, zh_batch = next(iter(train_dataset))\n",
    "print(\"英文索引序列的 batch\")\n",
    "print(en_batch)\n",
    "print('-' * 20)\n",
    "print(\"中文索引序列的 batch\")\n",
    "print(zh_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('It is important.', '这很重要。'),\n",
      " ('The numbers speak for themselves.', '数字证明了一切。')]\n"
     ]
    }
   ],
   "source": [
    "demo_examples = [\n",
    "    (\"It is important.\", \"这很重要。\"),\n",
    "    (\"The numbers speak for themselves.\", \"数字证明了一切。\"),\n",
    "]\n",
    "pprint(demo_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inp: tf.Tensor(\n",
      "[[8113  103    9 1066 7903 8114    0    0]\n",
      " [8113   16 4111 6735   12 2750 7903 8114]], shape=(2, 8), dtype=int64)\n",
      "\n",
      "tar: tf.Tensor(\n",
      "[[4205   10  241   86   27    3 4206    0    0    0]\n",
      " [4205  165  489  398  191   14    7  560    3 4206]], shape=(2, 10), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 2\n",
    "demo_examples = tf.data.Dataset.from_tensor_slices((\n",
    "    [en for en, _ in demo_examples], [zh for _, zh in demo_examples]\n",
    "))\n",
    "\n",
    "# 將兩個句子透過之前定義的字典轉換成子詞的序列（sequence of subwords）\n",
    "# 並添加 padding token: <pad> 來確保 batch 裡的句子有一樣長度\n",
    "demo_dataset = demo_examples.map(tf_encode)\\\n",
    "  .padded_batch(batch_size, padded_shapes=([-1], [-1]))\n",
    "\n",
    "# 取出這個 demo dataset 裡唯一一個 batch\n",
    "inp, tar = next(iter(demo_dataset))\n",
    "print('inp:', inp)\n",
    "print('' * 10)\n",
    "print('tar:', tar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(2, 8, 4), dtype=float32, numpy=\n",
       " array([[[-0.02940199, -0.02748928,  0.009587  ,  0.02695702],\n",
       "         [ 0.02781546, -0.00051839, -0.04208876, -0.03737368],\n",
       "         [ 0.04085249, -0.01280526, -0.02142819,  0.00932728],\n",
       "         [-0.04287729,  0.00559031, -0.01610124,  0.02254481],\n",
       "         [ 0.02117098, -0.03711741, -0.01554632,  0.00051199],\n",
       "         [-0.00089258,  0.03358604,  0.04734136, -0.02150711],\n",
       "         [-0.01283282,  0.03373785,  0.0044058 , -0.04340933],\n",
       "         [-0.01283282,  0.03373785,  0.0044058 , -0.04340933]],\n",
       " \n",
       "        [[-0.02940199, -0.02748928,  0.009587  ,  0.02695702],\n",
       "         [-0.02589555, -0.00853635,  0.03899905,  0.02993281],\n",
       "         [ 0.02303552, -0.0043821 ,  0.03425056,  0.02410031],\n",
       "         [-0.02790775,  0.0453908 , -0.00815301, -0.01390294],\n",
       "         [ 0.02987817,  0.02534869,  0.04416693, -0.03023093],\n",
       "         [ 0.03675361,  0.04292214,  0.0072703 , -0.03425226],\n",
       "         [ 0.02117098, -0.03711741, -0.01554632,  0.00051199],\n",
       "         [-0.00089258,  0.03358604,  0.04734136, -0.02150711]]],\n",
       "       dtype=float32)>,\n",
       " <tf.Tensor: shape=(2, 10, 4), dtype=float32, numpy=\n",
       " array([[[ 0.02309941,  0.01578486, -0.03650398,  0.04221227],\n",
       "         [ 0.02180279, -0.03836104, -0.0052525 , -0.01320656],\n",
       "         [-0.0190685 , -0.03266476, -0.03334127,  0.04303731],\n",
       "         [ 0.03618959, -0.00743835,  0.01616282,  0.04101736],\n",
       "         [ 0.00099171,  0.03080915, -0.03851545,  0.03447074],\n",
       "         [-0.02579685,  0.02527661,  0.04145398, -0.04307295],\n",
       "         [-0.00221508,  0.03029467,  0.02829352,  0.04788197],\n",
       "         [-0.02839622,  0.04139889, -0.00255186, -0.03752811],\n",
       "         [-0.02839622,  0.04139889, -0.00255186, -0.03752811],\n",
       "         [-0.02839622,  0.04139889, -0.00255186, -0.03752811]],\n",
       " \n",
       "        [[ 0.02309941,  0.01578486, -0.03650398,  0.04221227],\n",
       "         [-0.00205498,  0.04796953, -0.00836701,  0.03551394],\n",
       "         [-0.0049132 , -0.03055991,  0.03992986, -0.02458232],\n",
       "         [ 0.02458249, -0.02872031,  0.01778464, -0.02678457],\n",
       "         [ 0.03532792,  0.02878738, -0.00509523, -0.0492527 ],\n",
       "         [-0.00865723, -0.02110739, -0.03051426, -0.02639371],\n",
       "         [ 0.02155342, -0.00126004,  0.00749636, -0.00912453],\n",
       "         [-0.01634679,  0.00527266,  0.01074455,  0.02497052],\n",
       "         [-0.02579685,  0.02527661,  0.04145398, -0.04307295],\n",
       "         [-0.00221508,  0.03029467,  0.02829352,  0.04788197]]],\n",
       "       dtype=float32)>)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# + 2 是因為我們額外加了 <start> 以及 <end> tokens\n",
    "vocab_size_en = subword_encoder_en.vocab_size + 2\n",
    "vocab_size_zh = subword_encoder_zh.vocab_size + 2\n",
    "\n",
    "# 為了方便 demo, 將詞彙轉換到一個 4 維的詞嵌入空間\n",
    "d_model = 4\n",
    "embedding_layer_en = tf.keras.layers.Embedding(vocab_size_en, d_model)\n",
    "embedding_layer_zh = tf.keras.layers.Embedding(vocab_size_zh, d_model)\n",
    "\n",
    "emb_inp = embedding_layer_en(inp)\n",
    "emb_tar = embedding_layer_zh(tar)\n",
    "emb_inp, emb_tar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tar[0]: tf.Tensor([0 0 0], shape=(3,), dtype=int64)\n",
      "--------------------\n",
      "emb_tar[0]: tf.Tensor(\n",
      "[[-0.02839622  0.04139889 -0.00255186 -0.03752811]\n",
      " [-0.02839622  0.04139889 -0.00255186 -0.03752811]\n",
      " [-0.02839622  0.04139889 -0.00255186 -0.03752811]], shape=(3, 4), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(\"tar[0]:\", tar[0][-3:])\n",
    "print(\"-\" * 20)\n",
    "print(\"emb_tar[0]:\", emb_tar[0][-3:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 1, 1, 8), dtype=float32, numpy=\n",
       "array([[[[0., 0., 0., 0., 0., 0., 1., 1.]]],\n",
       "\n",
       "\n",
       "       [[[0., 0., 0., 0., 0., 0., 0., 0.]]]], dtype=float32)>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_padding_mask(seq):\n",
    "    # padding mask 的工作就是把索引序列中為 0 的位置設為 1\n",
    "    mask = tf.cast(tf.equal(seq, 0), tf.float32)\n",
    "    return mask[:, tf.newaxis, tf.newaxis, :] #　broadcasting\n",
    "\n",
    "inp_mask = create_padding_mask(inp)\n",
    "inp_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inp: tf.Tensor(\n",
      "[[8113  103    9 1066 7903 8114    0    0]\n",
      " [8113   16 4111 6735   12 2750 7903 8114]], shape=(2, 8), dtype=int64)\n",
      "--------------------\n",
      "tf.squeeze(inp_mask): tf.Tensor(\n",
      "[[0. 0. 0. 0. 0. 0. 1. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]], shape=(2, 8), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"inp:\", inp)\n",
    "print(\"-\" * 20)\n",
    "print(\"tf.squeeze(inp_mask):\", tf.squeeze(inp_mask))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 8, 4), dtype=float32, numpy=\n",
       "array([[[1., 0., 0., 0.],\n",
       "        [0., 1., 0., 1.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [1., 0., 1., 0.],\n",
       "        [1., 0., 1., 0.],\n",
       "        [0., 1., 0., 1.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [0., 1., 0., 1.]],\n",
       "\n",
       "       [[1., 0., 1., 1.],\n",
       "        [1., 0., 1., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [1., 0., 1., 0.],\n",
       "        [0., 1., 0., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 1., 0.]]], dtype=float32)>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 設定一個 seed 確保我們每次都拿到一樣的隨機結果\n",
    "tf.random.set_seed(9527)\n",
    "\n",
    "# 自注意力機制：查詢 `q` 跟鍵值 `k` 都是 `emb_inp`\n",
    "q = emb_inp\n",
    "k = emb_inp\n",
    "# 簡單產生一個跟 `emb_inp` 同樣 shape 的 binary vector\n",
    "v = tf.cast(tf.math.greater(tf.random.uniform(shape=emb_inp.shape), 0.5), tf.float32)\n",
    "v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(q, k, v, mask):\n",
    "    # 將 `q`、 `k` 做點積再 scale\n",
    "    matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n",
    "\n",
    "    dk = tf.cast(tf.shape(k)[-1], tf.float32)  # 取得 seq_k 的序列長度\n",
    "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)  # scale by sqrt(dk)\n",
    "\n",
    "    # 將遮罩「加」到被丟入 softmax 前的 logits\n",
    "    if mask is not None:\n",
    "        scaled_attention_logits += (mask * -1e9)\n",
    "\n",
    "    # 取 softmax 是為了得到總和為 1 的比例之後對 `v` 做加權平均\n",
    "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n",
    "\n",
    "    # 以注意權重對 v 做加權平均（weighted average）\n",
    "    output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n",
    "\n",
    "    return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output: tf.Tensor(\n",
      "[[[0.3753399  0.3747662  0.37508252 0.49974045]\n",
      "  [0.37475583 0.37513748 0.37496054 0.50021064]\n",
      "  [0.37499675 0.3749313  0.37495598 0.50008714]\n",
      "  [0.3752308  0.3748534  0.37511384 0.49978137]\n",
      "  [0.37511206 0.37487078 0.37499475 0.49998224]\n",
      "  [0.3747127  0.37527195 0.3749196  0.5001556 ]\n",
      "  [0.3746645  0.37529942 0.3749698  0.5001717 ]\n",
      "  [0.3746645  0.37529942 0.3749698  0.5001717 ]]\n",
      "\n",
      " [[0.6251311  0.24970885 0.62504077 0.37487447]\n",
      "  [0.6251049  0.24979301 0.6250699  0.37489533]\n",
      "  [0.624972   0.25000775 0.62488645 0.37499285]\n",
      "  [0.6250523  0.25008556 0.62523466 0.3750197 ]\n",
      "  [0.6248019  0.25033873 0.62493974 0.375137  ]\n",
      "  [0.624817   0.25040787 0.6249676  0.37516844]\n",
      "  [0.62495446 0.24994777 0.62482053 0.37499923]\n",
      "  [0.624902   0.25021926 0.6250832  0.37507576]]], shape=(2, 8, 4), dtype=float32)\n",
      "--------------------\n",
      "attention_weights: tf.Tensor(\n",
      "[[[0.125177   0.12488609 0.12497426 0.12512204 0.12504087 0.12496053\n",
      "   0.1249196  0.1249196 ]\n",
      "  [0.12482883 0.1252137  0.12507316 0.12488215 0.12504484 0.12489024\n",
      "   0.12503353 0.12503353]\n",
      "  [0.12495689 0.12511314 0.12515587 0.12492786 0.12511201 0.12490202\n",
      "   0.12491614 0.12491614]\n",
      "  [0.12510477 0.1249222  0.12492798 0.12517215 0.12495389 0.12494341\n",
      "   0.12498779 0.12498779]\n",
      "  [0.12502298 0.12508428 0.12511148 0.12495323 0.12513587 0.12488083\n",
      "   0.12490567 0.12490567]\n",
      "  [0.12492479 0.1249118  0.12488367 0.12492491 0.12486301 0.12522845\n",
      "   0.1251317  0.1251317 ]\n",
      "  [0.12485838 0.1250296  0.12487227 0.12494376 0.12486234 0.12510614\n",
      "   0.1251637  0.1251637 ]\n",
      "  [0.12485838 0.1250296  0.12487227 0.12494376 0.12486234 0.12510614\n",
      "   0.1251637  0.1251637 ]]\n",
      "\n",
      " [[0.12516564 0.12514925 0.12503944 0.12495811 0.12489024 0.12481861\n",
      "   0.12502952 0.12494919]\n",
      "  [0.12510231 0.12516384 0.12505984 0.1249413  0.12495543 0.12483758\n",
      "   0.12491484 0.12502488]\n",
      "  [0.12498508 0.12505239 0.12510279 0.12486783 0.12504385 0.12496389\n",
      "   0.1249669  0.12501723]\n",
      "  [0.12493413 0.1249642  0.12489816 0.12518294 0.1250127  0.12507287\n",
      "   0.12485447 0.12508056]\n",
      "  [0.12479828 0.1249103  0.12500612 0.12494461 0.12519619 0.12514254\n",
      "   0.12485797 0.12514399]\n",
      "  [0.12476055 0.12482633 0.12496009 0.12503868 0.1251765  0.12523137\n",
      "   0.1248959  0.12511061]\n",
      "  [0.12505144 0.12498362 0.12504315 0.1249003  0.12497186 0.12497591\n",
      "   0.12516436 0.12490926]\n",
      "  [0.12485649 0.12497903 0.12497881 0.12501174 0.12514329 0.12507597\n",
      "   0.12479473 0.12515998]]], shape=(2, 8, 8), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "mask = None\n",
    "output, attention_weights = scaled_dot_product_attention(q, k, v, mask)\n",
    "print(\"output:\", output)\n",
    "print(\"-\" * 20)\n",
    "print(\"attention_weights:\", attention_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inp: tf.Tensor(\n",
      "[[8113  103    9 1066 7903 8114    0    0]\n",
      " [8113   16 4111 6735   12 2750 7903 8114]], shape=(2, 8), dtype=int64)\n",
      "--------------------\n",
      "inp_mask: tf.Tensor(\n",
      "[[[[0. 0. 0. 0. 0. 0. 1. 1.]]]\n",
      "\n",
      "\n",
      " [[[0. 0. 0. 0. 0. 0. 0. 0.]]]], shape=(2, 1, 1, 8), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "def create_padding_mask(seq):\n",
    "    # padding mask 的工作就是把索引序列中為 0 的位置設為 1\n",
    "    mask = tf.cast(tf.equal(seq, 0), tf.float32)\n",
    "    return mask[:, tf.newaxis, tf.newaxis, :] #　broadcasting\n",
    "\n",
    "print(\"inp:\", inp)\n",
    "inp_mask = create_padding_mask(inp)\n",
    "print(\"-\" * 20)\n",
    "print(\"inp_mask:\", inp_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention_weights: tf.Tensor(\n",
      "[[[0.16686688 0.16647908 0.16659662 0.16679361 0.16668542 0.16657832\n",
      "   0.         0.        ]\n",
      "  [0.16645332 0.16696653 0.16677913 0.16652443 0.16674137 0.16653521\n",
      "   0.         0.        ]\n",
      "  [0.16657193 0.16678022 0.16683717 0.16653322 0.16677871 0.16649878\n",
      "   0.         0.        ]\n",
      "  [0.16680093 0.1665575  0.16656522 0.16689077 0.16659975 0.16658579\n",
      "   0.         0.        ]\n",
      "  [0.16665538 0.1667371  0.16677335 0.16656241 0.16680586 0.1664659\n",
      "   0.         0.        ]\n",
      "  [0.1666249  0.16660757 0.16657004 0.16662505 0.16654249 0.16702992\n",
      "   0.         0.        ]\n",
      "  [0.16655058 0.16677895 0.1665691  0.16666447 0.16655585 0.16688107\n",
      "   0.         0.        ]\n",
      "  [0.16655058 0.16677895 0.1665691  0.16666447 0.16655585 0.16688107\n",
      "   0.         0.        ]]\n",
      "\n",
      " [[0.12516564 0.12514925 0.12503944 0.12495811 0.12489024 0.12481861\n",
      "   0.12502952 0.12494919]\n",
      "  [0.12510231 0.12516384 0.12505984 0.1249413  0.12495543 0.12483758\n",
      "   0.12491484 0.12502488]\n",
      "  [0.12498508 0.12505239 0.12510279 0.12486783 0.12504385 0.12496389\n",
      "   0.1249669  0.12501723]\n",
      "  [0.12493413 0.1249642  0.12489816 0.12518294 0.1250127  0.12507287\n",
      "   0.12485447 0.12508056]\n",
      "  [0.12479828 0.1249103  0.12500612 0.12494461 0.12519619 0.12514254\n",
      "   0.12485797 0.12514399]\n",
      "  [0.12476055 0.12482633 0.12496009 0.12503868 0.1251765  0.12523137\n",
      "   0.1248959  0.12511061]\n",
      "  [0.12505144 0.12498362 0.12504315 0.1249003  0.12497186 0.12497591\n",
      "   0.12516436 0.12490926]\n",
      "  [0.12485649 0.12497903 0.12497881 0.12501174 0.12514329 0.12507597\n",
      "   0.12479473 0.12515998]]], shape=(2, 8, 8), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# 這次讓我們將 padding mask 放入注意函式並觀察\n",
    "# 注意權重的變化\n",
    "mask = tf.squeeze(inp_mask, axis=1) # (batch_size, 1, seq_len_q)\n",
    "_, attention_weights = scaled_dot_product_attention(q, k, v, mask)\n",
    "print(\"attention_weights:\", attention_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 8, 2), dtype=float32, numpy=\n",
       "array([[[0.        , 0.        ],\n",
       "        [0.        , 0.        ],\n",
       "        [0.        , 0.        ],\n",
       "        [0.        , 0.        ],\n",
       "        [0.        , 0.        ],\n",
       "        [0.        , 0.        ],\n",
       "        [0.        , 0.        ],\n",
       "        [0.        , 0.        ]],\n",
       "\n",
       "       [[0.12502952, 0.12494919],\n",
       "        [0.12491484, 0.12502488],\n",
       "        [0.1249669 , 0.12501723],\n",
       "        [0.12485447, 0.12508056],\n",
       "        [0.12485797, 0.12514399],\n",
       "        [0.1248959 , 0.12511061],\n",
       "        [0.12516436, 0.12490926],\n",
       "        [0.12479473, 0.12515998]]], dtype=float32)>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 事實上也不完全是上句話的翻譯，\n",
    "# 因為我們在第一個維度還是把兩個句子都拿出來方便你比較\n",
    "attention_weights[:, :, -2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emb_tar: tf.Tensor(\n",
      "[[[ 0.02309941  0.01578486 -0.03650398  0.04221227]\n",
      "  [ 0.02180279 -0.03836104 -0.0052525  -0.01320656]\n",
      "  [-0.0190685  -0.03266476 -0.03334127  0.04303731]\n",
      "  [ 0.03618959 -0.00743835  0.01616282  0.04101736]\n",
      "  [ 0.00099171  0.03080915 -0.03851545  0.03447074]\n",
      "  [-0.02579685  0.02527661  0.04145398 -0.04307295]\n",
      "  [-0.00221508  0.03029467  0.02829352  0.04788197]\n",
      "  [-0.02839622  0.04139889 -0.00255186 -0.03752811]\n",
      "  [-0.02839622  0.04139889 -0.00255186 -0.03752811]\n",
      "  [-0.02839622  0.04139889 -0.00255186 -0.03752811]]\n",
      "\n",
      " [[ 0.02309941  0.01578486 -0.03650398  0.04221227]\n",
      "  [-0.00205498  0.04796953 -0.00836701  0.03551394]\n",
      "  [-0.0049132  -0.03055991  0.03992986 -0.02458232]\n",
      "  [ 0.02458249 -0.02872031  0.01778464 -0.02678457]\n",
      "  [ 0.03532792  0.02878738 -0.00509523 -0.0492527 ]\n",
      "  [-0.00865723 -0.02110739 -0.03051426 -0.02639371]\n",
      "  [ 0.02155342 -0.00126004  0.00749636 -0.00912453]\n",
      "  [-0.01634679  0.00527266  0.01074455  0.02497052]\n",
      "  [-0.02579685  0.02527661  0.04145398 -0.04307295]\n",
      "  [-0.00221508  0.03029467  0.02829352  0.04788197]]], shape=(2, 10, 4), dtype=float32)\n",
      "--------------------\n",
      "look_ahead_mask tf.Tensor(\n",
      "[[0. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [0. 0. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [0. 0. 0. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [0. 0. 0. 0. 1. 1. 1. 1. 1. 1.]\n",
      " [0. 0. 0. 0. 0. 1. 1. 1. 1. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 1. 1. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 1. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]], shape=(10, 10), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# 建立一個 2 維矩陣，維度為 (size, size)，\n",
    "# 其遮罩為一個右上角的三角形\n",
    "def create_look_ahead_mask(size):\n",
    "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "    return mask  # (seq_len, seq_len)\n",
    "\n",
    "seq_len = emb_tar.shape[1] # 注意這次我們用中文的詞嵌入張量 `emb_tar`\n",
    "look_ahead_mask = create_look_ahead_mask(seq_len)\n",
    "print(\"emb_tar:\", emb_tar)\n",
    "print(\"-\" * 20)\n",
    "print(\"look_ahead_mask\", look_ahead_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention_weights: tf.Tensor(\n",
      "[[[1.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.        ]\n",
      "  [0.49967298 0.5003271  0.         0.         0.         0.\n",
      "   0.         0.         0.         0.        ]\n",
      "  [0.3332953  0.33302316 0.33368158 0.         0.         0.\n",
      "   0.         0.         0.         0.        ]\n",
      "  [0.25003257 0.2498561  0.24989752 0.25021377 0.         0.\n",
      "   0.         0.         0.         0.        ]\n",
      "  [0.20017864 0.19970043 0.20001577 0.19990137 0.20020382 0.\n",
      "   0.         0.         0.         0.        ]\n",
      "  [0.16648294 0.16667846 0.1664795  0.16659202 0.16658287 0.16718417\n",
      "   0.         0.         0.         0.        ]\n",
      "  [0.14288522 0.14264204 0.14279628 0.14293528 0.14289069 0.14277938\n",
      "   0.14307113 0.         0.         0.        ]\n",
      "  [0.12492007 0.12490728 0.12486716 0.12483124 0.12501663 0.12521917\n",
      "   0.12497888 0.1252596  0.         0.        ]\n",
      "  [0.11101444 0.11100308 0.11096742 0.1109355  0.11110026 0.11128026\n",
      "   0.11106671 0.11131617 0.11131617 0.        ]\n",
      "  [0.09989457 0.09988434 0.09985226 0.09982353 0.09997179 0.10013375\n",
      "   0.0999416  0.10016607 0.10016607 0.10016607]]\n",
      "\n",
      " [[1.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.        ]\n",
      "  [0.49985975 0.5001403  0.         0.         0.         0.\n",
      "   0.         0.         0.         0.        ]\n",
      "  [0.3329623  0.3330336  0.33400407 0.         0.         0.\n",
      "   0.         0.         0.         0.        ]\n",
      "  [0.24977945 0.24967171 0.25025332 0.25029555 0.         0.\n",
      "   0.         0.         0.         0.        ]\n",
      "  [0.1998431  0.1998655  0.19990072 0.20003231 0.20035847 0.\n",
      "   0.         0.         0.         0.        ]\n",
      "  [0.16660614 0.16651092 0.16666041 0.16669704 0.16669574 0.16682968\n",
      "   0.         0.         0.         0.        ]\n",
      "  [0.14282052 0.14279835 0.14286603 0.14290085 0.14291462 0.14282288\n",
      "   0.1428767  0.         0.         0.        ]\n",
      "  [0.12503847 0.12508324 0.12499889 0.12495106 0.12490863 0.12495571\n",
      "   0.12498385 0.12508014 0.         0.        ]\n",
      "  [0.11088645 0.11104831 0.11119718 0.11111174 0.11117818 0.1110579\n",
      "   0.11108869 0.11107807 0.11135351 0.        ]\n",
      "  [0.10005799 0.10013337 0.09993912 0.09990206 0.09990182 0.09984995\n",
      "   0.09997167 0.100072   0.09998387 0.10018817]]], shape=(2, 10, 10), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 讓我們用目標語言（中文）的 batch\n",
    "# 來模擬 Decoder 處理的情況\n",
    "temp_q = temp_k = emb_tar\n",
    "temp_v = tf.cast(tf.math.greater(\n",
    "    tf.random.uniform(shape=emb_tar.shape), 0.5), tf.float32)\n",
    "\n",
    "# 將 look_ahead_mask 放入注意函式\n",
    "_, attention_weights = scaled_dot_product_attention(\n",
    "    temp_q, temp_k, temp_v, look_ahead_mask)\n",
    "\n",
    "print(\"attention_weights:\", attention_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 10), dtype=float32, numpy=\n",
       "array([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]], dtype=float32)>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_weights[:, 0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: tf.Tensor(\n",
      "[[[-0.02940199 -0.02748928  0.009587    0.02695702]\n",
      "  [ 0.02781546 -0.00051839 -0.04208876 -0.03737368]\n",
      "  [ 0.04085249 -0.01280526 -0.02142819  0.00932728]\n",
      "  [-0.04287729  0.00559031 -0.01610124  0.02254481]\n",
      "  [ 0.02117098 -0.03711741 -0.01554632  0.00051199]\n",
      "  [-0.00089258  0.03358604  0.04734136 -0.02150711]\n",
      "  [-0.01283282  0.03373785  0.0044058  -0.04340933]\n",
      "  [-0.01283282  0.03373785  0.0044058  -0.04340933]]\n",
      "\n",
      " [[-0.02940199 -0.02748928  0.009587    0.02695702]\n",
      "  [-0.02589555 -0.00853635  0.03899905  0.02993281]\n",
      "  [ 0.02303552 -0.0043821   0.03425056  0.02410031]\n",
      "  [-0.02790775  0.0453908  -0.00815301 -0.01390294]\n",
      "  [ 0.02987817  0.02534869  0.04416693 -0.03023093]\n",
      "  [ 0.03675361  0.04292214  0.0072703  -0.03425226]\n",
      "  [ 0.02117098 -0.03711741 -0.01554632  0.00051199]\n",
      "  [-0.00089258  0.03358604  0.04734136 -0.02150711]]], shape=(2, 8, 4), dtype=float32)\n",
      "output: tf.Tensor(\n",
      "[[[[-0.02940199 -0.02748928]\n",
      "   [ 0.02781546 -0.00051839]\n",
      "   [ 0.04085249 -0.01280526]\n",
      "   [-0.04287729  0.00559031]\n",
      "   [ 0.02117098 -0.03711741]\n",
      "   [-0.00089258  0.03358604]\n",
      "   [-0.01283282  0.03373785]\n",
      "   [-0.01283282  0.03373785]]\n",
      "\n",
      "  [[ 0.009587    0.02695702]\n",
      "   [-0.04208876 -0.03737368]\n",
      "   [-0.02142819  0.00932728]\n",
      "   [-0.01610124  0.02254481]\n",
      "   [-0.01554632  0.00051199]\n",
      "   [ 0.04734136 -0.02150711]\n",
      "   [ 0.0044058  -0.04340933]\n",
      "   [ 0.0044058  -0.04340933]]]\n",
      "\n",
      "\n",
      " [[[-0.02940199 -0.02748928]\n",
      "   [-0.02589555 -0.00853635]\n",
      "   [ 0.02303552 -0.0043821 ]\n",
      "   [-0.02790775  0.0453908 ]\n",
      "   [ 0.02987817  0.02534869]\n",
      "   [ 0.03675361  0.04292214]\n",
      "   [ 0.02117098 -0.03711741]\n",
      "   [-0.00089258  0.03358604]]\n",
      "\n",
      "  [[ 0.009587    0.02695702]\n",
      "   [ 0.03899905  0.02993281]\n",
      "   [ 0.03425056  0.02410031]\n",
      "   [-0.00815301 -0.01390294]\n",
      "   [ 0.04416693 -0.03023093]\n",
      "   [ 0.0072703  -0.03425226]\n",
      "   [-0.01554632  0.00051199]\n",
      "   [ 0.04734136 -0.02150711]]]], shape=(2, 2, 8, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "def split_heads(x, d_model, num_heads):\n",
    "    # x.shape: (batch_size, seq_len, d_model)\n",
    "    batch_size = tf.shape(x)[0]\n",
    "\n",
    "    # 我們要確保維度 `d_model` 可以被平分成 `num_heads` 個 `depth` 維度\n",
    "    assert d_model % num_heads == 0\n",
    "    depth = d_model // num_heads  # 這是分成多頭以後每個向量的維度 \n",
    "\n",
    "    # 將最後一個 d_model 維度分成 num_heads 個 depth 維度。\n",
    "    # 最後一個維度變成兩個維度，張量 x 從 3 維到 4 維\n",
    "    # (batch_size, seq_len, num_heads, depth)\n",
    "    reshaped_x = tf.reshape(x, shape=(batch_size, -1, num_heads, depth))\n",
    "\n",
    "    # 將 head 的維度拉前使得最後兩個維度為子詞以及其對應的 depth 向量\n",
    "    # (batch_size, num_heads, seq_len, depth)\n",
    "    output = tf.transpose(reshaped_x, perm=[0, 2, 1, 3])\n",
    "\n",
    "    return output\n",
    "\n",
    "# 我們的 `emb_inp` 裡頭的子詞本來就是 4 維的詞嵌入向量\n",
    "d_model = 4\n",
    "# 將 4 維詞嵌入向量分為 2 個 head 的 2 維矩陣\n",
    "num_heads = 2\n",
    "x = emb_inp\n",
    "\n",
    "output = split_heads(x, d_model, num_heads)  \n",
    "print(\"x:\", x)\n",
    "print(\"output:\", output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 實作一個執行多頭注意力機制的 keras layer\n",
    "# 在初始的時候指定輸出維度 `d_model` & `num_heads，\n",
    "# 在呼叫的時候輸入 `v`, `k`, `q` 以及 `mask`\n",
    "# 輸出跟 scaled_dot_product_attention 函式一樣有兩個：\n",
    "# output.shape            == (batch_size, seq_len_q, d_model)\n",
    "# attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "  # 在初始的時候建立一些必要參數\n",
    "  def __init__(self, d_model, num_heads):\n",
    "    super(MultiHeadAttention, self).__init__()\n",
    "    self.num_heads = num_heads # 指定要將 `d_model` 拆成幾個 heads\n",
    "    self.d_model = d_model # 在 split_heads 之前的基底維度\n",
    "    \n",
    "    assert d_model % self.num_heads == 0  # 前面看過，要確保可以平分\n",
    "    \n",
    "    self.depth = d_model // self.num_heads  # 每個 head 裡子詞的新的 repr. 維度\n",
    "    \n",
    "    self.wq = tf.keras.layers.Dense(d_model)  # 分別給 q, k, v 的 3 個線性轉換 \n",
    "    self.wk = tf.keras.layers.Dense(d_model)  # 注意我們並沒有指定 activation func\n",
    "    self.wv = tf.keras.layers.Dense(d_model)\n",
    "    \n",
    "    self.dense = tf.keras.layers.Dense(d_model)  # 多 heads 串接後通過的線性轉換\n",
    "    \n",
    "    \n",
    "  # 這跟我們前面看過的函式有 87% 相似\n",
    "  def split_heads(self, x, batch_size):\n",
    "    \"\"\"Split the last dimension into (num_heads, depth).\n",
    "    Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
    "    \"\"\"\n",
    "    x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "    return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "  \n",
    "  # multi-head attention 的實際執行流程，注意參數順序（這邊跟論文以及 TensorFlow 官方教學一致）\n",
    "  def call(self, v, k, q, mask):\n",
    "    batch_size = tf.shape(q)[0]\n",
    "    \n",
    "    # 將輸入的 q, k, v 都各自做一次線性轉換到 `d_model` 維空間\n",
    "    q = self.wq(q)  # (batch_size, seq_len, d_model)\n",
    "    k = self.wk(k)  # (batch_size, seq_len, d_model)\n",
    "    v = self.wv(v)  # (batch_size, seq_len, d_model)\n",
    "    \n",
    "    # 前面看過的，將最後一個 `d_model` 維度分成 `num_heads` 個 `depth` 維度\n",
    "    q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n",
    "    k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n",
    "    v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n",
    "    \n",
    "    # 利用 broadcasting 讓每個句子的每個 head 的 qi, ki, vi 都各自進行注意力機制\n",
    "    # 輸出會多一個 head 維度\n",
    "    scaled_attention, attention_weights = scaled_dot_product_attention(\n",
    "        q, k, v, mask)\n",
    "    # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n",
    "    # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "    \n",
    "    \n",
    "    # 跟我們在 `split_heads` 函式做的事情剛好相反，先做 transpose 再做 reshape\n",
    "    # 將 `num_heads` 個 `depth` 維度串接回原來的 `d_model` 維度\n",
    "    scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n",
    "    # (batch_size, seq_len_q, num_heads, depth)\n",
    "    concat_attention = tf.reshape(scaled_attention, \n",
    "                                  (batch_size, -1, self.d_model)) \n",
    "    # (batch_size, seq_len_q, d_model)\n",
    "\n",
    "    # 通過最後一個線性轉換\n",
    "    output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n",
    "        \n",
    "    return output, attention_weights    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d_model: 4\n",
      "num_heads: 2\n",
      "\n",
      "q.shape: (2, 8, 4)\n",
      "k.shape: (2, 8, 4)\n",
      "v.shape: (2, 8, 4)\n",
      "padding_mask.shape: (2, 1, 1, 8)\n",
      "output.shape: (2, 8, 4)\n",
      "attention_weights.shape: (2, 2, 8, 8)\n",
      "\n",
      "output: tf.Tensor(\n",
      "[[[ 0.00708912 -0.00861574 -0.00824888 -0.00125531]\n",
      "  [ 0.00704044 -0.00856255 -0.00824743 -0.00130908]\n",
      "  [ 0.00706525 -0.00858905 -0.00825153 -0.00128642]\n",
      "  [ 0.00707757 -0.00860305 -0.00824692 -0.00126657]\n",
      "  [ 0.00706903 -0.00859383 -0.00825078 -0.00128115]\n",
      "  [ 0.00706301 -0.00858597 -0.00824349 -0.00127383]\n",
      "  [ 0.00704599 -0.00856815 -0.00824224 -0.00129322]\n",
      "  [ 0.00704599 -0.00856815 -0.00824224 -0.00129322]]\n",
      "\n",
      " [[-0.0084104   0.00985345  0.0218877   0.01573933]\n",
      "  [-0.00840437  0.00984869  0.02188719  0.01574546]\n",
      "  [-0.00840402  0.00984824  0.02187178  0.01572827]\n",
      "  [-0.00845825  0.0099027   0.02189481  0.01570285]\n",
      "  [-0.00844073  0.00988418  0.02187815  0.01570553]\n",
      "  [-0.00845728  0.00990096  0.02187634  0.01568546]\n",
      "  [-0.00842486  0.00986613  0.02187385  0.01571093]\n",
      "  [-0.0084405   0.0098847   0.02188747  0.0157156 ]]], shape=(2, 8, 4), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# emb_inp.shape == (batch_size, seq_len, d_model)\n",
    "#               == (2, 8, 4)\n",
    "assert d_model == emb_inp.shape[-1]  == 4\n",
    "num_heads = 2\n",
    "\n",
    "print(f\"d_model: {d_model}\")\n",
    "print(f\"num_heads: {num_heads}\\n\")\n",
    "\n",
    "# 初始化一個 multi-head attention layer\n",
    "mha = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "# 簡單將 v, k, q 都設置為 `emb_inp`\n",
    "# 順便看看 padding mask 的作用。\n",
    "# 別忘記，第一個英文序列的最後兩個 tokens 是 <pad>\n",
    "v = k = q = emb_inp\n",
    "padding_mask = create_padding_mask(inp)\n",
    "print(\"q.shape:\", q.shape)\n",
    "print(\"k.shape:\", k.shape)\n",
    "print(\"v.shape:\", v.shape)\n",
    "print(\"padding_mask.shape:\", padding_mask.shape)\n",
    "\n",
    "output, attention_weights = mha(v, k, q, mask)\n",
    "print(\"output.shape:\", output.shape)\n",
    "print(\"attention_weights.shape:\", attention_weights.shape)\n",
    "\n",
    "print(\"\\noutput:\", output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 建立 Transformer 裡 Encoder / Decoder layer 都有使用到的 Feed Forward 元件\n",
    "def point_wise_feed_forward_network(d_model, dff):\n",
    "  \n",
    "  # 此 FFN 對輸入做兩個線性轉換，中間加了一個 ReLU activation func\n",
    "    return tf.keras.Sequential([\n",
    "      tf.keras.layers.Dense(dff, activation='relu'),  # (batch_size, seq_len, dff)\n",
    "      tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.shape: (64, 10, 512)\n",
      "out.shape: (64, 10, 512)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "batch_size = 64\n",
    "seq_len = 10\n",
    "d_model = 512\n",
    "dff = 2048\n",
    "\n",
    "x = tf.random.uniform((batch_size, seq_len, d_model))\n",
    "ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "out = ffn(x)\n",
    "print(\"x.shape:\", x.shape)\n",
    "print(\"out.shape:\", out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5, 4), dtype=float32, numpy=\n",
       "array([[ 2.8674245 , -2.174698  , -1.3073453 , -6.4233937 ],\n",
       "       [ 2.8674245 , -2.174698  , -1.3073453 , -6.4233937 ],\n",
       "       [ 3.6502066 , -0.97325826, -2.4126563 , -6.509499  ],\n",
       "       [ 3.6502066 , -0.97325826, -2.4126563 , -6.509499  ],\n",
       "       [ 3.6502066 , -0.97325826, -2.4126563 , -6.509499  ]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_model = 4 # FFN 的輸入輸出張量的最後一維皆為 `d_model`\n",
    "dff = 6\n",
    "\n",
    "# 建立一個小 FFN\n",
    "small_ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "# 懂子詞梗的站出來\n",
    "dummy_sentence = tf.constant([[5, 5, 6, 6], \n",
    "                              [5, 5, 6, 6], \n",
    "                              [9, 5, 2, 7], \n",
    "                              [9, 5, 2, 7],\n",
    "                              [9, 5, 2, 7]], dtype=tf.float32)\n",
    "small_ffn(dummy_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Encoder 裡頭會有 N 個 EncoderLayers，而每個 EncoderLayer 裡又有兩個 sub-layers: MHA & FFN\n",
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "  # Transformer 論文內預設 dropout rate 為 0.1\n",
    "  def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "    super(EncoderLayer, self).__init__()\n",
    "\n",
    "    self.mha = MultiHeadAttention(d_model, num_heads)\n",
    "    self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "    # layer norm 很常在 RNN-based 的模型被使用。一個 sub-layer 一個 layer norm\n",
    "    self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "    self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "    \n",
    "    # 一樣，一個 sub-layer 一個 dropout layer\n",
    "    self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "    self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "    \n",
    "  # 需要丟入 `training` 參數是因為 dropout 在訓練以及測試的行為有所不同\n",
    "  def call(self, x, training, mask):\n",
    "    # 除了 `attn`，其他張量的 shape 皆為 (batch_size, input_seq_len, d_model)\n",
    "    # attn.shape == (batch_size, num_heads, input_seq_len, input_seq_len)\n",
    "    \n",
    "    # sub-layer 1: MHA\n",
    "    # Encoder 利用注意機制關注自己當前的序列，因此 v, k, q 全部都是自己\n",
    "    # 另外別忘了我們還需要 padding mask 來遮住輸入序列中的 <pad> token\n",
    "    attn_output, attn = self.mha(x, x, x, mask)  \n",
    "    attn_output = self.dropout1(attn_output, training=training) \n",
    "    out1 = self.layernorm1(x + attn_output)  \n",
    "    \n",
    "    # sub-layer 2: FFN\n",
    "    ffn_output = self.ffn(out1) \n",
    "    ffn_output = self.dropout2(ffn_output, training=training)  # 記得 training\n",
    "    out2 = self.layernorm2(out1 + ffn_output)\n",
    "    \n",
    "    return out2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inp: tf.Tensor(\n",
      "[[8113  103    9 1066 7903 8114    0    0]\n",
      " [8113   16 4111 6735   12 2750 7903 8114]], shape=(2, 8), dtype=int64)\n",
      "--------------------\n",
      "padding_mask: tf.Tensor(\n",
      "[[[[0. 0. 0. 0. 0. 0. 1. 1.]]]\n",
      "\n",
      "\n",
      " [[[0. 0. 0. 0. 0. 0. 0. 0.]]]], shape=(2, 1, 1, 8), dtype=float32)\n",
      "--------------------\n",
      "emb_inp: tf.Tensor(\n",
      "[[[-0.02940199 -0.02748928  0.009587    0.02695702]\n",
      "  [ 0.02781546 -0.00051839 -0.04208876 -0.03737368]\n",
      "  [ 0.04085249 -0.01280526 -0.02142819  0.00932728]\n",
      "  [-0.04287729  0.00559031 -0.01610124  0.02254481]\n",
      "  [ 0.02117098 -0.03711741 -0.01554632  0.00051199]\n",
      "  [-0.00089258  0.03358604  0.04734136 -0.02150711]\n",
      "  [-0.01283282  0.03373785  0.0044058  -0.04340933]\n",
      "  [-0.01283282  0.03373785  0.0044058  -0.04340933]]\n",
      "\n",
      " [[-0.02940199 -0.02748928  0.009587    0.02695702]\n",
      "  [-0.02589555 -0.00853635  0.03899905  0.02993281]\n",
      "  [ 0.02303552 -0.0043821   0.03425056  0.02410031]\n",
      "  [-0.02790775  0.0453908  -0.00815301 -0.01390294]\n",
      "  [ 0.02987817  0.02534869  0.04416693 -0.03023093]\n",
      "  [ 0.03675361  0.04292214  0.0072703  -0.03425226]\n",
      "  [ 0.02117098 -0.03711741 -0.01554632  0.00051199]\n",
      "  [-0.00089258  0.03358604  0.04734136 -0.02150711]]], shape=(2, 8, 4), dtype=float32)\n",
      "--------------------\n",
      "enc_out: tf.Tensor(\n",
      "[[[-1.2790084  -0.4484179   0.2844689   1.4429574 ]\n",
      "  [ 0.8189746   0.89949    -1.5816016  -0.13686289]\n",
      "  [ 1.2944572  -0.2120691  -1.4614167   0.3790287 ]\n",
      "  [-1.4679327   1.1489065  -0.32746476  0.6464911 ]\n",
      "  [ 1.3575203  -1.3017013  -0.50809157  0.45227253]\n",
      "  [-0.4725885   1.4475989   0.2889553  -1.2639656 ]\n",
      "  [-0.2845264   1.6905513  -0.8935656  -0.5124593 ]\n",
      "  [-0.2845264   1.6905513  -0.8935656  -0.5124593 ]]\n",
      "\n",
      " [[-0.09594733 -1.3889819   0.05000272  1.4349266 ]\n",
      "  [-0.80072045 -1.177281    0.897902    1.0800993 ]\n",
      "  [ 1.1408303  -1.6084566   0.2656777   0.20194861]\n",
      "  [-0.73713     1.6235303  -0.90576535  0.01936519]\n",
      "  [ 1.3079005   0.28855962 -0.11566927 -1.4807909 ]\n",
      "  [ 1.1174244   0.8156742  -1.2963963  -0.63670236]\n",
      "  [ 1.3715899  -1.3202223  -0.45836774  0.40700006]\n",
      "  [ 0.38809723  0.7998333   0.524973   -1.7129036 ]]], shape=(2, 8, 4), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# 之後可以調的超參數。這邊為了 demo 設小一點\n",
    "d_model = 4\n",
    "num_heads = 2\n",
    "dff = 8\n",
    "\n",
    "# 新建一個使用上述參數的 Encoder Layer\n",
    "enc_layer = EncoderLayer(d_model, num_heads, dff)\n",
    "padding_mask = create_padding_mask(inp)  # 建立一個當前輸入 batch 使用的 padding mask\n",
    "enc_out = enc_layer(emb_inp, training=False, mask=padding_mask)  # (batch_size, seq_len, d_model)\n",
    "\n",
    "print(\"inp:\", inp)\n",
    "print(\"-\" * 20)\n",
    "print(\"padding_mask:\", padding_mask)\n",
    "print(\"-\" * 20)\n",
    "print(\"emb_inp:\", emb_inp)\n",
    "print(\"-\" * 20)\n",
    "print(\"enc_out:\", enc_out)\n",
    "assert emb_inp.shape == enc_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoder 裡頭會有 N 個 DecoderLayer，\n",
    "# 而 DecoderLayer 又有三個 sub-layers: 自注意的 MHA, 關注 Encoder 輸出的 MHA & FFN\n",
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        # 3 個 sub-layers 的主角們\n",
    "        self.mha1 = MultiHeadAttention(d_model, num_heads)\n",
    "        self.mha2 = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "        # 定義每個 sub-layer 用的 LayerNorm\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        # 定義每個 sub-layer 用的 Dropout\n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout3 = tf.keras.layers.Dropout(rate)\n",
    "    \n",
    "    \n",
    "    def call(self, x, enc_output, training, combined_mask, inp_padding_mask):\n",
    "        # 所有 sub-layers 的主要輸出皆為 (batch_size, target_seq_len, d_model)\n",
    "        # enc_output 為 Encoder 輸出序列，shape 為 (batch_size, input_seq_len, d_model)\n",
    "        # attn_weights_block_1 則為 (batch_size, num_heads, target_seq_len, target_seq_len)\n",
    "        # attn_weights_block_2 則為 (batch_size, num_heads, target_seq_len, input_seq_len)\n",
    "\n",
    "\n",
    "        # sub-layer 1: Decoder layer 自己對輸出序列做注意力。\n",
    "        # 我們同時需要 look ahead mask 以及輸出序列的 padding mask \n",
    "        # 來避免前面已生成的子詞關注到未來的子詞以及 <pad>\n",
    "        attn1, attn_weights_block1 = self.mha1(x, x, x, combined_mask)\n",
    "        attn1 = self.dropout1(attn1, training=training)\n",
    "        out1 = self.layernorm1(attn1 + x)\n",
    "\n",
    "        # sub-layer 2: Decoder layer 關注 Encoder 的最後輸出\n",
    "        # 記得我們一樣需要對 Encoder 的輸出套用 padding mask 避免關注到 <pad>\n",
    "        attn2, attn_weights_block2 = self.mha2(\n",
    "            enc_output, enc_output, out1, inp_padding_mask)  # (batch_size, target_seq_len, d_model)\n",
    "        attn2 = self.dropout2(attn2, training=training)\n",
    "        out2 = self.layernorm2(attn2 + out1)  # (batch_size, target_seq_len, d_model)\n",
    "\n",
    "        # sub-layer 3: FFN 部分跟 Encoder layer 完全一樣\n",
    "        ffn_output = self.ffn(out2)  # (batch_size, target_seq_len, d_model)\n",
    "\n",
    "        ffn_output = self.dropout3(ffn_output, training=training)\n",
    "        out3 = self.layernorm3(ffn_output + out2)  # (batch_size, target_seq_len, d_model)\n",
    "\n",
    "        # 除了主要輸出 `out3` 以外，輸出 multi-head 注意權重方便之後理解模型內部狀況\n",
    "        return out3, attn_weights_block1, attn_weights_block2    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tar: tf.Tensor(\n",
      "[[4205   10  241   86   27    3 4206    0    0    0]\n",
      " [4205  165  489  398  191   14    7  560    3 4206]], shape=(2, 10), dtype=int64)\n",
      "--------------------\n",
      "tar_padding_mask: tf.Tensor(\n",
      "[[[[0. 0. 0. 0. 0. 0. 0. 1. 1. 1.]]]\n",
      "\n",
      "\n",
      " [[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]]], shape=(2, 1, 1, 10), dtype=float32)\n",
      "--------------------\n",
      "look_ahead_mask: tf.Tensor(\n",
      "[[0. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [0. 0. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [0. 0. 0. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [0. 0. 0. 0. 1. 1. 1. 1. 1. 1.]\n",
      " [0. 0. 0. 0. 0. 1. 1. 1. 1. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 1. 1. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 1. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]], shape=(10, 10), dtype=float32)\n",
      "--------------------\n",
      "combined_mask: tf.Tensor(\n",
      "[[[[0. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "   [0. 0. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "   [0. 0. 0. 1. 1. 1. 1. 1. 1. 1.]\n",
      "   [0. 0. 0. 0. 1. 1. 1. 1. 1. 1.]\n",
      "   [0. 0. 0. 0. 0. 1. 1. 1. 1. 1.]\n",
      "   [0. 0. 0. 0. 0. 0. 1. 1. 1. 1.]\n",
      "   [0. 0. 0. 0. 0. 0. 0. 1. 1. 1.]\n",
      "   [0. 0. 0. 0. 0. 0. 0. 1. 1. 1.]\n",
      "   [0. 0. 0. 0. 0. 0. 0. 1. 1. 1.]\n",
      "   [0. 0. 0. 0. 0. 0. 0. 1. 1. 1.]]]\n",
      "\n",
      "\n",
      " [[[0. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "   [0. 0. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "   [0. 0. 0. 1. 1. 1. 1. 1. 1. 1.]\n",
      "   [0. 0. 0. 0. 1. 1. 1. 1. 1. 1.]\n",
      "   [0. 0. 0. 0. 0. 1. 1. 1. 1. 1.]\n",
      "   [0. 0. 0. 0. 0. 0. 1. 1. 1. 1.]\n",
      "   [0. 0. 0. 0. 0. 0. 0. 1. 1. 1.]\n",
      "   [0. 0. 0. 0. 0. 0. 0. 0. 1. 1.]\n",
      "   [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "   [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]]], shape=(2, 1, 10, 10), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "tar_padding_mask = create_padding_mask(tar)\n",
    "look_ahead_mask = create_look_ahead_mask(tar.shape[-1])\n",
    "combined_mask = tf.maximum(tar_padding_mask, look_ahead_mask)\n",
    "\n",
    "print(\"tar:\", tar)\n",
    "print(\"-\" * 20)\n",
    "print(\"tar_padding_mask:\", tar_padding_mask)\n",
    "print(\"-\" * 20)\n",
    "print(\"look_ahead_mask:\", look_ahead_mask)\n",
    "print(\"-\" * 20)\n",
    "print(\"combined_mask:\", combined_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emb_tar: tf.Tensor(\n",
      "[[[ 0.02309941  0.01578486 -0.03650398  0.04221227]\n",
      "  [ 0.02180279 -0.03836104 -0.0052525  -0.01320656]\n",
      "  [-0.0190685  -0.03266476 -0.03334127  0.04303731]\n",
      "  [ 0.03618959 -0.00743835  0.01616282  0.04101736]\n",
      "  [ 0.00099171  0.03080915 -0.03851545  0.03447074]\n",
      "  [-0.02579685  0.02527661  0.04145398 -0.04307295]\n",
      "  [-0.00221508  0.03029467  0.02829352  0.04788197]\n",
      "  [-0.02839622  0.04139889 -0.00255186 -0.03752811]\n",
      "  [-0.02839622  0.04139889 -0.00255186 -0.03752811]\n",
      "  [-0.02839622  0.04139889 -0.00255186 -0.03752811]]\n",
      "\n",
      " [[ 0.02309941  0.01578486 -0.03650398  0.04221227]\n",
      "  [-0.00205498  0.04796953 -0.00836701  0.03551394]\n",
      "  [-0.0049132  -0.03055991  0.03992986 -0.02458232]\n",
      "  [ 0.02458249 -0.02872031  0.01778464 -0.02678457]\n",
      "  [ 0.03532792  0.02878738 -0.00509523 -0.0492527 ]\n",
      "  [-0.00865723 -0.02110739 -0.03051426 -0.02639371]\n",
      "  [ 0.02155342 -0.00126004  0.00749636 -0.00912453]\n",
      "  [-0.01634679  0.00527266  0.01074455  0.02497052]\n",
      "  [-0.02579685  0.02527661  0.04145398 -0.04307295]\n",
      "  [-0.00221508  0.03029467  0.02829352  0.04788197]]], shape=(2, 10, 4), dtype=float32)\n",
      "--------------------\n",
      "enc_out: tf.Tensor(\n",
      "[[[-1.2790084  -0.4484179   0.2844689   1.4429574 ]\n",
      "  [ 0.8189746   0.89949    -1.5816016  -0.13686289]\n",
      "  [ 1.2944572  -0.2120691  -1.4614167   0.3790287 ]\n",
      "  [-1.4679327   1.1489065  -0.32746476  0.6464911 ]\n",
      "  [ 1.3575203  -1.3017013  -0.50809157  0.45227253]\n",
      "  [-0.4725885   1.4475989   0.2889553  -1.2639656 ]\n",
      "  [-0.2845264   1.6905513  -0.8935656  -0.5124593 ]\n",
      "  [-0.2845264   1.6905513  -0.8935656  -0.5124593 ]]\n",
      "\n",
      " [[-0.09594733 -1.3889819   0.05000272  1.4349266 ]\n",
      "  [-0.80072045 -1.177281    0.897902    1.0800993 ]\n",
      "  [ 1.1408303  -1.6084566   0.2656777   0.20194861]\n",
      "  [-0.73713     1.6235303  -0.90576535  0.01936519]\n",
      "  [ 1.3079005   0.28855962 -0.11566927 -1.4807909 ]\n",
      "  [ 1.1174244   0.8156742  -1.2963963  -0.63670236]\n",
      "  [ 1.3715899  -1.3202223  -0.45836774  0.40700006]\n",
      "  [ 0.38809723  0.7998333   0.524973   -1.7129036 ]]], shape=(2, 8, 4), dtype=float32)\n",
      "--------------------\n",
      "dec_out: tf.Tensor(\n",
      "[[[ 0.24520996  0.9124286  -1.6827396   0.52510095]\n",
      "  [ 1.4490802  -1.3585964  -0.20380807  0.11332427]\n",
      "  [-0.4028599  -0.530555   -0.7824085   1.7158233 ]\n",
      "  [ 0.8677076  -1.2873157  -0.65675914  1.0763671 ]\n",
      "  [-0.12391096  1.1748888  -1.5384061   0.48742837]\n",
      "  [-0.24596532  0.5464132   1.1906333  -1.4910812 ]\n",
      "  [-1.7202684   0.7570852   0.5242093   0.4389739 ]\n",
      "  [-0.02801281  1.3993088   0.05628167 -1.4275779 ]\n",
      "  [-0.02801275  1.3993089   0.05628166 -1.427578  ]\n",
      "  [-0.02801275  1.3993089   0.05628166 -1.427578  ]]\n",
      "\n",
      " [[ 0.11055017  1.2710681  -1.5327494   0.15113118]\n",
      "  [-0.36196986  1.7159778  -0.7391547  -0.614853  ]\n",
      "  [ 0.1960543  -1.0118155   1.5491637  -0.7334023 ]\n",
      "  [ 1.3901527  -1.410459    0.20747498 -0.18716866]\n",
      "  [ 0.93018496  0.9748236  -0.52242076 -1.3825879 ]\n",
      "  [ 1.5940356  -0.124051   -1.1611568  -0.30882782]\n",
      "  [ 1.412476   -1.2407097   0.388878   -0.5606442 ]\n",
      "  [-1.4752018  -0.31004715  1.151554    0.633695  ]\n",
      "  [-0.50473964  0.42634052  1.37344    -1.2950408 ]\n",
      "  [-1.6073794  -0.07197207  0.86243844  0.816913  ]]], shape=(2, 10, 4), dtype=float32)\n",
      "--------------------\n",
      "dec_self_attn_weights.shape: (2, 2, 10, 10)\n",
      "dec_enc_attn_weights: (2, 2, 10, 8)\n"
     ]
    }
   ],
   "source": [
    "# 超參數\n",
    "d_model = 4\n",
    "num_heads = 2\n",
    "dff = 8\n",
    "dec_layer = DecoderLayer(d_model, num_heads, dff)\n",
    "\n",
    "# 來源、目標語言的序列都需要 padding mask\n",
    "inp_padding_mask = create_padding_mask(inp)\n",
    "tar_padding_mask = create_padding_mask(tar)\n",
    "\n",
    "# masked MHA 用的遮罩，把 padding 跟未來子詞都蓋住\n",
    "look_ahead_mask = create_look_ahead_mask(tar.shape[-1])\n",
    "combined_mask = tf.maximum(tar_padding_mask, look_ahead_mask)\n",
    "\n",
    "# 實際初始一個 decoder layer 並做 3 個 sub-layers 的計算\n",
    "dec_out, dec_self_attn_weights, dec_enc_attn_weights = dec_layer(\n",
    "    emb_tar, enc_out, False, combined_mask, inp_padding_mask)\n",
    "\n",
    "print(\"emb_tar:\", emb_tar)\n",
    "print(\"-\" * 20)\n",
    "print(\"enc_out:\", enc_out)\n",
    "print(\"-\" * 20)\n",
    "print(\"dec_out:\", dec_out)\n",
    "assert emb_tar.shape == dec_out.shape\n",
    "print(\"-\" * 20)\n",
    "print(\"dec_self_attn_weights.shape:\", dec_self_attn_weights.shape)\n",
    "print(\"dec_enc_attn_weights:\", dec_enc_attn_weights.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 50, 512), dtype=float32, numpy=\n",
       "array([[[ 0.        ,  0.        ,  0.        , ...,  1.        ,\n",
       "          1.        ,  1.        ],\n",
       "        [ 0.84147096,  0.8218562 ,  0.8019618 , ...,  1.        ,\n",
       "          1.        ,  1.        ],\n",
       "        [ 0.9092974 ,  0.9364147 ,  0.95814437, ...,  1.        ,\n",
       "          1.        ,  1.        ],\n",
       "        ...,\n",
       "        [ 0.12357312,  0.97718984, -0.24295525, ...,  0.9999863 ,\n",
       "          0.99998724,  0.99998814],\n",
       "        [-0.76825464,  0.7312359 ,  0.63279754, ...,  0.9999857 ,\n",
       "          0.9999867 ,  0.9999876 ],\n",
       "        [-0.95375264, -0.14402692,  0.99899054, ...,  0.9999851 ,\n",
       "          0.9999861 ,  0.9999871 ]]], dtype=float32)>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 以下直接參考 TensorFlow 官方 tutorial \n",
    "def get_angles(pos, i, d_model):\n",
    "    angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
    "    return pos * angle_rates\n",
    "\n",
    "def positional_encoding(position, d_model):\n",
    "    angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
    "                          np.arange(d_model)[np.newaxis, :],\n",
    "                          d_model)\n",
    "\n",
    "    # apply sin to even indices in the array; 2i\n",
    "    sines = np.sin(angle_rads[:, 0::2])\n",
    "\n",
    "    # apply cos to odd indices in the array; 2i+1\n",
    "    cosines = np.cos(angle_rads[:, 1::2])\n",
    "\n",
    "    pos_encoding = np.concatenate([sines, cosines], axis=-1)\n",
    "\n",
    "    pos_encoding = pos_encoding[np.newaxis, ...]\n",
    "\n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)\n",
    "\n",
    "\n",
    "seq_len = 50\n",
    "d_model = 512\n",
    "\n",
    "pos_encoding = positional_encoding(seq_len, d_model)\n",
    "pos_encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.pcolormesh(pos_encoding[0], cmap='RdBu')\n",
    "plt.xlabel('d_model')\n",
    "plt.xlim((0, 512))\n",
    "plt.ylabel('Position')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "  # Encoder 的初始參數除了本來就要給 EncoderLayer 的參數還多了：\n",
    "  # - num_layers: 決定要有幾個 EncoderLayers, 前面影片中的 `N`\n",
    "  # - input_vocab_size: 用來把索引轉成詞嵌入向量\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, rate=0.1):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "\n",
    "        self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n",
    "        self.pos_encoding = positional_encoding(input_vocab_size, self.d_model)\n",
    "\n",
    "        # 建立 `num_layers` 個 EncoderLayers\n",
    "        self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) \n",
    "                           for _ in range(num_layers)]\n",
    "\n",
    "        self.dropout = tf.keras.layers.Dropout(rate)\n",
    "        \n",
    "\n",
    "    def call(self, x, training, mask):\n",
    "        # 輸入的 x.shape == (batch_size, input_seq_len)\n",
    "        # 以下各 layer 的輸出皆為 (batch_size, input_seq_len, d_model)\n",
    "        input_seq_len = tf.shape(x)[1]\n",
    "\n",
    "        # 將 2 維的索引序列轉成 3 維的詞嵌入張量，並依照論文乘上 sqrt(d_model)\n",
    "        # 再加上對應長度的位置編碼\n",
    "        x = self.embedding(x)\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        x += self.pos_encoding[:, :input_seq_len, :]\n",
    "\n",
    "        # 對 embedding 跟位置編碼的總合做 regularization\n",
    "        # 這在 Decoder 也會做\n",
    "        x = self.dropout(x, training=training)\n",
    "    \n",
    "        # 通過 N 個 EncoderLayer 做編碼\n",
    "        for i, enc_layer in enumerate(self.enc_layers):\n",
    "            x = enc_layer(x, training, mask)\n",
    "            # 以下只是用來 demo EncoderLayer outputs\n",
    "            #print('-' * 20)\n",
    "            #print(f\"EncoderLayer {i + 1}'s output:\", x)\n",
    "\n",
    "\n",
    "        return x        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inp: tf.Tensor(\n",
      "[[8113  103    9 1066 7903 8114    0    0]\n",
      " [8113   16 4111 6735   12 2750 7903 8114]], shape=(2, 8), dtype=int64)\n",
      "--------------------\n",
      "enc_out: tf.Tensor(\n",
      "[[[-0.7849332  -0.591968   -0.33270508  1.7096064 ]\n",
      "  [-0.50706536 -0.5110137  -0.7082318   1.7263108 ]\n",
      "  [-0.39270186 -0.03102632 -1.1583618   1.5820901 ]\n",
      "  [-0.5561631   0.38050288 -1.2407897   1.4164499 ]\n",
      "  [-0.90431994  0.19381054 -0.847289    1.5577984 ]\n",
      "  [-0.9732156  -0.22992761 -0.46524626  1.6683894 ]\n",
      "  [-0.84681964 -0.5434473  -0.31013626  1.7004031 ]\n",
      "  [-0.6243278  -0.5679047  -0.539001    1.7312335 ]]\n",
      "\n",
      " [[-0.7742376  -0.6076475  -0.32800585  1.7098908 ]\n",
      "  [-0.47978237 -0.56156063 -0.6860291   1.7273722 ]\n",
      "  [-0.30068296 -0.07366994 -1.1973958   1.5717487 ]\n",
      "  [-0.5147843   0.27872467 -1.229085    1.4651445 ]\n",
      "  [-0.8963448   0.26754576 -0.895411    1.52421   ]\n",
      "  [-0.9755362  -0.22618702 -0.4656963   1.6674196 ]\n",
      "  [-0.87600446 -0.54483974 -0.27099538  1.6918396 ]\n",
      "  [-0.6013047  -0.59936655 -0.5306772   1.7313485 ]]], shape=(2, 8, 4), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# 超參數\n",
    "num_layers = 2 # 2 層的 Encoder\n",
    "d_model = 4\n",
    "num_heads = 2\n",
    "dff = 8\n",
    "input_vocab_size = subword_encoder_en.vocab_size + 2 # 記得加上 <start>, <end>\n",
    "\n",
    "# 初始化一個 Encoder\n",
    "encoder = Encoder(num_layers, d_model, num_heads, dff, input_vocab_size)\n",
    "\n",
    "# 將 2 維的索引序列丟入 Encoder 做編碼\n",
    "enc_out = encoder(inp, training=False, mask=None)\n",
    "print(\"inp:\", inp)\n",
    "print(\"-\" * 20)\n",
    "print(\"enc_out:\", enc_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.layers.Layer):\n",
    "    # 初始參數跟 Encoder 只差在用 `target_vocab_size` 而非 `inp_vocab_size`\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size, rate=0.1):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "\n",
    "        # 為中文（目標語言）建立詞嵌入層\n",
    "        self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n",
    "        self.pos_encoding = positional_encoding(target_vocab_size, self.d_model)\n",
    "\n",
    "        self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate) \n",
    "                           for _ in range(num_layers)]\n",
    "        self.dropout = tf.keras.layers.Dropout(rate)\n",
    "        \n",
    "        \n",
    "    # 呼叫時的參數跟 DecoderLayer 一模一樣\n",
    "    def call(self, x, enc_output, training, combined_mask, inp_padding_mask):\n",
    "    \n",
    "        tar_seq_len = tf.shape(x)[1]\n",
    "        attention_weights = {}  # 用來存放每個 Decoder layer 的注意權重\n",
    "\n",
    "        # 這邊跟 Encoder 做的事情完全一樣\n",
    "        x = self.embedding(x)  # (batch_size, tar_seq_len, d_model)\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        x += self.pos_encoding[:, :tar_seq_len, :]\n",
    "        x = self.dropout(x, training=training)\n",
    "\n",
    "\n",
    "        for i, dec_layer in enumerate(self.dec_layers):\n",
    "            x, block1, block2 = dec_layer(x, enc_output, training,combined_mask, inp_padding_mask)\n",
    "\n",
    "            # 將從每個 Decoder layer 取得的注意權重全部存下來回傳，方便我們觀察\n",
    "            attention_weights['decoder_layer{}_block1'.format(i + 1)] = block1\n",
    "            attention_weights['decoder_layer{}_block2'.format(i + 1)] = block2\n",
    "\n",
    "        # x.shape == (batch_size, tar_seq_len, d_model)\n",
    "        return x, attention_weights        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tar: tf.Tensor(\n",
      "[[4205   10  241   86   27    3 4206    0    0    0]\n",
      " [4205  165  489  398  191   14    7  560    3 4206]], shape=(2, 10), dtype=int64)\n",
      "--------------------\n",
      "combined_mask: tf.Tensor(\n",
      "[[[[0. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "   [0. 0. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "   [0. 0. 0. 1. 1. 1. 1. 1. 1. 1.]\n",
      "   [0. 0. 0. 0. 1. 1. 1. 1. 1. 1.]\n",
      "   [0. 0. 0. 0. 0. 1. 1. 1. 1. 1.]\n",
      "   [0. 0. 0. 0. 0. 0. 1. 1. 1. 1.]\n",
      "   [0. 0. 0. 0. 0. 0. 0. 1. 1. 1.]\n",
      "   [0. 0. 0. 0. 0. 0. 0. 1. 1. 1.]\n",
      "   [0. 0. 0. 0. 0. 0. 0. 1. 1. 1.]\n",
      "   [0. 0. 0. 0. 0. 0. 0. 1. 1. 1.]]]\n",
      "\n",
      "\n",
      " [[[0. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "   [0. 0. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "   [0. 0. 0. 1. 1. 1. 1. 1. 1. 1.]\n",
      "   [0. 0. 0. 0. 1. 1. 1. 1. 1. 1.]\n",
      "   [0. 0. 0. 0. 0. 1. 1. 1. 1. 1.]\n",
      "   [0. 0. 0. 0. 0. 0. 1. 1. 1. 1.]\n",
      "   [0. 0. 0. 0. 0. 0. 0. 1. 1. 1.]\n",
      "   [0. 0. 0. 0. 0. 0. 0. 0. 1. 1.]\n",
      "   [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "   [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]]], shape=(2, 1, 10, 10), dtype=float32)\n",
      "--------------------\n",
      "enc_out: tf.Tensor(\n",
      "[[[-0.7849332  -0.591968   -0.33270508  1.7096064 ]\n",
      "  [-0.50706536 -0.5110137  -0.7082318   1.7263108 ]\n",
      "  [-0.39270186 -0.03102632 -1.1583618   1.5820901 ]\n",
      "  [-0.5561631   0.38050288 -1.2407897   1.4164499 ]\n",
      "  [-0.90431994  0.19381054 -0.847289    1.5577984 ]\n",
      "  [-0.9732156  -0.22992761 -0.46524626  1.6683894 ]\n",
      "  [-0.84681964 -0.5434473  -0.31013626  1.7004031 ]\n",
      "  [-0.6243278  -0.5679047  -0.539001    1.7312335 ]]\n",
      "\n",
      " [[-0.7742376  -0.6076475  -0.32800585  1.7098908 ]\n",
      "  [-0.47978237 -0.56156063 -0.6860291   1.7273722 ]\n",
      "  [-0.30068296 -0.07366994 -1.1973958   1.5717487 ]\n",
      "  [-0.5147843   0.27872467 -1.229085    1.4651445 ]\n",
      "  [-0.8963448   0.26754576 -0.895411    1.52421   ]\n",
      "  [-0.9755362  -0.22618702 -0.4656963   1.6674196 ]\n",
      "  [-0.87600446 -0.54483974 -0.27099538  1.6918396 ]\n",
      "  [-0.6013047  -0.59936655 -0.5306772   1.7313485 ]]], shape=(2, 8, 4), dtype=float32)\n",
      "--------------------\n",
      "inp_padding_mask: tf.Tensor(\n",
      "[[[[0. 0. 0. 0. 0. 0. 1. 1.]]]\n",
      "\n",
      "\n",
      " [[[0. 0. 0. 0. 0. 0. 0. 0.]]]], shape=(2, 1, 1, 8), dtype=float32)\n",
      "--------------------\n",
      "dec_out: tf.Tensor(\n",
      "[[[-0.56521416 -1.0581812   1.6000751   0.02332021]\n",
      "  [-0.340198   -1.2377603   1.5330343   0.04492389]\n",
      "  [ 0.3675253  -1.4228351   1.3287866  -0.27347657]\n",
      "  [ 0.09472068 -1.353683    1.455942   -0.19697979]\n",
      "  [-0.3839205  -1.0940721   1.6231283  -0.14513561]\n",
      "  [-0.41729766 -1.0276326   1.6514215  -0.20649134]\n",
      "  [-0.33023405 -1.0454822   1.6500466  -0.2743302 ]\n",
      "  [-0.19232097 -1.1254803   1.6149355  -0.29713422]\n",
      "  [ 0.4082282  -1.3586452   1.3515031  -0.40108618]\n",
      "  [ 0.19979605 -1.4183373   1.3857938  -0.16725263]]\n",
      "\n",
      " [[-0.5650454  -1.0544488   1.602678    0.01681651]\n",
      "  [-0.36043385 -1.2348609   1.5300139   0.06528082]\n",
      "  [ 0.24521802 -1.4295446   1.3651297  -0.18080297]\n",
      "  [-0.06483471 -1.3449187   1.4773033  -0.06755004]\n",
      "  [-0.41885287 -1.0775514   1.6267892  -0.13038504]\n",
      "  [-0.400182   -1.0338532   1.650498   -0.21646306]\n",
      "  [-0.35319278 -1.0375832   1.652348   -0.26157212]\n",
      "  [-0.24463181 -1.1371143   1.6107953  -0.22904916]\n",
      "  [ 0.19615427 -1.3627281   1.4271016  -0.26052776]\n",
      "  [ 0.08419981 -1.3687493   1.4467621  -0.16221273]]], shape=(2, 10, 4), dtype=float32)\n",
      "--------------------\n",
      "decoder_layer1_block1.shape: (2, 2, 10, 10)\n",
      "decoder_layer1_block2.shape: (2, 2, 10, 8)\n",
      "decoder_layer2_block1.shape: (2, 2, 10, 10)\n",
      "decoder_layer2_block2.shape: (2, 2, 10, 8)\n"
     ]
    }
   ],
   "source": [
    "# 超參數\n",
    "num_layers = 2 # 2 層的 Decoder\n",
    "d_model = 4\n",
    "num_heads = 2\n",
    "dff = 8\n",
    "target_vocab_size = subword_encoder_zh.vocab_size + 2 # 記得加上 <start>, <end>\n",
    "\n",
    "# 遮罩\n",
    "inp_padding_mask = create_padding_mask(inp)\n",
    "tar_padding_mask = create_padding_mask(tar)\n",
    "look_ahead_mask = create_look_ahead_mask(tar.shape[1])\n",
    "combined_mask = tf.math.maximum(tar_padding_mask, look_ahead_mask)\n",
    "\n",
    "# 初始化一個 Decoder\n",
    "decoder = Decoder(num_layers, d_model, num_heads, dff, target_vocab_size)\n",
    "\n",
    "# 將 2 維的索引序列以及遮罩丟入 Decoder\n",
    "print(\"tar:\", tar)\n",
    "print(\"-\" * 20)\n",
    "print(\"combined_mask:\", combined_mask)\n",
    "print(\"-\" * 20)\n",
    "print(\"enc_out:\", enc_out)\n",
    "print(\"-\" * 20)\n",
    "print(\"inp_padding_mask:\", inp_padding_mask)\n",
    "print(\"-\" * 20)\n",
    "dec_out, attn = decoder(tar, enc_out, training=False, \n",
    "                        combined_mask=combined_mask,\n",
    "                        inp_padding_mask=inp_padding_mask)\n",
    "print(\"dec_out:\", dec_out)\n",
    "print(\"-\" * 20)\n",
    "for block_name, attn_weights in attn.items():\n",
    "    print(f\"{block_name}.shape: {attn_weights.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer 之上已經沒有其他 layers 了，我們使用 tf.keras.Model 建立一個模型\n",
    "class Transformer(tf.keras.Model):\n",
    "    # 初始參數包含 Encoder & Decoder 都需要超參數以及中英字典數目\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, target_vocab_size, rate=0.1):\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        self.encoder = Encoder(num_layers, d_model, num_heads, dff, \n",
    "                               input_vocab_size, rate)\n",
    "\n",
    "        self.decoder = Decoder(num_layers, d_model, num_heads, dff, \n",
    "                               target_vocab_size, rate)\n",
    "        # 這個 FFN 輸出跟中文字典一樣大的 logits 數，等通過 softmax 就代表每個中文字的出現機率\n",
    "        self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
    "  \n",
    "    # enc_padding_mask 跟 dec_padding_mask 都是英文序列的 padding mask，\n",
    "    # 只是一個給 Encoder layer 的 MHA 用，一個是給 Decoder layer 的 MHA 2 使用\n",
    "    def call(self, inp, tar, training, enc_padding_mask, combined_mask, dec_padding_mask):\n",
    "\n",
    "        enc_output = self.encoder(inp, training, enc_padding_mask)  # (batch_size, inp_seq_len, d_model)\n",
    "\n",
    "        # dec_output.shape == (batch_size, tar_seq_len, d_model)\n",
    "        dec_output, attention_weights = self.decoder(\n",
    "            tar, enc_output, training, combined_mask, dec_padding_mask)\n",
    "\n",
    "        # 將 Decoder 輸出通過最後一個 linear layer\n",
    "        final_output = self.final_layer(dec_output)  # (batch_size, tar_seq_len, target_vocab_size)\n",
    "\n",
    "        return final_output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tar: tf.Tensor(\n",
      "[[4205   10  241   86   27    3 4206    0    0    0]\n",
      " [4205  165  489  398  191   14    7  560    3 4206]], shape=(2, 10), dtype=int64)\n",
      "--------------------\n",
      "tar_inp: tf.Tensor(\n",
      "[[4205   10  241   86   27    3 4206    0    0]\n",
      " [4205  165  489  398  191   14    7  560    3]], shape=(2, 9), dtype=int64)\n",
      "--------------------\n",
      "tar_real: tf.Tensor(\n",
      "[[  10  241   86   27    3 4206    0    0    0]\n",
      " [ 165  489  398  191   14    7  560    3 4206]], shape=(2, 9), dtype=int64)\n",
      "--------------------\n",
      "predictions: tf.Tensor(\n",
      "[[[ 0.01349578 -0.00199539 -0.00217387 ... -0.03862738 -0.03212878\n",
      "   -0.07692746]\n",
      "  [ 0.037483    0.01585472 -0.02548709 ... -0.04276202 -0.02495992\n",
      "   -0.05491883]\n",
      "  [ 0.05718527  0.0288353  -0.04577482 ... -0.0450176  -0.01315335\n",
      "   -0.03639909]\n",
      "  ...\n",
      "  [ 0.01202047 -0.00400385 -0.00099438 ... -0.03859971 -0.03085512\n",
      "   -0.07979749]\n",
      "  [ 0.02357969  0.00501019 -0.0119309  ... -0.04091505 -0.02892826\n",
      "   -0.06939011]\n",
      "  [ 0.04867784  0.02382022 -0.03683803 ... -0.04392422 -0.01941058\n",
      "   -0.04347048]]\n",
      "\n",
      " [[ 0.01676658 -0.00080312 -0.00556348 ... -0.03981712 -0.0293731\n",
      "   -0.07665334]\n",
      "  [ 0.03873826  0.01607162 -0.02685272 ... -0.04328423 -0.0234593\n",
      "   -0.0552263 ]\n",
      "  [ 0.0564083   0.02865588 -0.04492006 ... -0.04475704 -0.014088\n",
      "   -0.03639094]\n",
      "  ...\n",
      "  [ 0.01514175 -0.00298802 -0.0042616  ... -0.0397689  -0.02800198\n",
      "   -0.0797462 ]\n",
      "  [ 0.02867932  0.00800282 -0.01704067 ... -0.04215823 -0.02618419\n",
      "   -0.06638923]\n",
      "  [ 0.0505631   0.02489875 -0.03880979 ... -0.04421616 -0.01803543\n",
      "   -0.04204436]]], shape=(2, 9, 4207), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# 超參數\n",
    "num_layers = 1\n",
    "d_model = 4\n",
    "num_heads = 2\n",
    "dff = 8\n",
    "\n",
    "# + 2 是為了 <start> & <end> token\n",
    "input_vocab_size = subword_encoder_en.vocab_size + 2\n",
    "output_vocab_size = subword_encoder_zh.vocab_size + 2\n",
    "\n",
    "# 重點中的重點。訓練時用前一個字來預測下一個中文字\n",
    "tar_inp = tar[:, :-1]\n",
    "tar_real = tar[:, 1:]\n",
    "\n",
    "# 來源 / 目標語言用的遮罩。注意 `comined_mask` 已經將目標語言的兩種遮罩合而為一\n",
    "inp_padding_mask = create_padding_mask(inp)\n",
    "tar_padding_mask = create_padding_mask(tar_inp)\n",
    "look_ahead_mask = create_look_ahead_mask(tar_inp.shape[1])\n",
    "combined_mask = tf.math.maximum(tar_padding_mask, look_ahead_mask)\n",
    "\n",
    "# 初始化我們的第一個 transformer\n",
    "transformer = Transformer(num_layers, d_model, num_heads, dff, \n",
    "                          input_vocab_size, output_vocab_size)\n",
    "\n",
    "# 將英文、中文序列丟入取得 Transformer 預測下個中文字的結果\n",
    "predictions, attn_weights = transformer(inp, tar_inp, False, inp_padding_mask, \n",
    "                                        combined_mask, inp_padding_mask)\n",
    "print(\"tar:\", tar)\n",
    "print(\"-\" * 20)\n",
    "print(\"tar_inp:\", tar_inp)\n",
    "print(\"-\" * 20)\n",
    "print(\"tar_real:\", tar_real)\n",
    "print(\"-\" * 20)\n",
    "print(\"predictions:\", predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3,), dtype=float32, numpy=array([0.31326166, 0.31326166, 1.3132616 ], dtype=float32)>"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "# 假設我們要解的是一個 binary classifcation， 0 跟 1 個代表一個 label\n",
    "real = tf.constant([1, 1, 0], shape=(1, 3), dtype=tf.float32)\n",
    "pred = tf.constant([[0, 1], [0, 1], [0, 1]], dtype=tf.float32)\n",
    "loss_object(real, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions: tf.Tensor(\n",
      "[[[ 0.01349578 -0.00199539 -0.00217387 ... -0.03862738 -0.03212878\n",
      "   -0.07692746]\n",
      "  [ 0.037483    0.01585472 -0.02548709 ... -0.04276202 -0.02495992\n",
      "   -0.05491883]\n",
      "  [ 0.05718527  0.0288353  -0.04577482 ... -0.0450176  -0.01315335\n",
      "   -0.03639909]\n",
      "  ...\n",
      "  [ 0.01202047 -0.00400385 -0.00099438 ... -0.03859971 -0.03085512\n",
      "   -0.07979749]\n",
      "  [ 0.02357969  0.00501019 -0.0119309  ... -0.04091505 -0.02892826\n",
      "   -0.06939011]\n",
      "  [ 0.04867784  0.02382022 -0.03683803 ... -0.04392422 -0.01941058\n",
      "   -0.04347048]]\n",
      "\n",
      " [[ 0.01676658 -0.00080312 -0.00556348 ... -0.03981712 -0.0293731\n",
      "   -0.07665334]\n",
      "  [ 0.03873826  0.01607162 -0.02685272 ... -0.04328423 -0.0234593\n",
      "   -0.0552263 ]\n",
      "  [ 0.0564083   0.02865588 -0.04492006 ... -0.04475704 -0.014088\n",
      "   -0.03639094]\n",
      "  ...\n",
      "  [ 0.01514175 -0.00298802 -0.0042616  ... -0.0397689  -0.02800198\n",
      "   -0.0797462 ]\n",
      "  [ 0.02867932  0.00800282 -0.01704067 ... -0.04215823 -0.02618419\n",
      "   -0.06638923]\n",
      "  [ 0.0505631   0.02489875 -0.03880979 ... -0.04421616 -0.01803543\n",
      "   -0.04204436]]], shape=(2, 9, 4207), dtype=float32)\n",
      "--------------------\n",
      "tf.Tensor(\n",
      "[[1.376189  2.9352083 3.868732  3.4191108 2.608355  1.5664341 1.1489887\n",
      "  1.9882673 3.5525477]\n",
      " [1.4309777 2.921915  3.873899  3.5009158 2.649916  1.6611692 1.1839204\n",
      "  2.2150593 3.6206648]], shape=(2, 9), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(\"predictions:\", predictions)\n",
    "print(\"-\" * 20)\n",
    "print(tf.reduce_sum(predictions, axis=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(real, pred):\n",
    "    # 這次的 mask 將序列中不等於 0 的位置視為 1，其餘為 0 \n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    # 照樣計算所有位置的 cross entropy 但不加總\n",
    "    loss_ = loss_object(real, pred)\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask  # 只計算非 <pad> 位置的損失 \n",
    "\n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(\n",
    "    name='train_accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_vocab_size: 8115\n",
      "target_vocab_size: 4207\n"
     ]
    }
   ],
   "source": [
    "num_layers = 4 \n",
    "d_model = 128\n",
    "dff = 512\n",
    "num_heads = 8\n",
    "\n",
    "input_vocab_size = subword_encoder_en.vocab_size + 2\n",
    "target_vocab_size = subword_encoder_zh.vocab_size + 2\n",
    "dropout_rate = 0.1  # 預設值\n",
    "\n",
    "print(\"input_vocab_size:\", input_vocab_size)\n",
    "print(\"target_vocab_size:\", target_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    # 論文預設 `warmup_steps` = 4000\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super(CustomSchedule, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "\n",
    "        self.warmup_steps = warmup_steps\n",
    "    \n",
    "    def __call__(self, step):\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "\n",
    "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
    "  \n",
    "# 將客製化 learning rate schdeule 丟入 Adam opt.\n",
    "# Adam opt. 的參數都跟論文相同\n",
    "learning_rate = CustomSchedule(d_model)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, \n",
    "                                     epsilon=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'Train Step')"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "d_models = [128, 256, 512]\n",
    "warmup_steps = [1000 * i for i in range(1, 4)]\n",
    "\n",
    "schedules = []\n",
    "labels = []\n",
    "colors = [\"blue\", \"red\", \"black\"]\n",
    "for d in d_models:\n",
    "    schedules += [CustomSchedule(d, s) for s in warmup_steps]\n",
    "    labels += [f\"d_model: {d}, warm: {s}\" for s in warmup_steps]\n",
    "\n",
    "for i, (schedule, label) in enumerate(zip(schedules, labels)):\n",
    "    plt.plot(schedule(tf.range(10000, dtype=tf.float32)), \n",
    "           label=label, color=colors[i // 3])\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.ylabel(\"Learning Rate\")\n",
    "plt.xlabel(\"Train Step\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "這個 Transformer 有 4 層 Encoder / Decoder layers\n",
      "d_model: 128\n",
      "num_heads: 8\n",
      "dff: 512\n",
      "input_vocab_size: 8115\n",
      "target_vocab_size: 4207\n",
      "dropout_rate: 0.1\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transformer = Transformer(num_layers, d_model, num_heads, dff,\n",
    "                          input_vocab_size, target_vocab_size, dropout_rate)\n",
    "\n",
    "print(f\"\"\"這個 Transformer 有 {num_layers} 層 Encoder / Decoder layers\n",
    "d_model: {d_model}\n",
    "num_heads: {num_heads}\n",
    "dff: {dff}\n",
    "input_vocab_size: {input_vocab_size}\n",
    "target_vocab_size: {target_vocab_size}\n",
    "dropout_rate: {dropout_rate}\n",
    "\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "沒找到 checkpoint，從頭訓練。\n"
     ]
    }
   ],
   "source": [
    "train_perc = 20\n",
    "val_prec = 1\n",
    "drop_prec = 100 - train_perc - val_prec\n",
    "\n",
    "# 方便比較不同實驗/ 不同超參數設定的結果\n",
    "run_id = f\"{num_layers}layers_{d_model}d_{num_heads}heads_{dff}dff_{train_perc}train_perc\"\n",
    "checkpoint_path = os.path.join(checkpoint_path, run_id)\n",
    "log_dir = os.path.join(log_dir, run_id)\n",
    "\n",
    "# tf.train.Checkpoint 可以幫我們把想要存下來的東西整合起來，方便儲存與讀取\n",
    "# 一般來說你會想存下模型以及 optimizer 的狀態\n",
    "ckpt = tf.train.Checkpoint(transformer=transformer,\n",
    "                           optimizer=optimizer)\n",
    "\n",
    "# ckpt_manager 會去 checkpoint_path 看有沒有符合 ckpt 裡頭定義的東西\n",
    "# 存檔的時候只保留最近 5 次 checkpoints，其他自動刪除\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
    "\n",
    "# 如果在 checkpoint 路徑上有發現檔案就讀進來\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "\n",
    "    # 用來確認之前訓練多少 epochs 了\n",
    "    last_epoch = int(ckpt_manager.latest_checkpoint.split(\"-\")[-1])\n",
    "    print(f'已讀取最新的 checkpoint，模型已訓練 {last_epoch} epochs。')\n",
    "else:\n",
    "    last_epoch = 0\n",
    "    print(\"沒找到 checkpoint，從頭訓練。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 為 Transformer 的 Encoder / Decoder 準備遮罩\n",
    "def create_masks(inp, tar):\n",
    "    # 英文句子的 padding mask，要交給 Encoder layer 自注意力機制用的\n",
    "    enc_padding_mask = create_padding_mask(inp)\n",
    "\n",
    "    # 同樣也是英文句子的 padding mask，但是是要交給 Decoder layer 的 MHA 2 \n",
    "    # 關注 Encoder 輸出序列用的\n",
    "    dec_padding_mask = create_padding_mask(inp)\n",
    "\n",
    "    # Decoder layer 的 MHA1 在做自注意力機制用的\n",
    "    # `combined_mask` 是中文句子的 padding mask 跟 look ahead mask 的疊加\n",
    "    look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n",
    "    dec_target_padding_mask = create_padding_mask(tar)\n",
    "    combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
    "\n",
    "    return enc_padding_mask, combined_mask, dec_padding_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_step_signature = [\n",
    "    tf.TensorSpec(shape=(None, None), dtype=tf.int64),\n",
    "    tf.TensorSpec(shape=(None, None), dtype=tf.int64),\n",
    "]\n",
    "\n",
    "@tf.function  # 讓 TensorFlow 幫我們將 eager code 優化並加快運算\n",
    "def train_step(inp, tar):\n",
    "    # 前面說過的，用去尾的原始序列去預測下一個字的序列\n",
    "    tar_inp = tar[:, :-1]\n",
    "    tar_real = tar[:, 1:]\n",
    "\n",
    "    # 建立 3 個遮罩\n",
    "    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)\n",
    "\n",
    "    # 紀錄 Transformer 的所有運算過程以方便之後做梯度下降\n",
    "    with tf.GradientTape() as tape:\n",
    "        # 注意是丟入 `tar_inp` 而非 `tar`。記得將 `training` 參數設定為 True\n",
    "        predictions, _ = transformer(inp, tar_inp, \n",
    "                                     True, \n",
    "                                     enc_padding_mask, \n",
    "                                     combined_mask, \n",
    "                                     dec_padding_mask)\n",
    "        # 跟影片中顯示的相同，計算左移一個字的序列跟模型預測分佈之間的差異，當作 loss\n",
    "        loss = loss_function(tar_real, predictions)\n",
    "\n",
    "    # 取出梯度並呼叫前面定義的 Adam optimizer 幫我們更新 Transformer 裡頭可訓練的參數\n",
    "    gradients = tape.gradient(loss, transformer.trainable_variables)    \n",
    "    optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
    "\n",
    "    # 將 loss 以及訓練 acc 記錄到 TensorBoard 上，非必要\n",
    "    train_loss(loss)\n",
    "    train_accuracy(tar_real, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "此超參數組合的 Transformer 已經訓練 0 epochs。\n",
      "剩餘 epochs：-30\n",
      "WARNING:tensorflow:5 out of the last 6 calls to <function train_step at 0x0000021049D7A550> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 6 calls to <function train_step at 0x0000021049D7A550> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving checkpoint for epoch 1 at nmt\\checkpoints\\4layers_128d_8heads_512dff_20train_perc\\ckpt-1\n",
      "Epoch 1 Loss 5.1823 Accuracy 0.0221\n",
      "Time taken for 1 epoch: 456.5564181804657 secs\n",
      "\n",
      "Saving checkpoint for epoch 2 at nmt\\checkpoints\\4layers_128d_8heads_512dff_20train_perc\\ckpt-2\n",
      "Epoch 2 Loss 4.2416 Accuracy 0.0601\n",
      "Time taken for 1 epoch: 453.46982884407043 secs\n",
      "\n",
      "Saving checkpoint for epoch 3 at nmt\\checkpoints\\4layers_128d_8heads_512dff_20train_perc\\ckpt-3\n",
      "Epoch 3 Loss 3.7431 Accuracy 0.0990\n",
      "Time taken for 1 epoch: 446.1782886981964 secs\n",
      "\n",
      "Saving checkpoint for epoch 4 at nmt\\checkpoints\\4layers_128d_8heads_512dff_20train_perc\\ckpt-4\n",
      "Epoch 4 Loss 3.2646 Accuracy 0.1510\n",
      "Time taken for 1 epoch: 453.1815617084503 secs\n",
      "\n",
      "Saving checkpoint for epoch 5 at nmt\\checkpoints\\4layers_128d_8heads_512dff_20train_perc\\ckpt-5\n",
      "Epoch 5 Loss 2.9631 Accuracy 0.1811\n",
      "Time taken for 1 epoch: 452.84260416030884 secs\n",
      "\n",
      "Saving checkpoint for epoch 6 at nmt\\checkpoints\\4layers_128d_8heads_512dff_20train_perc\\ckpt-6\n",
      "Epoch 6 Loss 2.7771 Accuracy 0.1984\n",
      "Time taken for 1 epoch: 454.5953063964844 secs\n",
      "\n",
      "Saving checkpoint for epoch 7 at nmt\\checkpoints\\4layers_128d_8heads_512dff_20train_perc\\ckpt-7\n",
      "Epoch 7 Loss 2.6356 Accuracy 0.2120\n",
      "Time taken for 1 epoch: 462.6776821613312 secs\n",
      "\n",
      "Saving checkpoint for epoch 8 at nmt\\checkpoints\\4layers_128d_8heads_512dff_20train_perc\\ckpt-8\n",
      "Epoch 8 Loss 2.5192 Accuracy 0.2239\n",
      "Time taken for 1 epoch: 457.89526534080505 secs\n",
      "\n",
      "Saving checkpoint for epoch 9 at nmt\\checkpoints\\4layers_128d_8heads_512dff_20train_perc\\ckpt-9\n",
      "Epoch 9 Loss 2.4192 Accuracy 0.2349\n",
      "Time taken for 1 epoch: 492.7595705986023 secs\n",
      "\n",
      "Saving checkpoint for epoch 10 at nmt\\checkpoints\\4layers_128d_8heads_512dff_20train_perc\\ckpt-10\n",
      "Epoch 10 Loss 2.3208 Accuracy 0.2455\n",
      "Time taken for 1 epoch: 453.71798396110535 secs\n",
      "\n",
      "Saving checkpoint for epoch 11 at nmt\\checkpoints\\4layers_128d_8heads_512dff_20train_perc\\ckpt-11\n",
      "Epoch 11 Loss 2.2200 Accuracy 0.2580\n",
      "Time taken for 1 epoch: 461.741801738739 secs\n",
      "\n",
      "Saving checkpoint for epoch 12 at nmt\\checkpoints\\4layers_128d_8heads_512dff_20train_perc\\ckpt-12\n",
      "Epoch 12 Loss 2.1255 Accuracy 0.2699\n",
      "Time taken for 1 epoch: 504.74473667144775 secs\n",
      "\n",
      "Saving checkpoint for epoch 13 at nmt\\checkpoints\\4layers_128d_8heads_512dff_20train_perc\\ckpt-13\n",
      "Epoch 13 Loss 2.0340 Accuracy 0.2822\n",
      "Time taken for 1 epoch: 531.6135003566742 secs\n",
      "\n",
      "Saving checkpoint for epoch 14 at nmt\\checkpoints\\4layers_128d_8heads_512dff_20train_perc\\ckpt-14\n",
      "Epoch 14 Loss 1.9499 Accuracy 0.2941\n",
      "Time taken for 1 epoch: 528.3129043579102 secs\n",
      "\n",
      "Saving checkpoint for epoch 15 at nmt\\checkpoints\\4layers_128d_8heads_512dff_20train_perc\\ckpt-15\n",
      "Epoch 15 Loss 1.8722 Accuracy 0.3048\n",
      "Time taken for 1 epoch: 527.085098028183 secs\n",
      "\n",
      "Saving checkpoint for epoch 16 at nmt\\checkpoints\\4layers_128d_8heads_512dff_20train_perc\\ckpt-16\n",
      "Epoch 16 Loss 1.8010 Accuracy 0.3155\n",
      "Time taken for 1 epoch: 518.8096520900726 secs\n",
      "\n",
      "Saving checkpoint for epoch 17 at nmt\\checkpoints\\4layers_128d_8heads_512dff_20train_perc\\ckpt-17\n",
      "Epoch 17 Loss 1.7347 Accuracy 0.3247\n",
      "Time taken for 1 epoch: 562.3070707321167 secs\n",
      "\n",
      "Saving checkpoint for epoch 18 at nmt\\checkpoints\\4layers_128d_8heads_512dff_20train_perc\\ckpt-18\n",
      "Epoch 18 Loss 1.6734 Accuracy 0.3334\n",
      "Time taken for 1 epoch: 553.7411601543427 secs\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 定義我們要看幾遍數據集\n",
    "EPOCHS = 30\n",
    "print(f\"此超參數組合的 Transformer 已經訓練 {last_epoch} epochs。\")\n",
    "print(f\"剩餘 epochs：{min(0, last_epoch - EPOCHS)}\")\n",
    "\n",
    "\n",
    "# 用來寫資訊到 TensorBoard，非必要但十分推薦\n",
    "summary_writer = tf.summary.create_file_writer(log_dir)\n",
    "\n",
    "# 比對設定的 `EPOCHS` 以及已訓練的 `last_epoch` 來決定還要訓練多少 epochs\n",
    "for epoch in range(last_epoch, EPOCHS):\n",
    "    start = time.time()\n",
    "\n",
    "    # 重置紀錄 TensorBoard 的 metrics\n",
    "    train_loss.reset_states()\n",
    "    train_accuracy.reset_states()\n",
    "\n",
    "    # 一個 epoch 就是把我們定義的訓練資料集一個一個 batch 拿出來處理，直到看完整個數據集 \n",
    "    for (step_idx, (inp, tar)) in enumerate(train_dataset):\n",
    "    \n",
    "        # 每次 step 就是將數據丟入 Transformer，讓它生預測結果並計算梯度最小化 loss\n",
    "        train_step(inp, tar)  \n",
    "\n",
    "    # 每個 epoch 完成就存一次檔    \n",
    "    if (epoch + 1) % 1 == 0:\n",
    "        ckpt_save_path = ckpt_manager.save()\n",
    "        print ('Saving checkpoint for epoch {} at {}'.format(epoch+1,ckpt_save_path))\n",
    "    \n",
    "    \n",
    "    # 將 loss 以及 accuracy 寫到 TensorBoard 上\n",
    "    with summary_writer.as_default():\n",
    "        tf.summary.scalar(\"train_loss\", train_loss.result(), step=epoch + 1)\n",
    "        tf.summary.scalar(\"train_acc\", train_accuracy.result(), step=epoch + 1)\n",
    "  \n",
    "    print('Epoch {} Loss {:.4f} Accuracy {:.4f}'.format(epoch + 1, \n",
    "                                                train_loss.result(), \n",
    "                                                train_accuracy.result()))\n",
    "    print('Time taken for 1 epoch: {} secs\\n'.format(time.time() - start))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 給定一個英文句子，輸出預測的中文索引數字序列以及注意權重 dict\n",
    "def evaluate(inp_sentence):\n",
    "  \n",
    "    # 準備英文句子前後會加上的 <start>, <end>\n",
    "    start_token = [subword_encoder_en.vocab_size]\n",
    "    end_token = [subword_encoder_en.vocab_size + 1]\n",
    "\n",
    "    # inp_sentence 是字串，我們用 Subword Tokenizer 將其變成子詞的索引序列\n",
    "    # 並在前後加上 BOS / EOS\n",
    "    inp_sentence = start_token + subword_encoder_en.encode(inp_sentence) + end_token\n",
    "    encoder_input = tf.expand_dims(inp_sentence, 0)\n",
    "\n",
    "    # 跟我們在影片裡看到的一樣，Decoder 在第一個時間點吃進去的輸入\n",
    "    # 是一個只包含一個中文 <start> token 的序列\n",
    "    decoder_input = [subword_encoder_zh.vocab_size]\n",
    "    output = tf.expand_dims(decoder_input, 0)  # 增加 batch 維度\n",
    "\n",
    "    # auto-regressive，一次生成一個中文字並將預測加到輸入再度餵進 Transformer\n",
    "    for i in range(MAX_LENGTH):\n",
    "        # 每多一個生成的字就得產生新的遮罩\n",
    "        enc_padding_mask, combined_mask, dec_padding_mask = create_masks(\n",
    "            encoder_input, output)\n",
    "  \n",
    "        # predictions.shape == (batch_size, seq_len, vocab_size)\n",
    "        predictions, attention_weights = transformer(encoder_input, \n",
    "                                                     output,\n",
    "                                                     False,\n",
    "                                                     enc_padding_mask,\n",
    "                                                     combined_mask,\n",
    "                                                     dec_padding_mask)\n",
    "        \n",
    "        \n",
    "        # 將序列中最後一個 distribution 取出，並將裡頭值最大的當作模型最新的預測字\n",
    "        predictions = predictions[: , -1:, :]  # (batch_size, 1, vocab_size)\n",
    "\n",
    "        predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
    "\n",
    "        # 遇到 <end> token 就停止回傳，代表模型已經產生完結果\n",
    "        if tf.equal(predicted_id, subword_encoder_zh.vocab_size + 1):\n",
    "              return tf.squeeze(output, axis=0), attention_weights\n",
    "\n",
    "        #將 Transformer 新預測的中文索引加到輸出序列中，讓 Decoder 可以在產生\n",
    "        # 下個中文字的時候關注到最新的 `predicted_id`\n",
    "        output = tf.concat([output, predicted_id], axis=-1)  \n",
    "        \n",
    "# 將 batch 的維度去掉後回傳預測的中文索引序列\n",
    "return tf.squeeze(output, axis=0), attention_weights        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 要被翻譯的英文句子\n",
    "sentence = \"China, India, and others have enjoyed continuing economic growth.\"\n",
    "\n",
    "# 取得預測的中文索引序列\n",
    "predicted_seq, _ = evaluate(sentence)\n",
    "\n",
    "# 過濾掉 <start> & <end> tokens 並用中文的 subword tokenizer 幫我們將索引序列還原回中文句子\n",
    "target_vocab_size = subword_encoder_zh.vocab_size\n",
    "predicted_seq_without_bos_eos = [idx for idx in predicted_seq if idx < target_vocab_size]\n",
    "predicted_sentence = subword_encoder_zh.decode(predicted_seq_without_bos_eos)\n",
    "\n",
    "print(\"sentence:\", sentence)\n",
    "print(\"-\" * 20)\n",
    "print(\"predicted_seq:\", predicted_seq)\n",
    "print(\"-\" * 20)\n",
    "print(\"predicted_sentence:\", predicted_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
