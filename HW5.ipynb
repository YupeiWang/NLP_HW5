{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from pprint import pprint\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 uninstall tensorflow-gpu==2.0.0-beta0\n",
    "!pip3 uninstall -y tensorflow\n",
    "!pip3 uninstall keras\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorboard~=2.6->tensorflow) (2.24.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorboard~=2.6->tensorflow) (3.3.4)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorboard~=2.6->tensorflow) (1.32.1)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorboard~=2.6->tensorflow) (1.0.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorboard~=2.6->tensorflow) (1.8.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorboard~=2.6->tensorflow) (0.4.4)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorboard~=2.6->tensorflow) (50.3.1.post20201107)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (2020.6.20)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (1.25.11)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow) (4.2.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in c:\\users\\user\\anaconda3\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow) (4.7.2)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow) (1.3.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\user\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow) (3.1.1)\n",
      "Installing collected packages: keras, clang, grpcio, tensorboard, tensorflow-estimator, tensorflow\n",
      "  Attempting uninstall: grpcio\n",
      "    Found existing installation: grpcio 1.34.1\n",
      "    Uninstalling grpcio-1.34.1:\n",
      "      Successfully uninstalled grpcio-1.34.1\n",
      "  Attempting uninstall: tensorboard\n",
      "    Found existing installation: tensorboard 2.5.0\n",
      "    Uninstalling tensorboard-2.5.0:\n",
      "      Successfully uninstalled tensorboard-2.5.0\n",
      "  Attempting uninstall: tensorflow-estimator\n",
      "    Found existing installation: tensorflow-estimator 2.5.0\n",
      "    Uninstalling tensorflow-estimator-2.5.0:\n",
      "      Successfully uninstalled tensorflow-estimator-2.5.0\n",
      "Successfully installed clang-5.0 grpcio-1.39.0 keras-2.6.0 tensorboard-2.6.0 tensorflow-2.6.0 tensorflow-estimator-2.6.0\n"
     ]
    }
   ],
   "source": [
    "!pip3 uninstall tensorflow-gpu==2.0.0-beta0\n",
    "!pip3 install tensorflow-gpu==2.0.0-beta0\n",
    "!pip3 uninstall tensorflow\n",
    "!pip3 install tensorflow\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requirement already satisfied: tensorflow-metadata in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-datasets) (1.2.0)\n",
      "Requirement already satisfied: protobuf>=3.12.2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-datasets) (3.17.3)\n",
      "Requirement already satisfied: six in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-datasets) (1.15.0)\n",
      "Requirement already satisfied: termcolor in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-datasets) (1.1.0)\n",
      "Requirement already satisfied: promise in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-datasets) (2.3)\n",
      "Requirement already satisfied: attrs>=18.1.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-datasets) (20.3.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-datasets) (2.24.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-datasets) (1.19.2)\n",
      "Requirement already satisfied: tqdm in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-datasets) (4.50.2)\n",
      "Requirement already satisfied: dill in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-datasets) (0.3.4)\n",
      "Requirement already satisfied: zipp>=3.1.0; python_version < \"3.10\" in c:\\users\\user\\anaconda3\\lib\\site-packages (from importlib-resources; python_version < \"3.9\"->tensorflow-datasets) (3.4.0)\n",
      "Requirement already satisfied: googleapis-common-protos<2,>=1.52.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-metadata->tensorflow-datasets) (1.53.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests>=2.19.0->tensorflow-datasets) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests>=2.19.0->tensorflow-datasets) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests>=2.19.0->tensorflow-datasets) (1.25.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests>=2.19.0->tensorflow-datasets) (2020.6.20)\n"
     ]
    }
   ],
   "source": [
    "!pip3 uninstall tensorflow-datasets\n",
    "!pip3 install tensorflow-datasets\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install --ignore-installed --upgrade --ignore-installed tensorflow\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0\n"
     ]
    }
   ],
   "source": [
    "!pip3 install tensorflow-gpu==2.0.0-beta0\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "clear_output()\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(level=\"ERROR\")\n",
    "\n",
    "np.set_printoptions(suppress=True) #讓 numpy 不要顯示科學記號"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"nmt\"\n",
    "en_vocab_file = os.path.join(output_dir, \"en_vocab\")\n",
    "zh_vocab_file = os.path.join(output_dir, \"zh_vocab\")\n",
    "checkpoint_path = os.path.join(output_dir, \"checkpoints\")\n",
    "log_dir = os.path.join(output_dir, 'logs')\n",
    "download_dir = \"tensorflow-datasets/downloads\"\n",
    "\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{Split('train'): ['newscommentary_v14',\n",
      "                  'wikititles_v1',\n",
      "                  'uncorpus_v1',\n",
      "                  'casia2015',\n",
      "                  'casict2011',\n",
      "                  'casict2015',\n",
      "                  'datum2015',\n",
      "                  'datum2017',\n",
      "                  'neu2017'],\n",
      " Split('validation'): ['newstest2018']}\n"
     ]
    }
   ],
   "source": [
    "tmp_builder = tfds.builder(\"wmt19_translate/zh-en\")\n",
    "pprint(tmp_builder.subsets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = tfds.translate.wmt.WmtConfig(\n",
    "    version=\"0.0.3\",\n",
    "    language_pair=(\"zh\", \"en\"),\n",
    "    subsets={\n",
    "        tfds.Split.TRAIN: [\"newscommentary_v14\"]\n",
    "    },\n",
    ")\n",
    "builder = tfds.builder(\"wmt_translate\", config=config)\n",
    "builder.download_and_prepare(download_dir=download_dir)\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<PrefetchDataset shapes: ((), ()), types: (tf.string, tf.string)>\n",
      "<PrefetchDataset shapes: ((), ()), types: (tf.string, tf.string)>\n"
     ]
    }
   ],
   "source": [
    "examples = builder.as_dataset(split=['train[:20%]','train[20%:21%]','train[21%:]'], as_supervised=True)\n",
    "\n",
    "train_examples, val_examples, _ = examples\n",
    "print(train_examples)\n",
    "print(val_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b'The fear is real and visceral, and politicians ignore it at their peril.', shape=(), dtype=string)\n",
      "tf.Tensor(b'\\xe8\\xbf\\x99\\xe7\\xa7\\x8d\\xe6\\x81\\x90\\xe6\\x83\\xa7\\xe6\\x98\\xaf\\xe7\\x9c\\x9f\\xe5\\xae\\x9e\\xe8\\x80\\x8c\\xe5\\x86\\x85\\xe5\\x9c\\xa8\\xe7\\x9a\\x84\\xe3\\x80\\x82 \\xe5\\xbf\\xbd\\xe8\\xa7\\x86\\xe5\\xae\\x83\\xe7\\x9a\\x84\\xe6\\x94\\xbf\\xe6\\xb2\\xbb\\xe5\\xae\\xb6\\xe4\\xbb\\xac\\xe5\\x89\\x8d\\xe9\\x80\\x94\\xe5\\xa0\\xaa\\xe5\\xbf\\xa7\\xe3\\x80\\x82', shape=(), dtype=string)\n",
      "----------\n",
      "tf.Tensor(b'In fact, the German political landscape needs nothing more than a truly liberal party, in the US sense of the word \\xe2\\x80\\x9cliberal\\xe2\\x80\\x9d \\xe2\\x80\\x93 a champion of the cause of individual freedom.', shape=(), dtype=string)\n",
      "tf.Tensor(b'\\xe4\\xba\\x8b\\xe5\\xae\\x9e\\xe4\\xb8\\x8a\\xef\\xbc\\x8c\\xe5\\xbe\\xb7\\xe5\\x9b\\xbd\\xe6\\x94\\xbf\\xe6\\xb2\\xbb\\xe5\\xb1\\x80\\xe5\\x8a\\xbf\\xe9\\x9c\\x80\\xe8\\xa6\\x81\\xe7\\x9a\\x84\\xe4\\xb8\\x8d\\xe8\\xbf\\x87\\xe6\\x98\\xaf\\xe4\\xb8\\x80\\xe4\\xb8\\xaa\\xe7\\xac\\xa6\\xe5\\x90\\x88\\xe7\\xbe\\x8e\\xe5\\x9b\\xbd\\xe6\\x89\\x80\\xe8\\xb0\\x93\\xe2\\x80\\x9c\\xe8\\x87\\xaa\\xe7\\x94\\xb1\\xe2\\x80\\x9d\\xe5\\xae\\x9a\\xe4\\xb9\\x89\\xe7\\x9a\\x84\\xe7\\x9c\\x9f\\xe6\\xad\\xa3\\xe7\\x9a\\x84\\xe8\\x87\\xaa\\xe7\\x94\\xb1\\xe5\\x85\\x9a\\xe6\\xb4\\xbe\\xef\\xbc\\x8c\\xe4\\xb9\\x9f\\xe5\\xb0\\xb1\\xe6\\x98\\xaf\\xe4\\xb8\\xaa\\xe4\\xba\\xba\\xe8\\x87\\xaa\\xe7\\x94\\xb1\\xe4\\xba\\x8b\\xe4\\xb8\\x9a\\xe7\\x9a\\x84\\xe5\\x80\\xa1\\xe5\\xaf\\xbc\\xe8\\x80\\x85\\xe3\\x80\\x82', shape=(), dtype=string)\n",
      "----------\n",
      "tf.Tensor(b'Shifting to renewable-energy sources will require enormous effort and major infrastructure investment.', shape=(), dtype=string)\n",
      "tf.Tensor(b'\\xe5\\xbf\\x85\\xe9\\xa1\\xbb\\xe4\\xbb\\x98\\xe5\\x87\\xba\\xe5\\xb7\\xa8\\xe5\\xa4\\xa7\\xe7\\x9a\\x84\\xe5\\x8a\\xaa\\xe5\\x8a\\x9b\\xe5\\x92\\x8c\\xe5\\x9f\\xba\\xe7\\xa1\\x80\\xe8\\xae\\xbe\\xe6\\x96\\xbd\\xe6\\x8a\\x95\\xe8\\xb5\\x84\\xe6\\x89\\x8d\\xe8\\x83\\xbd\\xe5\\xae\\x8c\\xe6\\x88\\x90\\xe5\\x90\\x91\\xe5\\x8f\\xaf\\xe5\\x86\\x8d\\xe7\\x94\\x9f\\xe8\\x83\\xbd\\xe6\\xba\\x90\\xe7\\x9a\\x84\\xe8\\xbf\\x87\\xe6\\xb8\\xa1\\xe3\\x80\\x82', shape=(), dtype=string)\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "for en, zh in train_examples.take(3):\n",
    "    print(en)\n",
    "    print(zh)\n",
    "    print('-' * 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The fear is real and visceral, and politicians ignore it at their peril.\n",
      "这种恐惧是真实而内在的。 忽视它的政治家们前途堪忧。\n",
      "----------\n",
      "In fact, the German political landscape needs nothing more than a truly liberal party, in the US sense of the word “liberal” – a champion of the cause of individual freedom.\n",
      "事实上，德国政治局势需要的不过是一个符合美国所谓“自由”定义的真正的自由党派，也就是个人自由事业的倡导者。\n",
      "----------\n",
      "Shifting to renewable-energy sources will require enormous effort and major infrastructure investment.\n",
      "必须付出巨大的努力和基础设施投资才能完成向可再生能源的过渡。\n",
      "----------\n",
      "In this sense, it is critical to recognize the fundamental difference between “urban villages” and their rural counterparts.\n",
      "在这方面，关键在于认识到“城市村落”和农村村落之间的根本区别。\n",
      "----------\n",
      "A strong European voice, such as Nicolas Sarkozy’s during the French presidency of the EU, may make a difference, but only for six months, and at the cost of reinforcing other European countries’ nationalist feelings in reaction to the expression of “Gallic pride.”\n",
      "法国担任轮值主席国期间尼古拉·萨科奇统一的欧洲声音可能让人耳目一新，但这种声音却只持续了短短六个月，而且付出了让其他欧洲国家在面对“高卢人的骄傲”时民族主义情感进一步被激发的代价。\n",
      "----------\n",
      "Most of Japan’s bondholders are nationals (if not the central bank) and have an interest in political stability.\n",
      "日本债券持有人大多为本国国民（甚至中央银行 ） ， 政治稳定符合他们的利益。\n",
      "----------\n",
      "Paul Romer, one of the originators of new growth theory, has accused some leading names, including the Nobel laureate Robert Lucas, of what he calls “mathiness” – using math to obfuscate rather than clarify.\n",
      "新增长理论创始人之一的保罗·罗默（Paul Romer）也批评一些著名经济学家，包括诺贝尔奖获得者罗伯特·卢卡斯（Robert Lucas）在内，说他们“数学性 ” （ 罗默的用语）太重，结果是让问题变得更加模糊而不是更加清晰。\n",
      "----------\n",
      "It is, in fact, a capsule depiction of the United States Federal Reserve and the European Central Bank.\n",
      "事实上，这就是对美联储和欧洲央行的简略描述。\n",
      "----------\n",
      "Given these variables, the degree to which migration is affected by asylum-seekers will not be easy to predict or control.\n",
      "考虑到这些变量，移民受寻求庇护者的影响程度很难预测或控制。\n",
      "----------\n",
      "WASHINGTON, DC – In the 2016 American presidential election, Hillary Clinton and Donald Trump agreed that the US economy is suffering from dilapidated infrastructure, and both called for greater investment in renovating and upgrading the country’s public capital stock.\n",
      "华盛顿—在2016年美国总统选举中，希拉里·克林顿和唐纳德·特朗普都认为美国经济饱受基础设施陈旧的拖累，两人都要求加大投资用于修缮和升级美国公共资本存量。\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sample_examples = []\n",
    "num_samples = 10\n",
    "\n",
    "for en_t, zh_t in train_examples.take(num_samples):\n",
    "    en = en_t.numpy().decode(\"utf-8\")\n",
    "    zh = zh_t.numpy().decode(\"utf-8\")\n",
    "\n",
    "    print(en)\n",
    "    print(zh)\n",
    "    print('-' * 10)\n",
    "\n",
    "    # 之後用來簡單評估模型的訓練情況\n",
    "    sample_examples.append((en, zh))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "沒有已建立的字典，從頭建立。\n",
      "字典大小：8113\n",
      "前 10 個 subwords：[', ', 'the_', 'of_', 'to_', 'and_', 's_', 'in_', 'a_', 'is_', 'that_']\n",
      "\n",
      "Wall time: 1min 51s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "## load_from_file 函式嘗試讀取之前已經建好的字典檔案\n",
    "try:\n",
    "    subword_encoder_en = tfds.deprecated.text.SubwordTextEncoder.load_from_file(en_vocab_file)\n",
    "    print(f\"載入已建立的字典： {en_vocab_file}\")\n",
    "except:\n",
    "    print(\"沒有已建立的字典，從頭建立。\")\n",
    "    subword_encoder_en = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(\n",
    "      (en.numpy() for en, _ in train_examples), \n",
    "      target_vocab_size=2**13) # 有需要可以調整字典大小\n",
    "  \n",
    "    # 將字典檔案存下以方便下次 warmstart\n",
    "    subword_encoder_en.save_to_file(en_vocab_file)\n",
    "  \n",
    "\n",
    "print(f\"字典大小：{subword_encoder_en.vocab_size}\")\n",
    "print(f\"前 10 個 subwords：{subword_encoder_en.subwords[:10]}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3461, 7889, 9, 3502, 4379, 1134, 7903]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_string = 'Taiwan is beautiful.'\n",
    "indices = subword_encoder_en.encode(sample_string)\n",
    "indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index     Subword\n",
      "---------------\n",
      " 3461     Taiwan\n",
      " 7889      \n",
      "    9     is \n",
      " 3502     bea\n",
      " 4379     uti\n",
      " 1134     ful\n",
      " 7903     .\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"{0:10}{1:6}\".format(\"Index\", \"Subword\"))\n",
    "print(\"-\" * 15)\n",
    "for idx in indices:\n",
    "    subword = subword_encoder_en.decode([idx])\n",
    "    print('{0:5}{1:6}'.format(idx, ' ' * 5 + subword))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Taiwan is beautiful.', 'Taiwan is beautiful.')\n"
     ]
    }
   ],
   "source": [
    "sample_string = 'Taiwan is beautiful.'\n",
    "indices = subword_encoder_en.encode(sample_string)\n",
    "decoded_string = subword_encoder_en.decode(indices)\n",
    "assert decoded_string == sample_string\n",
    "pprint((sample_string, decoded_string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#為中文建立一個字典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "沒有已建立的字典，從頭建立。\n",
      "字典大小：4205\n",
      "前 10 個 subwords：['的', '，', '。', '国', '在', '是', '一', '和', '不', '这']\n",
      "\n",
      "Wall time: 11min 15s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#https://www.tensorflow.org/datasets/api_docs/python/tfds/deprecated/text/SubwordTextEncoder\n",
    "try:\n",
    "    subword_encoder_zh = tfds.deprecated.text.SubwordTextEncoder.load_from_file(zh_vocab_file)\n",
    "    print(f\"載入已建立的字典： {zh_vocab_file}\")\n",
    "except:\n",
    "    print(\"沒有已建立的字典，從頭建立。\")\n",
    "    subword_encoder_zh = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(\n",
    "      (zh.numpy() for _, zh in train_examples), \n",
    "      target_vocab_size=2**13, # 有需要可以調整字典大小\n",
    "      max_subword_length=1) # 每一個中文字就是字典裡的一個單位\n",
    "\n",
    "    # 將字典檔案存下以方便下次 warmstart \n",
    "    subword_encoder_zh.save_to_file(zh_vocab_file)\n",
    "\n",
    "print(f\"字典大小：{subword_encoder_zh.vocab_size}\")\n",
    "print(f\"前 10 個 subwords：{subword_encoder_zh.subwords[:10]}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "这种恐惧是真实而内在的。 忽视它的政治家们前途堪忧。\n",
      "[10, 151, 574, 1298, 6, 374, 55, 29, 193, 5, 1, 3, 3981, 931, 431, 125, 1, 17, 124, 33, 20, 97, 1089, 1247, 861, 3]\n"
     ]
    }
   ],
   "source": [
    "sample_string = sample_examples[0][1]\n",
    "indices = subword_encoder_zh.encode(sample_string)\n",
    "print(sample_string)\n",
    "print(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[英中原文]（轉換前）\n",
      "The eurozone’s collapse forces a major realignment of European politics.\n",
      "欧元区的瓦解强迫欧洲政治进行一次重大改组。\n",
      "\n",
      "--------------------\n",
      "\n",
      "[英中序列]（轉換後）\n",
      "[16, 900, 11, 6, 1527, 874, 8, 230, 2259, 2728, 239, 3, 89, 1236, 7903]\n",
      "[44, 202, 168, 1, 852, 201, 231, 592, 44, 87, 17, 124, 106, 38, 7, 279, 86, 18, 212, 265, 3]\n"
     ]
    }
   ],
   "source": [
    "en = \"The eurozone’s collapse forces a major realignment of European politics.\"\n",
    "zh = \"欧元区的瓦解强迫欧洲政治进行一次重大改组。\"\n",
    "\n",
    "# 將文字轉成為 subword indices\n",
    "en_indices = subword_encoder_en.encode(en)\n",
    "zh_indices = subword_encoder_zh.encode(zh)\n",
    "\n",
    "print(\"[英中原文]（轉換前）\")\n",
    "print(en)\n",
    "print(zh)\n",
    "print()\n",
    "print('-' * 20)\n",
    "print()\n",
    "print(\"[英中序列]（轉換後）\")\n",
    "print(en_indices)\n",
    "print(zh_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def encode(en_t, zh_t):\n",
    "  # 因為字典的索引從 0 開始，\n",
    "  # 我們可以使用 subword_encoder_en.vocab_size 這個值作為 BOS 的索引值\n",
    "  # 用 subword_encoder_en.vocab_size + 1 作為 EOS 的索引值\n",
    "  en_indices = [subword_encoder_en.vocab_size] + subword_encoder_en.encode(\n",
    "      en_t.numpy()) + [subword_encoder_en.vocab_size + 1]\n",
    "  # 同理，不過是使用中文字典的最後一個索引 + 1\n",
    "  zh_indices = [subword_encoder_zh.vocab_size] + subword_encoder_zh.encode(\n",
    "      zh_t.numpy()) + [subword_encoder_zh.vocab_size + 1]\n",
    "  \n",
    "  return en_indices, zh_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "英文 BOS 的 index： 8113\n",
      "英文 EOS 的 index： 8114\n",
      "中文 BOS 的 index： 4205\n",
      "中文 EOS 的 index： 4206\n",
      "\n",
      "輸入為 2 個 Tensors：\n",
      "(<tf.Tensor: shape=(), dtype=string, numpy=b'The fear is real and visceral, and politicians ignore it at their peril.'>,\n",
      " <tf.Tensor: shape=(), dtype=string, numpy=b'\\xe8\\xbf\\x99\\xe7\\xa7\\x8d\\xe6\\x81\\x90\\xe6\\x83\\xa7\\xe6\\x98\\xaf\\xe7\\x9c\\x9f\\xe5\\xae\\x9e\\xe8\\x80\\x8c\\xe5\\x86\\x85\\xe5\\x9c\\xa8\\xe7\\x9a\\x84\\xe3\\x80\\x82 \\xe5\\xbf\\xbd\\xe8\\xa7\\x86\\xe5\\xae\\x83\\xe7\\x9a\\x84\\xe6\\x94\\xbf\\xe6\\xb2\\xbb\\xe5\\xae\\xb6\\xe4\\xbb\\xac\\xe5\\x89\\x8d\\xe9\\x80\\x94\\xe5\\xa0\\xaa\\xe5\\xbf\\xa7\\xe3\\x80\\x82'>)\n",
      "---------------\n",
      "輸出為 2 個索引序列：\n",
      "([8113,\n",
      "  16,\n",
      "  1284,\n",
      "  9,\n",
      "  243,\n",
      "  5,\n",
      "  1275,\n",
      "  1756,\n",
      "  156,\n",
      "  1,\n",
      "  5,\n",
      "  1016,\n",
      "  5566,\n",
      "  21,\n",
      "  38,\n",
      "  33,\n",
      "  2982,\n",
      "  7965,\n",
      "  7903,\n",
      "  8114],\n",
      " [4205,\n",
      "  10,\n",
      "  151,\n",
      "  574,\n",
      "  1298,\n",
      "  6,\n",
      "  374,\n",
      "  55,\n",
      "  29,\n",
      "  193,\n",
      "  5,\n",
      "  1,\n",
      "  3,\n",
      "  3981,\n",
      "  931,\n",
      "  431,\n",
      "  125,\n",
      "  1,\n",
      "  17,\n",
      "  124,\n",
      "  33,\n",
      "  20,\n",
      "  97,\n",
      "  1089,\n",
      "  1247,\n",
      "  861,\n",
      "  3,\n",
      "  4206])\n"
     ]
    }
   ],
   "source": [
    "en_t, zh_t = next(iter(train_examples))\n",
    "en_indices, zh_indices = encode(en_t, zh_t)\n",
    "print('英文 BOS 的 index：', subword_encoder_en.vocab_size)\n",
    "print('英文 EOS 的 index：', subword_encoder_en.vocab_size + 1)\n",
    "print('中文 BOS 的 index：', subword_encoder_zh.vocab_size)\n",
    "print('中文 EOS 的 index：', subword_encoder_zh.vocab_size + 1)\n",
    "\n",
    "print('\\n輸入為 2 個 Tensors：')\n",
    "pprint((en_t, zh_t))\n",
    "print('-' * 15)\n",
    "print('輸出為 2 個索引序列：')\n",
    "pprint((en_indices, zh_indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[8113   16 1284    9  243    5 1275 1756  156    1    5 1016 5566   21\n",
      "   38   33 2982 7965 7903 8114], shape=(20,), dtype=int64)\n",
      "tf.Tensor(\n",
      "[4205   10  151  574 1298    6  374   55   29  193    5    1    3 3981\n",
      "  931  431  125    1   17  124   33   20   97 1089 1247  861    3 4206], shape=(28,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "def tf_encode(en_t, zh_t):\n",
    "  # 在 `tf_encode` 函式裡頭的 `en_t` 與 `zh_t` 都不是 Eager Tensors\n",
    "  # 要到 `tf.py_funtion` 裡頭才是\n",
    "  # 另外因為索引都是整數，所以使用 `tf.int64`\n",
    "  return tf.py_function(encode, [en_t, zh_t], [tf.int64, tf.int64])\n",
    "\n",
    "# `tmp_dataset` 為說明用資料集，說明完所有重要的 func，\n",
    "# 我們會從頭建立一個正式的 `train_dataset`\n",
    "tmp_dataset = train_examples.map(tf_encode)\n",
    "en_indices, zh_indices = next(iter(tmp_dataset))\n",
    "print(en_indices)\n",
    "print(zh_indices)\n",
    "## tmp_dataset 的輸出已經是兩個索引序列，而非原文字串"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 40\n",
    "\n",
    "def filter_max_length(en, zh, max_length=MAX_LENGTH):\n",
    "  # en, zh 分別代表英文與中文的索引序列\n",
    "  return tf.logical_and(tf.size(en) <= max_length,\n",
    "                        tf.size(zh) <= max_length)\n",
    "\n",
    "# tf.data.Dataset.filter(func) 只會回傳 func 為真的例子\n",
    "tmp_dataset = tmp_dataset.filter(filter_max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "所有英文與中文序列長度都不超過 40 個 tokens\n",
      "訓練資料集裡總共有 29784 筆數據\n"
     ]
    }
   ],
   "source": [
    "# 因為我們數據量小可以這樣 count\n",
    "num_examples = 0\n",
    "for en_indices, zh_indices in tmp_dataset:\n",
    "    cond1 = len(en_indices) <= MAX_LENGTH\n",
    "    cond2 = len(zh_indices) <= MAX_LENGTH\n",
    "    assert cond1 and cond2\n",
    "    num_examples += 1\n",
    "\n",
    "print(f\"所有英文與中文序列長度都不超過 {MAX_LENGTH} 個 tokens\")\n",
    "print(f\"訓練資料集裡總共有 {num_examples} 筆數據\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "英文索引序列的 batch\n",
      "tf.Tensor(\n",
      "[[8113   16 1284 ...    0    0    0]\n",
      " [8113 1894 1302 ...    0    0    0]\n",
      " [8113   44   40 ...    0    0    0]\n",
      " ...\n",
      " [8113  122  506 ...    0    0    0]\n",
      " [8113   16  215 ...    0    0    0]\n",
      " [8113 7443 7889 ...    0    0    0]], shape=(64, 39), dtype=int64)\n",
      "--------------------\n",
      "中文索引序列的 batch\n",
      "tf.Tensor(\n",
      "[[4205   10  151 ...    0    0    0]\n",
      " [4205  206  275 ...    0    0    0]\n",
      " [4205    5   10 ...    0    0    0]\n",
      " ...\n",
      " [4205   34    6 ...    0    0    0]\n",
      " [4205  317  256 ...    0    0    0]\n",
      " [4205  167  326 ...    0    0    0]], shape=(64, 40), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 64\n",
    "# 將 batch 裡的所有序列都 pad 到同樣長度\n",
    "tmp_dataset = tmp_dataset.padded_batch(BATCH_SIZE, padded_shapes=([-1], [-1]))\n",
    "en_batch, zh_batch = next(iter(tmp_dataset))\n",
    "print(\"英文索引序列的 batch\")\n",
    "print(en_batch)\n",
    "print('-' * 20)\n",
    "print(\"中文索引序列的 batch\")\n",
    "print(zh_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 40\n",
    "BATCH_SIZE = 128\n",
    "BUFFER_SIZE = 15000\n",
    "\n",
    "# 訓練集\n",
    "train_dataset = (train_examples  # 輸出：(英文句子, 中文句子)\n",
    "                 .map(tf_encode) # 輸出：(英文索引序列, 中文索引序列)\n",
    "                 .filter(filter_max_length) # 同上，且序列長度都不超過 40\n",
    "                 .cache() # 加快讀取數據\n",
    "                 .shuffle(BUFFER_SIZE) # 將例子洗牌確保隨機性\n",
    "                 .padded_batch(BATCH_SIZE, # 將 batch 裡的序列都 pad 到一樣長度\n",
    "                               padded_shapes=([-1], [-1]))\n",
    "                 .prefetch(tf.data.experimental.AUTOTUNE)) # 加速\n",
    "# 驗證集\n",
    "val_dataset = (val_examples\n",
    "               .map(tf_encode)\n",
    "               .filter(filter_max_length)\n",
    "               .padded_batch(BATCH_SIZE, \n",
    "                             padded_shapes=([-1], [-1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "英文索引序列的 batch\n",
      "tf.Tensor(\n",
      "[[8113   87 5599 ...    0    0    0]\n",
      " [8113  998    5 ...    0    0    0]\n",
      " [8113 6266 1606 ...    0    0    0]\n",
      " ...\n",
      " [8113   16 6445 ...    0    0    0]\n",
      " [8113 5363   39 ...    0    0    0]\n",
      " [8113 5150 1662 ...    0    0    0]], shape=(128, 40), dtype=int64)\n",
      "--------------------\n",
      "中文索引序列的 batch\n",
      "tf.Tensor(\n",
      "[[4205   10  126 ...    0    0    0]\n",
      " [4205    4   33 ...    0    0    0]\n",
      " [4205   52   11 ...    0    0    0]\n",
      " ...\n",
      " [4205  526  538 ...    0    0    0]\n",
      " [4205  266 1380 ...    0    0    0]\n",
      " [4205   45  116 ...    0    0    0]], shape=(128, 40), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "en_batch, zh_batch = next(iter(train_dataset))\n",
    "print(\"英文索引序列的 batch\")\n",
    "print(en_batch)\n",
    "print('-' * 20)\n",
    "print(\"中文索引序列的 batch\")\n",
    "print(zh_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('It is important.', '这很重要。'),\n",
      " ('The numbers speak for themselves.', '数字证明了一切。')]\n"
     ]
    }
   ],
   "source": [
    "demo_examples = [\n",
    "    (\"It is important.\", \"这很重要。\"),\n",
    "    (\"The numbers speak for themselves.\", \"数字证明了一切。\"),\n",
    "]\n",
    "pprint(demo_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inp: tf.Tensor(\n",
      "[[8113  103    9 1066 7903 8114    0    0]\n",
      " [8113   16 4111 6735   12 2750 7903 8114]], shape=(2, 8), dtype=int64)\n",
      "\n",
      "tar: tf.Tensor(\n",
      "[[4205   10  241   86   27    3 4206    0    0    0]\n",
      " [4205  165  489  398  191   14    7  560    3 4206]], shape=(2, 10), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 2\n",
    "demo_examples = tf.data.Dataset.from_tensor_slices((\n",
    "    [en for en, _ in demo_examples], [zh for _, zh in demo_examples]\n",
    "))\n",
    "\n",
    "# 將兩個句子透過之前定義的字典轉換成子詞的序列（sequence of subwords）\n",
    "# 並添加 padding token: <pad> 來確保 batch 裡的句子有一樣長度\n",
    "demo_dataset = demo_examples.map(tf_encode)\\\n",
    "  .padded_batch(batch_size, padded_shapes=([-1], [-1]))\n",
    "\n",
    "# 取出這個 demo dataset 裡唯一一個 batch\n",
    "inp, tar = next(iter(demo_dataset))\n",
    "print('inp:', inp)\n",
    "print('' * 10)\n",
    "print('tar:', tar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(2, 8, 4), dtype=float32, numpy=\n",
       " array([[[-0.02940199, -0.02748928,  0.009587  ,  0.02695702],\n",
       "         [ 0.02781546, -0.00051839, -0.04208876, -0.03737368],\n",
       "         [ 0.04085249, -0.01280526, -0.02142819,  0.00932728],\n",
       "         [-0.04287729,  0.00559031, -0.01610124,  0.02254481],\n",
       "         [ 0.02117098, -0.03711741, -0.01554632,  0.00051199],\n",
       "         [-0.00089258,  0.03358604,  0.04734136, -0.02150711],\n",
       "         [-0.01283282,  0.03373785,  0.0044058 , -0.04340933],\n",
       "         [-0.01283282,  0.03373785,  0.0044058 , -0.04340933]],\n",
       " \n",
       "        [[-0.02940199, -0.02748928,  0.009587  ,  0.02695702],\n",
       "         [-0.02589555, -0.00853635,  0.03899905,  0.02993281],\n",
       "         [ 0.02303552, -0.0043821 ,  0.03425056,  0.02410031],\n",
       "         [-0.02790775,  0.0453908 , -0.00815301, -0.01390294],\n",
       "         [ 0.02987817,  0.02534869,  0.04416693, -0.03023093],\n",
       "         [ 0.03675361,  0.04292214,  0.0072703 , -0.03425226],\n",
       "         [ 0.02117098, -0.03711741, -0.01554632,  0.00051199],\n",
       "         [-0.00089258,  0.03358604,  0.04734136, -0.02150711]]],\n",
       "       dtype=float32)>,\n",
       " <tf.Tensor: shape=(2, 10, 4), dtype=float32, numpy=\n",
       " array([[[ 0.02309941,  0.01578486, -0.03650398,  0.04221227],\n",
       "         [ 0.02180279, -0.03836104, -0.0052525 , -0.01320656],\n",
       "         [-0.0190685 , -0.03266476, -0.03334127,  0.04303731],\n",
       "         [ 0.03618959, -0.00743835,  0.01616282,  0.04101736],\n",
       "         [ 0.00099171,  0.03080915, -0.03851545,  0.03447074],\n",
       "         [-0.02579685,  0.02527661,  0.04145398, -0.04307295],\n",
       "         [-0.00221508,  0.03029467,  0.02829352,  0.04788197],\n",
       "         [-0.02839622,  0.04139889, -0.00255186, -0.03752811],\n",
       "         [-0.02839622,  0.04139889, -0.00255186, -0.03752811],\n",
       "         [-0.02839622,  0.04139889, -0.00255186, -0.03752811]],\n",
       " \n",
       "        [[ 0.02309941,  0.01578486, -0.03650398,  0.04221227],\n",
       "         [-0.00205498,  0.04796953, -0.00836701,  0.03551394],\n",
       "         [-0.0049132 , -0.03055991,  0.03992986, -0.02458232],\n",
       "         [ 0.02458249, -0.02872031,  0.01778464, -0.02678457],\n",
       "         [ 0.03532792,  0.02878738, -0.00509523, -0.0492527 ],\n",
       "         [-0.00865723, -0.02110739, -0.03051426, -0.02639371],\n",
       "         [ 0.02155342, -0.00126004,  0.00749636, -0.00912453],\n",
       "         [-0.01634679,  0.00527266,  0.01074455,  0.02497052],\n",
       "         [-0.02579685,  0.02527661,  0.04145398, -0.04307295],\n",
       "         [-0.00221508,  0.03029467,  0.02829352,  0.04788197]]],\n",
       "       dtype=float32)>)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# + 2 是因為我們額外加了 <start> 以及 <end> tokens\n",
    "vocab_size_en = subword_encoder_en.vocab_size + 2\n",
    "vocab_size_zh = subword_encoder_zh.vocab_size + 2\n",
    "\n",
    "# 為了方便 demo, 將詞彙轉換到一個 4 維的詞嵌入空間\n",
    "d_model = 4\n",
    "embedding_layer_en = tf.keras.layers.Embedding(vocab_size_en, d_model)\n",
    "embedding_layer_zh = tf.keras.layers.Embedding(vocab_size_zh, d_model)\n",
    "\n",
    "emb_inp = embedding_layer_en(inp)\n",
    "emb_tar = embedding_layer_zh(tar)\n",
    "emb_inp, emb_tar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tar[0]: tf.Tensor([0 0 0], shape=(3,), dtype=int64)\n",
      "--------------------\n",
      "emb_tar[0]: tf.Tensor(\n",
      "[[-0.02839622  0.04139889 -0.00255186 -0.03752811]\n",
      " [-0.02839622  0.04139889 -0.00255186 -0.03752811]\n",
      " [-0.02839622  0.04139889 -0.00255186 -0.03752811]], shape=(3, 4), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(\"tar[0]:\", tar[0][-3:])\n",
    "print(\"-\" * 20)\n",
    "print(\"emb_tar[0]:\", emb_tar[0][-3:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 1, 1, 8), dtype=float32, numpy=\n",
       "array([[[[0., 0., 0., 0., 0., 0., 1., 1.]]],\n",
       "\n",
       "\n",
       "       [[[0., 0., 0., 0., 0., 0., 0., 0.]]]], dtype=float32)>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_padding_mask(seq):\n",
    "    # padding mask 的工作就是把索引序列中為 0 的位置設為 1\n",
    "    mask = tf.cast(tf.equal(seq, 0), tf.float32)\n",
    "    return mask[:, tf.newaxis, tf.newaxis, :] #　broadcasting\n",
    "\n",
    "inp_mask = create_padding_mask(inp)\n",
    "inp_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inp: tf.Tensor(\n",
      "[[8113  103    9 1066 7903 8114    0    0]\n",
      " [8113   16 4111 6735   12 2750 7903 8114]], shape=(2, 8), dtype=int64)\n",
      "--------------------\n",
      "tf.squeeze(inp_mask): tf.Tensor(\n",
      "[[0. 0. 0. 0. 0. 0. 1. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]], shape=(2, 8), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"inp:\", inp)\n",
    "print(\"-\" * 20)\n",
    "print(\"tf.squeeze(inp_mask):\", tf.squeeze(inp_mask))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 8, 4), dtype=float32, numpy=\n",
       "array([[[1., 0., 0., 0.],\n",
       "        [0., 1., 0., 1.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [1., 0., 1., 0.],\n",
       "        [1., 0., 1., 0.],\n",
       "        [0., 1., 0., 1.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [0., 1., 0., 1.]],\n",
       "\n",
       "       [[1., 0., 1., 1.],\n",
       "        [1., 0., 1., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [1., 0., 1., 0.],\n",
       "        [0., 1., 0., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 1., 0.]]], dtype=float32)>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 設定一個 seed 確保我們每次都拿到一樣的隨機結果\n",
    "tf.random.set_seed(9527)\n",
    "\n",
    "# 自注意力機制：查詢 `q` 跟鍵值 `k` 都是 `emb_inp`\n",
    "q = emb_inp\n",
    "k = emb_inp\n",
    "# 簡單產生一個跟 `emb_inp` 同樣 shape 的 binary vector\n",
    "v = tf.cast(tf.math.greater(tf.random.uniform(shape=emb_inp.shape), 0.5), tf.float32)\n",
    "v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(q, k, v, mask):\n",
    "    # 將 `q`、 `k` 做點積再 scale\n",
    "    matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n",
    "\n",
    "    dk = tf.cast(tf.shape(k)[-1], tf.float32)  # 取得 seq_k 的序列長度\n",
    "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)  # scale by sqrt(dk)\n",
    "\n",
    "    # 將遮罩「加」到被丟入 softmax 前的 logits\n",
    "    if mask is not None:\n",
    "        scaled_attention_logits += (mask * -1e9)\n",
    "\n",
    "    # 取 softmax 是為了得到總和為 1 的比例之後對 `v` 做加權平均\n",
    "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n",
    "\n",
    "    # 以注意權重對 v 做加權平均（weighted average）\n",
    "    output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n",
    "\n",
    "    return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output: tf.Tensor(\n",
      "[[[0.3753399  0.3747662  0.37508252 0.49974045]\n",
      "  [0.37475583 0.37513748 0.37496054 0.50021064]\n",
      "  [0.37499675 0.3749313  0.37495598 0.50008714]\n",
      "  [0.3752308  0.3748534  0.37511384 0.49978137]\n",
      "  [0.37511206 0.37487078 0.37499475 0.49998224]\n",
      "  [0.3747127  0.37527195 0.3749196  0.5001556 ]\n",
      "  [0.3746645  0.37529942 0.3749698  0.5001717 ]\n",
      "  [0.3746645  0.37529942 0.3749698  0.5001717 ]]\n",
      "\n",
      " [[0.6251311  0.24970885 0.62504077 0.37487447]\n",
      "  [0.6251049  0.24979301 0.6250699  0.37489533]\n",
      "  [0.624972   0.25000775 0.62488645 0.37499285]\n",
      "  [0.6250523  0.25008556 0.62523466 0.3750197 ]\n",
      "  [0.6248019  0.25033873 0.62493974 0.375137  ]\n",
      "  [0.624817   0.25040787 0.6249676  0.37516844]\n",
      "  [0.62495446 0.24994777 0.62482053 0.37499923]\n",
      "  [0.624902   0.25021926 0.6250832  0.37507576]]], shape=(2, 8, 4), dtype=float32)\n",
      "--------------------\n",
      "attention_weights: tf.Tensor(\n",
      "[[[0.125177   0.12488609 0.12497426 0.12512204 0.12504087 0.12496053\n",
      "   0.1249196  0.1249196 ]\n",
      "  [0.12482883 0.1252137  0.12507316 0.12488215 0.12504484 0.12489024\n",
      "   0.12503353 0.12503353]\n",
      "  [0.12495689 0.12511314 0.12515587 0.12492786 0.12511201 0.12490202\n",
      "   0.12491614 0.12491614]\n",
      "  [0.12510477 0.1249222  0.12492798 0.12517215 0.12495389 0.12494341\n",
      "   0.12498779 0.12498779]\n",
      "  [0.12502298 0.12508428 0.12511148 0.12495323 0.12513587 0.12488083\n",
      "   0.12490567 0.12490567]\n",
      "  [0.12492479 0.1249118  0.12488367 0.12492491 0.12486301 0.12522845\n",
      "   0.1251317  0.1251317 ]\n",
      "  [0.12485838 0.1250296  0.12487227 0.12494376 0.12486234 0.12510614\n",
      "   0.1251637  0.1251637 ]\n",
      "  [0.12485838 0.1250296  0.12487227 0.12494376 0.12486234 0.12510614\n",
      "   0.1251637  0.1251637 ]]\n",
      "\n",
      " [[0.12516564 0.12514925 0.12503944 0.12495811 0.12489024 0.12481861\n",
      "   0.12502952 0.12494919]\n",
      "  [0.12510231 0.12516384 0.12505984 0.1249413  0.12495543 0.12483758\n",
      "   0.12491484 0.12502488]\n",
      "  [0.12498508 0.12505239 0.12510279 0.12486783 0.12504385 0.12496389\n",
      "   0.1249669  0.12501723]\n",
      "  [0.12493413 0.1249642  0.12489816 0.12518294 0.1250127  0.12507287\n",
      "   0.12485447 0.12508056]\n",
      "  [0.12479828 0.1249103  0.12500612 0.12494461 0.12519619 0.12514254\n",
      "   0.12485797 0.12514399]\n",
      "  [0.12476055 0.12482633 0.12496009 0.12503868 0.1251765  0.12523137\n",
      "   0.1248959  0.12511061]\n",
      "  [0.12505144 0.12498362 0.12504315 0.1249003  0.12497186 0.12497591\n",
      "   0.12516436 0.12490926]\n",
      "  [0.12485649 0.12497903 0.12497881 0.12501174 0.12514329 0.12507597\n",
      "   0.12479473 0.12515998]]], shape=(2, 8, 8), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "mask = None\n",
    "output, attention_weights = scaled_dot_product_attention(q, k, v, mask)\n",
    "print(\"output:\", output)\n",
    "print(\"-\" * 20)\n",
    "print(\"attention_weights:\", attention_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inp: tf.Tensor(\n",
      "[[8113  103    9 1066 7903 8114    0    0]\n",
      " [8113   16 4111 6735   12 2750 7903 8114]], shape=(2, 8), dtype=int64)\n",
      "--------------------\n",
      "inp_mask: tf.Tensor(\n",
      "[[[[0. 0. 0. 0. 0. 0. 1. 1.]]]\n",
      "\n",
      "\n",
      " [[[0. 0. 0. 0. 0. 0. 0. 0.]]]], shape=(2, 1, 1, 8), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "def create_padding_mask(seq):\n",
    "    # padding mask 的工作就是把索引序列中為 0 的位置設為 1\n",
    "    mask = tf.cast(tf.equal(seq, 0), tf.float32)\n",
    "    return mask[:, tf.newaxis, tf.newaxis, :] #　broadcasting\n",
    "\n",
    "print(\"inp:\", inp)\n",
    "inp_mask = create_padding_mask(inp)\n",
    "print(\"-\" * 20)\n",
    "print(\"inp_mask:\", inp_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention_weights: tf.Tensor(\n",
      "[[[0.16686688 0.16647908 0.16659662 0.16679361 0.16668542 0.16657832\n",
      "   0.         0.        ]\n",
      "  [0.16645332 0.16696653 0.16677913 0.16652443 0.16674137 0.16653521\n",
      "   0.         0.        ]\n",
      "  [0.16657193 0.16678022 0.16683717 0.16653322 0.16677871 0.16649878\n",
      "   0.         0.        ]\n",
      "  [0.16680093 0.1665575  0.16656522 0.16689077 0.16659975 0.16658579\n",
      "   0.         0.        ]\n",
      "  [0.16665538 0.1667371  0.16677335 0.16656241 0.16680586 0.1664659\n",
      "   0.         0.        ]\n",
      "  [0.1666249  0.16660757 0.16657004 0.16662505 0.16654249 0.16702992\n",
      "   0.         0.        ]\n",
      "  [0.16655058 0.16677895 0.1665691  0.16666447 0.16655585 0.16688107\n",
      "   0.         0.        ]\n",
      "  [0.16655058 0.16677895 0.1665691  0.16666447 0.16655585 0.16688107\n",
      "   0.         0.        ]]\n",
      "\n",
      " [[0.12516564 0.12514925 0.12503944 0.12495811 0.12489024 0.12481861\n",
      "   0.12502952 0.12494919]\n",
      "  [0.12510231 0.12516384 0.12505984 0.1249413  0.12495543 0.12483758\n",
      "   0.12491484 0.12502488]\n",
      "  [0.12498508 0.12505239 0.12510279 0.12486783 0.12504385 0.12496389\n",
      "   0.1249669  0.12501723]\n",
      "  [0.12493413 0.1249642  0.12489816 0.12518294 0.1250127  0.12507287\n",
      "   0.12485447 0.12508056]\n",
      "  [0.12479828 0.1249103  0.12500612 0.12494461 0.12519619 0.12514254\n",
      "   0.12485797 0.12514399]\n",
      "  [0.12476055 0.12482633 0.12496009 0.12503868 0.1251765  0.12523137\n",
      "   0.1248959  0.12511061]\n",
      "  [0.12505144 0.12498362 0.12504315 0.1249003  0.12497186 0.12497591\n",
      "   0.12516436 0.12490926]\n",
      "  [0.12485649 0.12497903 0.12497881 0.12501174 0.12514329 0.12507597\n",
      "   0.12479473 0.12515998]]], shape=(2, 8, 8), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# 這次讓我們將 padding mask 放入注意函式並觀察\n",
    "# 注意權重的變化\n",
    "mask = tf.squeeze(inp_mask, axis=1) # (batch_size, 1, seq_len_q)\n",
    "_, attention_weights = scaled_dot_product_attention(q, k, v, mask)\n",
    "print(\"attention_weights:\", attention_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 8, 2), dtype=float32, numpy=\n",
       "array([[[0.        , 0.        ],\n",
       "        [0.        , 0.        ],\n",
       "        [0.        , 0.        ],\n",
       "        [0.        , 0.        ],\n",
       "        [0.        , 0.        ],\n",
       "        [0.        , 0.        ],\n",
       "        [0.        , 0.        ],\n",
       "        [0.        , 0.        ]],\n",
       "\n",
       "       [[0.12502952, 0.12494919],\n",
       "        [0.12491484, 0.12502488],\n",
       "        [0.1249669 , 0.12501723],\n",
       "        [0.12485447, 0.12508056],\n",
       "        [0.12485797, 0.12514399],\n",
       "        [0.1248959 , 0.12511061],\n",
       "        [0.12516436, 0.12490926],\n",
       "        [0.12479473, 0.12515998]]], dtype=float32)>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 事實上也不完全是上句話的翻譯，\n",
    "# 因為我們在第一個維度還是把兩個句子都拿出來方便你比較\n",
    "attention_weights[:, :, -2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emb_tar: tf.Tensor(\n",
      "[[[ 0.02309941  0.01578486 -0.03650398  0.04221227]\n",
      "  [ 0.02180279 -0.03836104 -0.0052525  -0.01320656]\n",
      "  [-0.0190685  -0.03266476 -0.03334127  0.04303731]\n",
      "  [ 0.03618959 -0.00743835  0.01616282  0.04101736]\n",
      "  [ 0.00099171  0.03080915 -0.03851545  0.03447074]\n",
      "  [-0.02579685  0.02527661  0.04145398 -0.04307295]\n",
      "  [-0.00221508  0.03029467  0.02829352  0.04788197]\n",
      "  [-0.02839622  0.04139889 -0.00255186 -0.03752811]\n",
      "  [-0.02839622  0.04139889 -0.00255186 -0.03752811]\n",
      "  [-0.02839622  0.04139889 -0.00255186 -0.03752811]]\n",
      "\n",
      " [[ 0.02309941  0.01578486 -0.03650398  0.04221227]\n",
      "  [-0.00205498  0.04796953 -0.00836701  0.03551394]\n",
      "  [-0.0049132  -0.03055991  0.03992986 -0.02458232]\n",
      "  [ 0.02458249 -0.02872031  0.01778464 -0.02678457]\n",
      "  [ 0.03532792  0.02878738 -0.00509523 -0.0492527 ]\n",
      "  [-0.00865723 -0.02110739 -0.03051426 -0.02639371]\n",
      "  [ 0.02155342 -0.00126004  0.00749636 -0.00912453]\n",
      "  [-0.01634679  0.00527266  0.01074455  0.02497052]\n",
      "  [-0.02579685  0.02527661  0.04145398 -0.04307295]\n",
      "  [-0.00221508  0.03029467  0.02829352  0.04788197]]], shape=(2, 10, 4), dtype=float32)\n",
      "--------------------\n",
      "look_ahead_mask tf.Tensor(\n",
      "[[0. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [0. 0. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [0. 0. 0. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [0. 0. 0. 0. 1. 1. 1. 1. 1. 1.]\n",
      " [0. 0. 0. 0. 0. 1. 1. 1. 1. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 1. 1. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 1. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]], shape=(10, 10), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# 建立一個 2 維矩陣，維度為 (size, size)，\n",
    "# 其遮罩為一個右上角的三角形\n",
    "def create_look_ahead_mask(size):\n",
    "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "    return mask  # (seq_len, seq_len)\n",
    "\n",
    "seq_len = emb_tar.shape[1] # 注意這次我們用中文的詞嵌入張量 `emb_tar`\n",
    "look_ahead_mask = create_look_ahead_mask(seq_len)\n",
    "print(\"emb_tar:\", emb_tar)\n",
    "print(\"-\" * 20)\n",
    "print(\"look_ahead_mask\", look_ahead_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention_weights: tf.Tensor(\n",
      "[[[1.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.        ]\n",
      "  [0.49967298 0.5003271  0.         0.         0.         0.\n",
      "   0.         0.         0.         0.        ]\n",
      "  [0.3332953  0.33302316 0.33368158 0.         0.         0.\n",
      "   0.         0.         0.         0.        ]\n",
      "  [0.25003257 0.2498561  0.24989752 0.25021377 0.         0.\n",
      "   0.         0.         0.         0.        ]\n",
      "  [0.20017864 0.19970043 0.20001577 0.19990137 0.20020382 0.\n",
      "   0.         0.         0.         0.        ]\n",
      "  [0.16648294 0.16667846 0.1664795  0.16659202 0.16658287 0.16718417\n",
      "   0.         0.         0.         0.        ]\n",
      "  [0.14288522 0.14264204 0.14279628 0.14293528 0.14289069 0.14277938\n",
      "   0.14307113 0.         0.         0.        ]\n",
      "  [0.12492007 0.12490728 0.12486716 0.12483124 0.12501663 0.12521917\n",
      "   0.12497888 0.1252596  0.         0.        ]\n",
      "  [0.11101444 0.11100308 0.11096742 0.1109355  0.11110026 0.11128026\n",
      "   0.11106671 0.11131617 0.11131617 0.        ]\n",
      "  [0.09989457 0.09988434 0.09985226 0.09982353 0.09997179 0.10013375\n",
      "   0.0999416  0.10016607 0.10016607 0.10016607]]\n",
      "\n",
      " [[1.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.        ]\n",
      "  [0.49985975 0.5001403  0.         0.         0.         0.\n",
      "   0.         0.         0.         0.        ]\n",
      "  [0.3329623  0.3330336  0.33400407 0.         0.         0.\n",
      "   0.         0.         0.         0.        ]\n",
      "  [0.24977945 0.24967171 0.25025332 0.25029555 0.         0.\n",
      "   0.         0.         0.         0.        ]\n",
      "  [0.1998431  0.1998655  0.19990072 0.20003231 0.20035847 0.\n",
      "   0.         0.         0.         0.        ]\n",
      "  [0.16660614 0.16651092 0.16666041 0.16669704 0.16669574 0.16682968\n",
      "   0.         0.         0.         0.        ]\n",
      "  [0.14282052 0.14279835 0.14286603 0.14290085 0.14291462 0.14282288\n",
      "   0.1428767  0.         0.         0.        ]\n",
      "  [0.12503847 0.12508324 0.12499889 0.12495106 0.12490863 0.12495571\n",
      "   0.12498385 0.12508014 0.         0.        ]\n",
      "  [0.11088645 0.11104831 0.11119718 0.11111174 0.11117818 0.1110579\n",
      "   0.11108869 0.11107807 0.11135351 0.        ]\n",
      "  [0.10005799 0.10013337 0.09993912 0.09990206 0.09990182 0.09984995\n",
      "   0.09997167 0.100072   0.09998387 0.10018817]]], shape=(2, 10, 10), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 讓我們用目標語言（中文）的 batch\n",
    "# 來模擬 Decoder 處理的情況\n",
    "temp_q = temp_k = emb_tar\n",
    "temp_v = tf.cast(tf.math.greater(\n",
    "    tf.random.uniform(shape=emb_tar.shape), 0.5), tf.float32)\n",
    "\n",
    "# 將 look_ahead_mask 放入注意函式\n",
    "_, attention_weights = scaled_dot_product_attention(\n",
    "    temp_q, temp_k, temp_v, look_ahead_mask)\n",
    "\n",
    "print(\"attention_weights:\", attention_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 10), dtype=float32, numpy=\n",
       "array([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]], dtype=float32)>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_weights[:, 0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: tf.Tensor(\n",
      "[[[-0.02940199 -0.02748928  0.009587    0.02695702]\n",
      "  [ 0.02781546 -0.00051839 -0.04208876 -0.03737368]\n",
      "  [ 0.04085249 -0.01280526 -0.02142819  0.00932728]\n",
      "  [-0.04287729  0.00559031 -0.01610124  0.02254481]\n",
      "  [ 0.02117098 -0.03711741 -0.01554632  0.00051199]\n",
      "  [-0.00089258  0.03358604  0.04734136 -0.02150711]\n",
      "  [-0.01283282  0.03373785  0.0044058  -0.04340933]\n",
      "  [-0.01283282  0.03373785  0.0044058  -0.04340933]]\n",
      "\n",
      " [[-0.02940199 -0.02748928  0.009587    0.02695702]\n",
      "  [-0.02589555 -0.00853635  0.03899905  0.02993281]\n",
      "  [ 0.02303552 -0.0043821   0.03425056  0.02410031]\n",
      "  [-0.02790775  0.0453908  -0.00815301 -0.01390294]\n",
      "  [ 0.02987817  0.02534869  0.04416693 -0.03023093]\n",
      "  [ 0.03675361  0.04292214  0.0072703  -0.03425226]\n",
      "  [ 0.02117098 -0.03711741 -0.01554632  0.00051199]\n",
      "  [-0.00089258  0.03358604  0.04734136 -0.02150711]]], shape=(2, 8, 4), dtype=float32)\n",
      "output: tf.Tensor(\n",
      "[[[[-0.02940199 -0.02748928]\n",
      "   [ 0.02781546 -0.00051839]\n",
      "   [ 0.04085249 -0.01280526]\n",
      "   [-0.04287729  0.00559031]\n",
      "   [ 0.02117098 -0.03711741]\n",
      "   [-0.00089258  0.03358604]\n",
      "   [-0.01283282  0.03373785]\n",
      "   [-0.01283282  0.03373785]]\n",
      "\n",
      "  [[ 0.009587    0.02695702]\n",
      "   [-0.04208876 -0.03737368]\n",
      "   [-0.02142819  0.00932728]\n",
      "   [-0.01610124  0.02254481]\n",
      "   [-0.01554632  0.00051199]\n",
      "   [ 0.04734136 -0.02150711]\n",
      "   [ 0.0044058  -0.04340933]\n",
      "   [ 0.0044058  -0.04340933]]]\n",
      "\n",
      "\n",
      " [[[-0.02940199 -0.02748928]\n",
      "   [-0.02589555 -0.00853635]\n",
      "   [ 0.02303552 -0.0043821 ]\n",
      "   [-0.02790775  0.0453908 ]\n",
      "   [ 0.02987817  0.02534869]\n",
      "   [ 0.03675361  0.04292214]\n",
      "   [ 0.02117098 -0.03711741]\n",
      "   [-0.00089258  0.03358604]]\n",
      "\n",
      "  [[ 0.009587    0.02695702]\n",
      "   [ 0.03899905  0.02993281]\n",
      "   [ 0.03425056  0.02410031]\n",
      "   [-0.00815301 -0.01390294]\n",
      "   [ 0.04416693 -0.03023093]\n",
      "   [ 0.0072703  -0.03425226]\n",
      "   [-0.01554632  0.00051199]\n",
      "   [ 0.04734136 -0.02150711]]]], shape=(2, 2, 8, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "def split_heads(x, d_model, num_heads):\n",
    "    # x.shape: (batch_size, seq_len, d_model)\n",
    "    batch_size = tf.shape(x)[0]\n",
    "\n",
    "    # 我們要確保維度 `d_model` 可以被平分成 `num_heads` 個 `depth` 維度\n",
    "    assert d_model % num_heads == 0\n",
    "    depth = d_model // num_heads  # 這是分成多頭以後每個向量的維度 \n",
    "\n",
    "    # 將最後一個 d_model 維度分成 num_heads 個 depth 維度。\n",
    "    # 最後一個維度變成兩個維度，張量 x 從 3 維到 4 維\n",
    "    # (batch_size, seq_len, num_heads, depth)\n",
    "    reshaped_x = tf.reshape(x, shape=(batch_size, -1, num_heads, depth))\n",
    "\n",
    "    # 將 head 的維度拉前使得最後兩個維度為子詞以及其對應的 depth 向量\n",
    "    # (batch_size, num_heads, seq_len, depth)\n",
    "    output = tf.transpose(reshaped_x, perm=[0, 2, 1, 3])\n",
    "\n",
    "    return output\n",
    "\n",
    "# 我們的 `emb_inp` 裡頭的子詞本來就是 4 維的詞嵌入向量\n",
    "d_model = 4\n",
    "# 將 4 維詞嵌入向量分為 2 個 head 的 2 維矩陣\n",
    "num_heads = 2\n",
    "x = emb_inp\n",
    "\n",
    "output = split_heads(x, d_model, num_heads)  \n",
    "print(\"x:\", x)\n",
    "print(\"output:\", output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 實作一個執行多頭注意力機制的 keras layer\n",
    "# 在初始的時候指定輸出維度 `d_model` & `num_heads，\n",
    "# 在呼叫的時候輸入 `v`, `k`, `q` 以及 `mask`\n",
    "# 輸出跟 scaled_dot_product_attention 函式一樣有兩個：\n",
    "# output.shape            == (batch_size, seq_len_q, d_model)\n",
    "# attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "  # 在初始的時候建立一些必要參數\n",
    "  def __init__(self, d_model, num_heads):\n",
    "    super(MultiHeadAttention, self).__init__()\n",
    "    self.num_heads = num_heads # 指定要將 `d_model` 拆成幾個 heads\n",
    "    self.d_model = d_model # 在 split_heads 之前的基底維度\n",
    "    \n",
    "    assert d_model % self.num_heads == 0  # 前面看過，要確保可以平分\n",
    "    \n",
    "    self.depth = d_model // self.num_heads  # 每個 head 裡子詞的新的 repr. 維度\n",
    "    \n",
    "    self.wq = tf.keras.layers.Dense(d_model)  # 分別給 q, k, v 的 3 個線性轉換 \n",
    "    self.wk = tf.keras.layers.Dense(d_model)  # 注意我們並沒有指定 activation func\n",
    "    self.wv = tf.keras.layers.Dense(d_model)\n",
    "    \n",
    "    self.dense = tf.keras.layers.Dense(d_model)  # 多 heads 串接後通過的線性轉換\n",
    "    \n",
    "    \n",
    "  # 這跟我們前面看過的函式有 87% 相似\n",
    "  def split_heads(self, x, batch_size):\n",
    "    \"\"\"Split the last dimension into (num_heads, depth).\n",
    "    Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
    "    \"\"\"\n",
    "    x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "    return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "  \n",
    "  # multi-head attention 的實際執行流程，注意參數順序（這邊跟論文以及 TensorFlow 官方教學一致）\n",
    "  def call(self, v, k, q, mask):\n",
    "    batch_size = tf.shape(q)[0]\n",
    "    \n",
    "    # 將輸入的 q, k, v 都各自做一次線性轉換到 `d_model` 維空間\n",
    "    q = self.wq(q)  # (batch_size, seq_len, d_model)\n",
    "    k = self.wk(k)  # (batch_size, seq_len, d_model)\n",
    "    v = self.wv(v)  # (batch_size, seq_len, d_model)\n",
    "    \n",
    "    # 前面看過的，將最後一個 `d_model` 維度分成 `num_heads` 個 `depth` 維度\n",
    "    q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n",
    "    k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n",
    "    v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n",
    "    \n",
    "    # 利用 broadcasting 讓每個句子的每個 head 的 qi, ki, vi 都各自進行注意力機制\n",
    "    # 輸出會多一個 head 維度\n",
    "    scaled_attention, attention_weights = scaled_dot_product_attention(\n",
    "        q, k, v, mask)\n",
    "    # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n",
    "    # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "    \n",
    "    \n",
    "    # 跟我們在 `split_heads` 函式做的事情剛好相反，先做 transpose 再做 reshape\n",
    "    # 將 `num_heads` 個 `depth` 維度串接回原來的 `d_model` 維度\n",
    "    scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n",
    "    # (batch_size, seq_len_q, num_heads, depth)\n",
    "    concat_attention = tf.reshape(scaled_attention, \n",
    "                                  (batch_size, -1, self.d_model)) \n",
    "    # (batch_size, seq_len_q, d_model)\n",
    "\n",
    "    # 通過最後一個線性轉換\n",
    "    output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n",
    "        \n",
    "    return output, attention_weights    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d_model: 4\n",
      "num_heads: 2\n",
      "\n",
      "q.shape: (2, 8, 4)\n",
      "k.shape: (2, 8, 4)\n",
      "v.shape: (2, 8, 4)\n",
      "padding_mask.shape: (2, 1, 1, 8)\n",
      "output.shape: (2, 8, 4)\n",
      "attention_weights.shape: (2, 2, 8, 8)\n",
      "\n",
      "output: tf.Tensor(\n",
      "[[[ 0.00708912 -0.00861574 -0.00824888 -0.00125531]\n",
      "  [ 0.00704044 -0.00856255 -0.00824743 -0.00130908]\n",
      "  [ 0.00706525 -0.00858905 -0.00825153 -0.00128642]\n",
      "  [ 0.00707757 -0.00860305 -0.00824692 -0.00126657]\n",
      "  [ 0.00706903 -0.00859383 -0.00825078 -0.00128115]\n",
      "  [ 0.00706301 -0.00858597 -0.00824349 -0.00127383]\n",
      "  [ 0.00704599 -0.00856815 -0.00824224 -0.00129322]\n",
      "  [ 0.00704599 -0.00856815 -0.00824224 -0.00129322]]\n",
      "\n",
      " [[-0.0084104   0.00985345  0.0218877   0.01573933]\n",
      "  [-0.00840437  0.00984869  0.02188719  0.01574546]\n",
      "  [-0.00840402  0.00984824  0.02187178  0.01572827]\n",
      "  [-0.00845825  0.0099027   0.02189481  0.01570285]\n",
      "  [-0.00844073  0.00988418  0.02187815  0.01570553]\n",
      "  [-0.00845728  0.00990096  0.02187634  0.01568546]\n",
      "  [-0.00842486  0.00986613  0.02187385  0.01571093]\n",
      "  [-0.0084405   0.0098847   0.02188747  0.0157156 ]]], shape=(2, 8, 4), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# emb_inp.shape == (batch_size, seq_len, d_model)\n",
    "#               == (2, 8, 4)\n",
    "assert d_model == emb_inp.shape[-1]  == 4\n",
    "num_heads = 2\n",
    "\n",
    "print(f\"d_model: {d_model}\")\n",
    "print(f\"num_heads: {num_heads}\\n\")\n",
    "\n",
    "# 初始化一個 multi-head attention layer\n",
    "mha = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "# 簡單將 v, k, q 都設置為 `emb_inp`\n",
    "# 順便看看 padding mask 的作用。\n",
    "# 別忘記，第一個英文序列的最後兩個 tokens 是 <pad>\n",
    "v = k = q = emb_inp\n",
    "padding_mask = create_padding_mask(inp)\n",
    "print(\"q.shape:\", q.shape)\n",
    "print(\"k.shape:\", k.shape)\n",
    "print(\"v.shape:\", v.shape)\n",
    "print(\"padding_mask.shape:\", padding_mask.shape)\n",
    "\n",
    "output, attention_weights = mha(v, k, q, mask)\n",
    "print(\"output.shape:\", output.shape)\n",
    "print(\"attention_weights.shape:\", attention_weights.shape)\n",
    "\n",
    "print(\"\\noutput:\", output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 建立 Transformer 裡 Encoder / Decoder layer 都有使用到的 Feed Forward 元件\n",
    "def point_wise_feed_forward_network(d_model, dff):\n",
    "  \n",
    "  # 此 FFN 對輸入做兩個線性轉換，中間加了一個 ReLU activation func\n",
    "    return tf.keras.Sequential([\n",
    "      tf.keras.layers.Dense(dff, activation='relu'),  # (batch_size, seq_len, dff)\n",
    "      tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.shape: (64, 10, 512)\n",
      "out.shape: (64, 10, 512)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "batch_size = 64\n",
    "seq_len = 10\n",
    "d_model = 512\n",
    "dff = 2048\n",
    "\n",
    "x = tf.random.uniform((batch_size, seq_len, d_model))\n",
    "ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "out = ffn(x)\n",
    "print(\"x.shape:\", x.shape)\n",
    "print(\"out.shape:\", out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5, 4), dtype=float32, numpy=\n",
       "array([[ 2.8674245 , -2.174698  , -1.3073453 , -6.4233937 ],\n",
       "       [ 2.8674245 , -2.174698  , -1.3073453 , -6.4233937 ],\n",
       "       [ 3.6502066 , -0.97325826, -2.4126563 , -6.509499  ],\n",
       "       [ 3.6502066 , -0.97325826, -2.4126563 , -6.509499  ],\n",
       "       [ 3.6502066 , -0.97325826, -2.4126563 , -6.509499  ]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_model = 4 # FFN 的輸入輸出張量的最後一維皆為 `d_model`\n",
    "dff = 6\n",
    "\n",
    "# 建立一個小 FFN\n",
    "small_ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "# 懂子詞梗的站出來\n",
    "dummy_sentence = tf.constant([[5, 5, 6, 6], \n",
    "                              [5, 5, 6, 6], \n",
    "                              [9, 5, 2, 7], \n",
    "                              [9, 5, 2, 7],\n",
    "                              [9, 5, 2, 7]], dtype=tf.float32)\n",
    "small_ffn(dummy_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Encoder 裡頭會有 N 個 EncoderLayers，而每個 EncoderLayer 裡又有兩個 sub-layers: MHA & FFN\n",
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "  # Transformer 論文內預設 dropout rate 為 0.1\n",
    "  def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "    super(EncoderLayer, self).__init__()\n",
    "\n",
    "    self.mha = MultiHeadAttention(d_model, num_heads)\n",
    "    self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "    # layer norm 很常在 RNN-based 的模型被使用。一個 sub-layer 一個 layer norm\n",
    "    self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "    self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "    \n",
    "    # 一樣，一個 sub-layer 一個 dropout layer\n",
    "    self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "    self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "    \n",
    "  # 需要丟入 `training` 參數是因為 dropout 在訓練以及測試的行為有所不同\n",
    "  def call(self, x, training, mask):\n",
    "    # 除了 `attn`，其他張量的 shape 皆為 (batch_size, input_seq_len, d_model)\n",
    "    # attn.shape == (batch_size, num_heads, input_seq_len, input_seq_len)\n",
    "    \n",
    "    # sub-layer 1: MHA\n",
    "    # Encoder 利用注意機制關注自己當前的序列，因此 v, k, q 全部都是自己\n",
    "    # 另外別忘了我們還需要 padding mask 來遮住輸入序列中的 <pad> token\n",
    "    attn_output, attn = self.mha(x, x, x, mask)  \n",
    "    attn_output = self.dropout1(attn_output, training=training) \n",
    "    out1 = self.layernorm1(x + attn_output)  \n",
    "    \n",
    "    # sub-layer 2: FFN\n",
    "    ffn_output = self.ffn(out1) \n",
    "    ffn_output = self.dropout2(ffn_output, training=training)  # 記得 training\n",
    "    out2 = self.layernorm2(out1 + ffn_output)\n",
    "    \n",
    "    return out2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inp: tf.Tensor(\n",
      "[[8113  103    9 1066 7903 8114    0    0]\n",
      " [8113   16 4111 6735   12 2750 7903 8114]], shape=(2, 8), dtype=int64)\n",
      "--------------------\n",
      "padding_mask: tf.Tensor(\n",
      "[[[[0. 0. 0. 0. 0. 0. 1. 1.]]]\n",
      "\n",
      "\n",
      " [[[0. 0. 0. 0. 0. 0. 0. 0.]]]], shape=(2, 1, 1, 8), dtype=float32)\n",
      "--------------------\n",
      "emb_inp: tf.Tensor(\n",
      "[[[-0.02940199 -0.02748928  0.009587    0.02695702]\n",
      "  [ 0.02781546 -0.00051839 -0.04208876 -0.03737368]\n",
      "  [ 0.04085249 -0.01280526 -0.02142819  0.00932728]\n",
      "  [-0.04287729  0.00559031 -0.01610124  0.02254481]\n",
      "  [ 0.02117098 -0.03711741 -0.01554632  0.00051199]\n",
      "  [-0.00089258  0.03358604  0.04734136 -0.02150711]\n",
      "  [-0.01283282  0.03373785  0.0044058  -0.04340933]\n",
      "  [-0.01283282  0.03373785  0.0044058  -0.04340933]]\n",
      "\n",
      " [[-0.02940199 -0.02748928  0.009587    0.02695702]\n",
      "  [-0.02589555 -0.00853635  0.03899905  0.02993281]\n",
      "  [ 0.02303552 -0.0043821   0.03425056  0.02410031]\n",
      "  [-0.02790775  0.0453908  -0.00815301 -0.01390294]\n",
      "  [ 0.02987817  0.02534869  0.04416693 -0.03023093]\n",
      "  [ 0.03675361  0.04292214  0.0072703  -0.03425226]\n",
      "  [ 0.02117098 -0.03711741 -0.01554632  0.00051199]\n",
      "  [-0.00089258  0.03358604  0.04734136 -0.02150711]]], shape=(2, 8, 4), dtype=float32)\n",
      "--------------------\n",
      "enc_out: tf.Tensor(\n",
      "[[[-1.2790084  -0.4484179   0.2844689   1.4429574 ]\n",
      "  [ 0.8189746   0.89949    -1.5816016  -0.13686289]\n",
      "  [ 1.2944572  -0.2120691  -1.4614167   0.3790287 ]\n",
      "  [-1.4679327   1.1489065  -0.32746476  0.6464911 ]\n",
      "  [ 1.3575203  -1.3017013  -0.50809157  0.45227253]\n",
      "  [-0.4725885   1.4475989   0.2889553  -1.2639656 ]\n",
      "  [-0.2845264   1.6905513  -0.8935656  -0.5124593 ]\n",
      "  [-0.2845264   1.6905513  -0.8935656  -0.5124593 ]]\n",
      "\n",
      " [[-0.09594733 -1.3889819   0.05000272  1.4349266 ]\n",
      "  [-0.80072045 -1.177281    0.897902    1.0800993 ]\n",
      "  [ 1.1408303  -1.6084566   0.2656777   0.20194861]\n",
      "  [-0.73713     1.6235303  -0.90576535  0.01936519]\n",
      "  [ 1.3079005   0.28855962 -0.11566927 -1.4807909 ]\n",
      "  [ 1.1174244   0.8156742  -1.2963963  -0.63670236]\n",
      "  [ 1.3715899  -1.3202223  -0.45836774  0.40700006]\n",
      "  [ 0.38809723  0.7998333   0.524973   -1.7129036 ]]], shape=(2, 8, 4), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# 之後可以調的超參數。這邊為了 demo 設小一點\n",
    "d_model = 4\n",
    "num_heads = 2\n",
    "dff = 8\n",
    "\n",
    "# 新建一個使用上述參數的 Encoder Layer\n",
    "enc_layer = EncoderLayer(d_model, num_heads, dff)\n",
    "padding_mask = create_padding_mask(inp)  # 建立一個當前輸入 batch 使用的 padding mask\n",
    "enc_out = enc_layer(emb_inp, training=False, mask=padding_mask)  # (batch_size, seq_len, d_model)\n",
    "\n",
    "print(\"inp:\", inp)\n",
    "print(\"-\" * 20)\n",
    "print(\"padding_mask:\", padding_mask)\n",
    "print(\"-\" * 20)\n",
    "print(\"emb_inp:\", emb_inp)\n",
    "print(\"-\" * 20)\n",
    "print(\"enc_out:\", enc_out)\n",
    "assert emb_inp.shape == enc_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoder 裡頭會有 N 個 DecoderLayer，\n",
    "# 而 DecoderLayer 又有三個 sub-layers: 自注意的 MHA, 關注 Encoder 輸出的 MHA & FFN\n",
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        # 3 個 sub-layers 的主角們\n",
    "        self.mha1 = MultiHeadAttention(d_model, num_heads)\n",
    "        self.mha2 = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "        # 定義每個 sub-layer 用的 LayerNorm\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        # 定義每個 sub-layer 用的 Dropout\n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout3 = tf.keras.layers.Dropout(rate)\n",
    "    \n",
    "    \n",
    "    def call(self, x, enc_output, training, combined_mask, inp_padding_mask):\n",
    "        # 所有 sub-layers 的主要輸出皆為 (batch_size, target_seq_len, d_model)\n",
    "        # enc_output 為 Encoder 輸出序列，shape 為 (batch_size, input_seq_len, d_model)\n",
    "        # attn_weights_block_1 則為 (batch_size, num_heads, target_seq_len, target_seq_len)\n",
    "        # attn_weights_block_2 則為 (batch_size, num_heads, target_seq_len, input_seq_len)\n",
    "\n",
    "\n",
    "        # sub-layer 1: Decoder layer 自己對輸出序列做注意力。\n",
    "        # 我們同時需要 look ahead mask 以及輸出序列的 padding mask \n",
    "        # 來避免前面已生成的子詞關注到未來的子詞以及 <pad>\n",
    "        attn1, attn_weights_block1 = self.mha1(x, x, x, combined_mask)\n",
    "        attn1 = self.dropout1(attn1, training=training)\n",
    "        out1 = self.layernorm1(attn1 + x)\n",
    "\n",
    "        # sub-layer 2: Decoder layer 關注 Encoder 的最後輸出\n",
    "        # 記得我們一樣需要對 Encoder 的輸出套用 padding mask 避免關注到 <pad>\n",
    "        attn2, attn_weights_block2 = self.mha2(\n",
    "            enc_output, enc_output, out1, inp_padding_mask)  # (batch_size, target_seq_len, d_model)\n",
    "        attn2 = self.dropout2(attn2, training=training)\n",
    "        out2 = self.layernorm2(attn2 + out1)  # (batch_size, target_seq_len, d_model)\n",
    "\n",
    "        # sub-layer 3: FFN 部分跟 Encoder layer 完全一樣\n",
    "        ffn_output = self.ffn(out2)  # (batch_size, target_seq_len, d_model)\n",
    "\n",
    "        ffn_output = self.dropout3(ffn_output, training=training)\n",
    "        out3 = self.layernorm3(ffn_output + out2)  # (batch_size, target_seq_len, d_model)\n",
    "\n",
    "        # 除了主要輸出 `out3` 以外，輸出 multi-head 注意權重方便之後理解模型內部狀況\n",
    "        return out3, attn_weights_block1, attn_weights_block2    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tar: tf.Tensor(\n",
      "[[4205   10  241   86   27    3 4206    0    0    0]\n",
      " [4205  165  489  398  191   14    7  560    3 4206]], shape=(2, 10), dtype=int64)\n",
      "--------------------\n",
      "tar_padding_mask: tf.Tensor(\n",
      "[[[[0. 0. 0. 0. 0. 0. 0. 1. 1. 1.]]]\n",
      "\n",
      "\n",
      " [[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]]], shape=(2, 1, 1, 10), dtype=float32)\n",
      "--------------------\n",
      "look_ahead_mask: tf.Tensor(\n",
      "[[0. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [0. 0. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [0. 0. 0. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [0. 0. 0. 0. 1. 1. 1. 1. 1. 1.]\n",
      " [0. 0. 0. 0. 0. 1. 1. 1. 1. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 1. 1. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 1. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]], shape=(10, 10), dtype=float32)\n",
      "--------------------\n",
      "combined_mask: tf.Tensor(\n",
      "[[[[0. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "   [0. 0. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "   [0. 0. 0. 1. 1. 1. 1. 1. 1. 1.]\n",
      "   [0. 0. 0. 0. 1. 1. 1. 1. 1. 1.]\n",
      "   [0. 0. 0. 0. 0. 1. 1. 1. 1. 1.]\n",
      "   [0. 0. 0. 0. 0. 0. 1. 1. 1. 1.]\n",
      "   [0. 0. 0. 0. 0. 0. 0. 1. 1. 1.]\n",
      "   [0. 0. 0. 0. 0. 0. 0. 1. 1. 1.]\n",
      "   [0. 0. 0. 0. 0. 0. 0. 1. 1. 1.]\n",
      "   [0. 0. 0. 0. 0. 0. 0. 1. 1. 1.]]]\n",
      "\n",
      "\n",
      " [[[0. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "   [0. 0. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "   [0. 0. 0. 1. 1. 1. 1. 1. 1. 1.]\n",
      "   [0. 0. 0. 0. 1. 1. 1. 1. 1. 1.]\n",
      "   [0. 0. 0. 0. 0. 1. 1. 1. 1. 1.]\n",
      "   [0. 0. 0. 0. 0. 0. 1. 1. 1. 1.]\n",
      "   [0. 0. 0. 0. 0. 0. 0. 1. 1. 1.]\n",
      "   [0. 0. 0. 0. 0. 0. 0. 0. 1. 1.]\n",
      "   [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "   [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]]], shape=(2, 1, 10, 10), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "tar_padding_mask = create_padding_mask(tar)\n",
    "look_ahead_mask = create_look_ahead_mask(tar.shape[-1])\n",
    "combined_mask = tf.maximum(tar_padding_mask, look_ahead_mask)\n",
    "\n",
    "print(\"tar:\", tar)\n",
    "print(\"-\" * 20)\n",
    "print(\"tar_padding_mask:\", tar_padding_mask)\n",
    "print(\"-\" * 20)\n",
    "print(\"look_ahead_mask:\", look_ahead_mask)\n",
    "print(\"-\" * 20)\n",
    "print(\"combined_mask:\", combined_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emb_tar: tf.Tensor(\n",
      "[[[ 0.02309941  0.01578486 -0.03650398  0.04221227]\n",
      "  [ 0.02180279 -0.03836104 -0.0052525  -0.01320656]\n",
      "  [-0.0190685  -0.03266476 -0.03334127  0.04303731]\n",
      "  [ 0.03618959 -0.00743835  0.01616282  0.04101736]\n",
      "  [ 0.00099171  0.03080915 -0.03851545  0.03447074]\n",
      "  [-0.02579685  0.02527661  0.04145398 -0.04307295]\n",
      "  [-0.00221508  0.03029467  0.02829352  0.04788197]\n",
      "  [-0.02839622  0.04139889 -0.00255186 -0.03752811]\n",
      "  [-0.02839622  0.04139889 -0.00255186 -0.03752811]\n",
      "  [-0.02839622  0.04139889 -0.00255186 -0.03752811]]\n",
      "\n",
      " [[ 0.02309941  0.01578486 -0.03650398  0.04221227]\n",
      "  [-0.00205498  0.04796953 -0.00836701  0.03551394]\n",
      "  [-0.0049132  -0.03055991  0.03992986 -0.02458232]\n",
      "  [ 0.02458249 -0.02872031  0.01778464 -0.02678457]\n",
      "  [ 0.03532792  0.02878738 -0.00509523 -0.0492527 ]\n",
      "  [-0.00865723 -0.02110739 -0.03051426 -0.02639371]\n",
      "  [ 0.02155342 -0.00126004  0.00749636 -0.00912453]\n",
      "  [-0.01634679  0.00527266  0.01074455  0.02497052]\n",
      "  [-0.02579685  0.02527661  0.04145398 -0.04307295]\n",
      "  [-0.00221508  0.03029467  0.02829352  0.04788197]]], shape=(2, 10, 4), dtype=float32)\n",
      "--------------------\n",
      "enc_out: tf.Tensor(\n",
      "[[[-1.2790084  -0.4484179   0.2844689   1.4429574 ]\n",
      "  [ 0.8189746   0.89949    -1.5816016  -0.13686289]\n",
      "  [ 1.2944572  -0.2120691  -1.4614167   0.3790287 ]\n",
      "  [-1.4679327   1.1489065  -0.32746476  0.6464911 ]\n",
      "  [ 1.3575203  -1.3017013  -0.50809157  0.45227253]\n",
      "  [-0.4725885   1.4475989   0.2889553  -1.2639656 ]\n",
      "  [-0.2845264   1.6905513  -0.8935656  -0.5124593 ]\n",
      "  [-0.2845264   1.6905513  -0.8935656  -0.5124593 ]]\n",
      "\n",
      " [[-0.09594733 -1.3889819   0.05000272  1.4349266 ]\n",
      "  [-0.80072045 -1.177281    0.897902    1.0800993 ]\n",
      "  [ 1.1408303  -1.6084566   0.2656777   0.20194861]\n",
      "  [-0.73713     1.6235303  -0.90576535  0.01936519]\n",
      "  [ 1.3079005   0.28855962 -0.11566927 -1.4807909 ]\n",
      "  [ 1.1174244   0.8156742  -1.2963963  -0.63670236]\n",
      "  [ 1.3715899  -1.3202223  -0.45836774  0.40700006]\n",
      "  [ 0.38809723  0.7998333   0.524973   -1.7129036 ]]], shape=(2, 8, 4), dtype=float32)\n",
      "--------------------\n",
      "dec_out: tf.Tensor(\n",
      "[[[ 0.24520996  0.9124286  -1.6827396   0.52510095]\n",
      "  [ 1.4490802  -1.3585964  -0.20380807  0.11332427]\n",
      "  [-0.4028599  -0.530555   -0.7824085   1.7158233 ]\n",
      "  [ 0.8677076  -1.2873157  -0.65675914  1.0763671 ]\n",
      "  [-0.12391096  1.1748888  -1.5384061   0.48742837]\n",
      "  [-0.24596532  0.5464132   1.1906333  -1.4910812 ]\n",
      "  [-1.7202684   0.7570852   0.5242093   0.4389739 ]\n",
      "  [-0.02801281  1.3993088   0.05628167 -1.4275779 ]\n",
      "  [-0.02801275  1.3993089   0.05628166 -1.427578  ]\n",
      "  [-0.02801275  1.3993089   0.05628166 -1.427578  ]]\n",
      "\n",
      " [[ 0.11055017  1.2710681  -1.5327494   0.15113118]\n",
      "  [-0.36196986  1.7159778  -0.7391547  -0.614853  ]\n",
      "  [ 0.1960543  -1.0118155   1.5491637  -0.7334023 ]\n",
      "  [ 1.3901527  -1.410459    0.20747498 -0.18716866]\n",
      "  [ 0.93018496  0.9748236  -0.52242076 -1.3825879 ]\n",
      "  [ 1.5940356  -0.124051   -1.1611568  -0.30882782]\n",
      "  [ 1.412476   -1.2407097   0.388878   -0.5606442 ]\n",
      "  [-1.4752018  -0.31004715  1.151554    0.633695  ]\n",
      "  [-0.50473964  0.42634052  1.37344    -1.2950408 ]\n",
      "  [-1.6073794  -0.07197207  0.86243844  0.816913  ]]], shape=(2, 10, 4), dtype=float32)\n",
      "--------------------\n",
      "dec_self_attn_weights.shape: (2, 2, 10, 10)\n",
      "dec_enc_attn_weights: (2, 2, 10, 8)\n"
     ]
    }
   ],
   "source": [
    "# 超參數\n",
    "d_model = 4\n",
    "num_heads = 2\n",
    "dff = 8\n",
    "dec_layer = DecoderLayer(d_model, num_heads, dff)\n",
    "\n",
    "# 來源、目標語言的序列都需要 padding mask\n",
    "inp_padding_mask = create_padding_mask(inp)\n",
    "tar_padding_mask = create_padding_mask(tar)\n",
    "\n",
    "# masked MHA 用的遮罩，把 padding 跟未來子詞都蓋住\n",
    "look_ahead_mask = create_look_ahead_mask(tar.shape[-1])\n",
    "combined_mask = tf.maximum(tar_padding_mask, look_ahead_mask)\n",
    "\n",
    "# 實際初始一個 decoder layer 並做 3 個 sub-layers 的計算\n",
    "dec_out, dec_self_attn_weights, dec_enc_attn_weights = dec_layer(\n",
    "    emb_tar, enc_out, False, combined_mask, inp_padding_mask)\n",
    "\n",
    "print(\"emb_tar:\", emb_tar)\n",
    "print(\"-\" * 20)\n",
    "print(\"enc_out:\", enc_out)\n",
    "print(\"-\" * 20)\n",
    "print(\"dec_out:\", dec_out)\n",
    "assert emb_tar.shape == dec_out.shape\n",
    "print(\"-\" * 20)\n",
    "print(\"dec_self_attn_weights.shape:\", dec_self_attn_weights.shape)\n",
    "print(\"dec_enc_attn_weights:\", dec_enc_attn_weights.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 50, 512), dtype=float32, numpy=\n",
       "array([[[ 0.        ,  0.        ,  0.        , ...,  1.        ,\n",
       "          1.        ,  1.        ],\n",
       "        [ 0.84147096,  0.8218562 ,  0.8019618 , ...,  1.        ,\n",
       "          1.        ,  1.        ],\n",
       "        [ 0.9092974 ,  0.9364147 ,  0.95814437, ...,  1.        ,\n",
       "          1.        ,  1.        ],\n",
       "        ...,\n",
       "        [ 0.12357312,  0.97718984, -0.24295525, ...,  0.9999863 ,\n",
       "          0.99998724,  0.99998814],\n",
       "        [-0.76825464,  0.7312359 ,  0.63279754, ...,  0.9999857 ,\n",
       "          0.9999867 ,  0.9999876 ],\n",
       "        [-0.95375264, -0.14402692,  0.99899054, ...,  0.9999851 ,\n",
       "          0.9999861 ,  0.9999871 ]]], dtype=float32)>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 以下直接參考 TensorFlow 官方 tutorial \n",
    "def get_angles(pos, i, d_model):\n",
    "    angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
    "    return pos * angle_rates\n",
    "\n",
    "def positional_encoding(position, d_model):\n",
    "    angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
    "                          np.arange(d_model)[np.newaxis, :],\n",
    "                          d_model)\n",
    "\n",
    "    # apply sin to even indices in the array; 2i\n",
    "    sines = np.sin(angle_rads[:, 0::2])\n",
    "\n",
    "    # apply cos to odd indices in the array; 2i+1\n",
    "    cosines = np.cos(angle_rads[:, 1::2])\n",
    "\n",
    "    pos_encoding = np.concatenate([sines, cosines], axis=-1)\n",
    "\n",
    "    pos_encoding = pos_encoding[np.newaxis, ...]\n",
    "\n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)\n",
    "\n",
    "\n",
    "seq_len = 50\n",
    "d_model = 512\n",
    "\n",
    "pos_encoding = positional_encoding(seq_len, d_model)\n",
    "pos_encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.pcolormesh(pos_encoding[0], cmap='RdBu')\n",
    "plt.xlabel('d_model')\n",
    "plt.xlim((0, 512))\n",
    "plt.ylabel('Position')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "  # Encoder 的初始參數除了本來就要給 EncoderLayer 的參數還多了：\n",
    "  # - num_layers: 決定要有幾個 EncoderLayers, 前面影片中的 `N`\n",
    "  # - input_vocab_size: 用來把索引轉成詞嵌入向量\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, rate=0.1):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "\n",
    "        self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n",
    "        self.pos_encoding = positional_encoding(input_vocab_size, self.d_model)\n",
    "\n",
    "        # 建立 `num_layers` 個 EncoderLayers\n",
    "        self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) \n",
    "                           for _ in range(num_layers)]\n",
    "\n",
    "        self.dropout = tf.keras.layers.Dropout(rate)\n",
    "        \n",
    "\n",
    "    def call(self, x, training, mask):\n",
    "        # 輸入的 x.shape == (batch_size, input_seq_len)\n",
    "        # 以下各 layer 的輸出皆為 (batch_size, input_seq_len, d_model)\n",
    "        input_seq_len = tf.shape(x)[1]\n",
    "\n",
    "        # 將 2 維的索引序列轉成 3 維的詞嵌入張量，並依照論文乘上 sqrt(d_model)\n",
    "        # 再加上對應長度的位置編碼\n",
    "        x = self.embedding(x)\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        x += self.pos_encoding[:, :input_seq_len, :]\n",
    "\n",
    "        # 對 embedding 跟位置編碼的總合做 regularization\n",
    "        # 這在 Decoder 也會做\n",
    "        x = self.dropout(x, training=training)\n",
    "    \n",
    "        # 通過 N 個 EncoderLayer 做編碼\n",
    "        for i, enc_layer in enumerate(self.enc_layers):\n",
    "            x = enc_layer(x, training, mask)\n",
    "            # 以下只是用來 demo EncoderLayer outputs\n",
    "            #print('-' * 20)\n",
    "            #print(f\"EncoderLayer {i + 1}'s output:\", x)\n",
    "\n",
    "\n",
    "        return x        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inp: tf.Tensor(\n",
      "[[8113  103    9 1066 7903 8114    0    0]\n",
      " [8113   16 4111 6735   12 2750 7903 8114]], shape=(2, 8), dtype=int64)\n",
      "--------------------\n",
      "enc_out: tf.Tensor(\n",
      "[[[-0.7849332  -0.591968   -0.33270508  1.7096064 ]\n",
      "  [-0.50706536 -0.5110137  -0.7082318   1.7263108 ]\n",
      "  [-0.39270186 -0.03102632 -1.1583618   1.5820901 ]\n",
      "  [-0.5561631   0.38050288 -1.2407897   1.4164499 ]\n",
      "  [-0.90431994  0.19381054 -0.847289    1.5577984 ]\n",
      "  [-0.9732156  -0.22992761 -0.46524626  1.6683894 ]\n",
      "  [-0.84681964 -0.5434473  -0.31013626  1.7004031 ]\n",
      "  [-0.6243278  -0.5679047  -0.539001    1.7312335 ]]\n",
      "\n",
      " [[-0.7742376  -0.6076475  -0.32800585  1.7098908 ]\n",
      "  [-0.47978237 -0.56156063 -0.6860291   1.7273722 ]\n",
      "  [-0.30068296 -0.07366994 -1.1973958   1.5717487 ]\n",
      "  [-0.5147843   0.27872467 -1.229085    1.4651445 ]\n",
      "  [-0.8963448   0.26754576 -0.895411    1.52421   ]\n",
      "  [-0.9755362  -0.22618702 -0.4656963   1.6674196 ]\n",
      "  [-0.87600446 -0.54483974 -0.27099538  1.6918396 ]\n",
      "  [-0.6013047  -0.59936655 -0.5306772   1.7313485 ]]], shape=(2, 8, 4), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# 超參數\n",
    "num_layers = 2 # 2 層的 Encoder\n",
    "d_model = 4\n",
    "num_heads = 2\n",
    "dff = 8\n",
    "input_vocab_size = subword_encoder_en.vocab_size + 2 # 記得加上 <start>, <end>\n",
    "\n",
    "# 初始化一個 Encoder\n",
    "encoder = Encoder(num_layers, d_model, num_heads, dff, input_vocab_size)\n",
    "\n",
    "# 將 2 維的索引序列丟入 Encoder 做編碼\n",
    "enc_out = encoder(inp, training=False, mask=None)\n",
    "print(\"inp:\", inp)\n",
    "print(\"-\" * 20)\n",
    "print(\"enc_out:\", enc_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.layers.Layer):\n",
    "    # 初始參數跟 Encoder 只差在用 `target_vocab_size` 而非 `inp_vocab_size`\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size, rate=0.1):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "\n",
    "        # 為中文（目標語言）建立詞嵌入層\n",
    "        self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n",
    "        self.pos_encoding = positional_encoding(target_vocab_size, self.d_model)\n",
    "\n",
    "        self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate) \n",
    "                           for _ in range(num_layers)]\n",
    "        self.dropout = tf.keras.layers.Dropout(rate)\n",
    "        \n",
    "        \n",
    "    # 呼叫時的參數跟 DecoderLayer 一模一樣\n",
    "    def call(self, x, enc_output, training, combined_mask, inp_padding_mask):\n",
    "    \n",
    "        tar_seq_len = tf.shape(x)[1]\n",
    "        attention_weights = {}  # 用來存放每個 Decoder layer 的注意權重\n",
    "\n",
    "        # 這邊跟 Encoder 做的事情完全一樣\n",
    "        x = self.embedding(x)  # (batch_size, tar_seq_len, d_model)\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        x += self.pos_encoding[:, :tar_seq_len, :]\n",
    "        x = self.dropout(x, training=training)\n",
    "\n",
    "\n",
    "        for i, dec_layer in enumerate(self.dec_layers):\n",
    "      x, block1, block2 = dec_layer(x, enc_output, training,combined_mask, inp_padding_mask)\n",
    "\n",
    "      # 將從每個 Decoder layer 取得的注意權重全部存下來回傳，方便我們觀察\n",
    "      attention_weights['decoder_layer{}_block1'.format(i + 1)] = block1\n",
    "      attention_weights['decoder_layer{}_block2'.format(i + 1)] = block2\n",
    "\n",
    "        # x.shape == (batch_size, tar_seq_len, d_model)\n",
    "        return x, attention_weights        \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
