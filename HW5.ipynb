{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from pprint import pprint\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(level=\"ERROR\")\n",
    "\n",
    "np.set_printoptions(suppress=True) #讓 numpy 不要顯示科學記號"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"nmt\"\n",
    "en_vocab_file = os.path.join(output_dir, \"en_vocab\")\n",
    "zh_vocab_file = os.path.join(output_dir, \"zh_vocab\")\n",
    "checkpoint_path = os.path.join(output_dir, \"checkpoints\")\n",
    "log_dir = os.path.join(output_dir, 'logs')\n",
    "download_dir = \"tensorflow-datasets/downloads\"\n",
    "\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{Split('train'): ['newscommentary_v14',\n",
      "                  'wikititles_v1',\n",
      "                  'uncorpus_v1',\n",
      "                  'casia2015',\n",
      "                  'casict2011',\n",
      "                  'casict2015',\n",
      "                  'datum2015',\n",
      "                  'datum2017',\n",
      "                  'neu2017'],\n",
      " Split('validation'): ['newstest2018']}\n"
     ]
    }
   ],
   "source": [
    "tmp_builder = tfds.builder(\"wmt19_translate/zh-en\")\n",
    "pprint(tmp_builder.subsets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = tfds.translate.wmt.WmtConfig(\n",
    "    version=\"0.0.3\",\n",
    "    language_pair=(\"zh\", \"en\"),\n",
    "    subsets={\n",
    "        tfds.Split.TRAIN: [\"newscommentary_v14\"]\n",
    "    },\n",
    ")\n",
    "builder = tfds.builder(\"wmt_translate\", config=config)\n",
    "builder.download_and_prepare(download_dir=download_dir)\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<PrefetchDataset shapes: ((), ()), types: (tf.string, tf.string)>\n",
      "<PrefetchDataset shapes: ((), ()), types: (tf.string, tf.string)>\n"
     ]
    }
   ],
   "source": [
    "examples = builder.as_dataset(split=['train[:20%]','train[20%:21%]','train[21%:]'], as_supervised=True)\n",
    "\n",
    "train_examples, val_examples, _ = examples\n",
    "print(train_examples)\n",
    "print(val_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b'The fear is real and visceral, and politicians ignore it at their peril.', shape=(), dtype=string)\n",
      "tf.Tensor(b'\\xe8\\xbf\\x99\\xe7\\xa7\\x8d\\xe6\\x81\\x90\\xe6\\x83\\xa7\\xe6\\x98\\xaf\\xe7\\x9c\\x9f\\xe5\\xae\\x9e\\xe8\\x80\\x8c\\xe5\\x86\\x85\\xe5\\x9c\\xa8\\xe7\\x9a\\x84\\xe3\\x80\\x82 \\xe5\\xbf\\xbd\\xe8\\xa7\\x86\\xe5\\xae\\x83\\xe7\\x9a\\x84\\xe6\\x94\\xbf\\xe6\\xb2\\xbb\\xe5\\xae\\xb6\\xe4\\xbb\\xac\\xe5\\x89\\x8d\\xe9\\x80\\x94\\xe5\\xa0\\xaa\\xe5\\xbf\\xa7\\xe3\\x80\\x82', shape=(), dtype=string)\n",
      "----------\n",
      "tf.Tensor(b'In fact, the German political landscape needs nothing more than a truly liberal party, in the US sense of the word \\xe2\\x80\\x9cliberal\\xe2\\x80\\x9d \\xe2\\x80\\x93 a champion of the cause of individual freedom.', shape=(), dtype=string)\n",
      "tf.Tensor(b'\\xe4\\xba\\x8b\\xe5\\xae\\x9e\\xe4\\xb8\\x8a\\xef\\xbc\\x8c\\xe5\\xbe\\xb7\\xe5\\x9b\\xbd\\xe6\\x94\\xbf\\xe6\\xb2\\xbb\\xe5\\xb1\\x80\\xe5\\x8a\\xbf\\xe9\\x9c\\x80\\xe8\\xa6\\x81\\xe7\\x9a\\x84\\xe4\\xb8\\x8d\\xe8\\xbf\\x87\\xe6\\x98\\xaf\\xe4\\xb8\\x80\\xe4\\xb8\\xaa\\xe7\\xac\\xa6\\xe5\\x90\\x88\\xe7\\xbe\\x8e\\xe5\\x9b\\xbd\\xe6\\x89\\x80\\xe8\\xb0\\x93\\xe2\\x80\\x9c\\xe8\\x87\\xaa\\xe7\\x94\\xb1\\xe2\\x80\\x9d\\xe5\\xae\\x9a\\xe4\\xb9\\x89\\xe7\\x9a\\x84\\xe7\\x9c\\x9f\\xe6\\xad\\xa3\\xe7\\x9a\\x84\\xe8\\x87\\xaa\\xe7\\x94\\xb1\\xe5\\x85\\x9a\\xe6\\xb4\\xbe\\xef\\xbc\\x8c\\xe4\\xb9\\x9f\\xe5\\xb0\\xb1\\xe6\\x98\\xaf\\xe4\\xb8\\xaa\\xe4\\xba\\xba\\xe8\\x87\\xaa\\xe7\\x94\\xb1\\xe4\\xba\\x8b\\xe4\\xb8\\x9a\\xe7\\x9a\\x84\\xe5\\x80\\xa1\\xe5\\xaf\\xbc\\xe8\\x80\\x85\\xe3\\x80\\x82', shape=(), dtype=string)\n",
      "----------\n",
      "tf.Tensor(b'Shifting to renewable-energy sources will require enormous effort and major infrastructure investment.', shape=(), dtype=string)\n",
      "tf.Tensor(b'\\xe5\\xbf\\x85\\xe9\\xa1\\xbb\\xe4\\xbb\\x98\\xe5\\x87\\xba\\xe5\\xb7\\xa8\\xe5\\xa4\\xa7\\xe7\\x9a\\x84\\xe5\\x8a\\xaa\\xe5\\x8a\\x9b\\xe5\\x92\\x8c\\xe5\\x9f\\xba\\xe7\\xa1\\x80\\xe8\\xae\\xbe\\xe6\\x96\\xbd\\xe6\\x8a\\x95\\xe8\\xb5\\x84\\xe6\\x89\\x8d\\xe8\\x83\\xbd\\xe5\\xae\\x8c\\xe6\\x88\\x90\\xe5\\x90\\x91\\xe5\\x8f\\xaf\\xe5\\x86\\x8d\\xe7\\x94\\x9f\\xe8\\x83\\xbd\\xe6\\xba\\x90\\xe7\\x9a\\x84\\xe8\\xbf\\x87\\xe6\\xb8\\xa1\\xe3\\x80\\x82', shape=(), dtype=string)\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "for en, zh in train_examples.take(3):\n",
    "    print(en)\n",
    "    print(zh)\n",
    "    print('-' * 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The fear is real and visceral, and politicians ignore it at their peril.\n",
      "这种恐惧是真实而内在的。 忽视它的政治家们前途堪忧。\n",
      "----------\n",
      "In fact, the German political landscape needs nothing more than a truly liberal party, in the US sense of the word “liberal” – a champion of the cause of individual freedom.\n",
      "事实上，德国政治局势需要的不过是一个符合美国所谓“自由”定义的真正的自由党派，也就是个人自由事业的倡导者。\n",
      "----------\n",
      "Shifting to renewable-energy sources will require enormous effort and major infrastructure investment.\n",
      "必须付出巨大的努力和基础设施投资才能完成向可再生能源的过渡。\n",
      "----------\n",
      "In this sense, it is critical to recognize the fundamental difference between “urban villages” and their rural counterparts.\n",
      "在这方面，关键在于认识到“城市村落”和农村村落之间的根本区别。\n",
      "----------\n",
      "A strong European voice, such as Nicolas Sarkozy’s during the French presidency of the EU, may make a difference, but only for six months, and at the cost of reinforcing other European countries’ nationalist feelings in reaction to the expression of “Gallic pride.”\n",
      "法国担任轮值主席国期间尼古拉·萨科奇统一的欧洲声音可能让人耳目一新，但这种声音却只持续了短短六个月，而且付出了让其他欧洲国家在面对“高卢人的骄傲”时民族主义情感进一步被激发的代价。\n",
      "----------\n",
      "Most of Japan’s bondholders are nationals (if not the central bank) and have an interest in political stability.\n",
      "日本债券持有人大多为本国国民（甚至中央银行 ） ， 政治稳定符合他们的利益。\n",
      "----------\n",
      "Paul Romer, one of the originators of new growth theory, has accused some leading names, including the Nobel laureate Robert Lucas, of what he calls “mathiness” – using math to obfuscate rather than clarify.\n",
      "新增长理论创始人之一的保罗·罗默（Paul Romer）也批评一些著名经济学家，包括诺贝尔奖获得者罗伯特·卢卡斯（Robert Lucas）在内，说他们“数学性 ” （ 罗默的用语）太重，结果是让问题变得更加模糊而不是更加清晰。\n",
      "----------\n",
      "It is, in fact, a capsule depiction of the United States Federal Reserve and the European Central Bank.\n",
      "事实上，这就是对美联储和欧洲央行的简略描述。\n",
      "----------\n",
      "Given these variables, the degree to which migration is affected by asylum-seekers will not be easy to predict or control.\n",
      "考虑到这些变量，移民受寻求庇护者的影响程度很难预测或控制。\n",
      "----------\n",
      "WASHINGTON, DC – In the 2016 American presidential election, Hillary Clinton and Donald Trump agreed that the US economy is suffering from dilapidated infrastructure, and both called for greater investment in renovating and upgrading the country’s public capital stock.\n",
      "华盛顿—在2016年美国总统选举中，希拉里·克林顿和唐纳德·特朗普都认为美国经济饱受基础设施陈旧的拖累，两人都要求加大投资用于修缮和升级美国公共资本存量。\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sample_examples = []\n",
    "num_samples = 10\n",
    "\n",
    "for en_t, zh_t in train_examples.take(num_samples):\n",
    "    en = en_t.numpy().decode(\"utf-8\")\n",
    "    zh = zh_t.numpy().decode(\"utf-8\")\n",
    "\n",
    "    print(en)\n",
    "    print(zh)\n",
    "    print('-' * 10)\n",
    "\n",
    "    # 之後用來簡單評估模型的訓練情況\n",
    "    sample_examples.append((en, zh))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "沒有已建立的字典，從頭建立。\n",
      "字典大小：8113\n",
      "前 10 個 subwords：[', ', 'the_', 'of_', 'to_', 'and_', 's_', 'in_', 'a_', 'is_', 'that_']\n",
      "\n",
      "Wall time: 1min 51s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "## load_from_file 函式嘗試讀取之前已經建好的字典檔案\n",
    "try:\n",
    "    subword_encoder_en = tfds.deprecated.text.SubwordTextEncoder.load_from_file(en_vocab_file)\n",
    "    print(f\"載入已建立的字典： {en_vocab_file}\")\n",
    "except:\n",
    "    print(\"沒有已建立的字典，從頭建立。\")\n",
    "    subword_encoder_en = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(\n",
    "      (en.numpy() for en, _ in train_examples), \n",
    "      target_vocab_size=2**13) # 有需要可以調整字典大小\n",
    "  \n",
    "    # 將字典檔案存下以方便下次 warmstart\n",
    "    subword_encoder_en.save_to_file(en_vocab_file)\n",
    "  \n",
    "\n",
    "print(f\"字典大小：{subword_encoder_en.vocab_size}\")\n",
    "print(f\"前 10 個 subwords：{subword_encoder_en.subwords[:10]}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3461, 7889, 9, 3502, 4379, 1134, 7903]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_string = 'Taiwan is beautiful.'\n",
    "indices = subword_encoder_en.encode(sample_string)\n",
    "indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index     Subword\n",
      "---------------\n",
      " 3461     Taiwan\n",
      " 7889      \n",
      "    9     is \n",
      " 3502     bea\n",
      " 4379     uti\n",
      " 1134     ful\n",
      " 7903     .\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"{0:10}{1:6}\".format(\"Index\", \"Subword\"))\n",
    "print(\"-\" * 15)\n",
    "for idx in indices:\n",
    "    subword = subword_encoder_en.decode([idx])\n",
    "    print('{0:5}{1:6}'.format(idx, ' ' * 5 + subword))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Taiwan is beautiful.', 'Taiwan is beautiful.')\n"
     ]
    }
   ],
   "source": [
    "sample_string = 'Taiwan is beautiful.'\n",
    "indices = subword_encoder_en.encode(sample_string)\n",
    "decoded_string = subword_encoder_en.decode(indices)\n",
    "assert decoded_string == sample_string\n",
    "pprint((sample_string, decoded_string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#為中文建立一個字典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "沒有已建立的字典，從頭建立。\n",
      "字典大小：4205\n",
      "前 10 個 subwords：['的', '，', '。', '国', '在', '是', '一', '和', '不', '这']\n",
      "\n",
      "Wall time: 11min 15s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#https://www.tensorflow.org/datasets/api_docs/python/tfds/deprecated/text/SubwordTextEncoder\n",
    "try:\n",
    "    subword_encoder_zh = tfds.deprecated.text.SubwordTextEncoder.load_from_file(zh_vocab_file)\n",
    "    print(f\"載入已建立的字典： {zh_vocab_file}\")\n",
    "except:\n",
    "    print(\"沒有已建立的字典，從頭建立。\")\n",
    "    subword_encoder_zh = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(\n",
    "      (zh.numpy() for _, zh in train_examples), \n",
    "      target_vocab_size=2**13, # 有需要可以調整字典大小\n",
    "      max_subword_length=1) # 每一個中文字就是字典裡的一個單位\n",
    "\n",
    "    # 將字典檔案存下以方便下次 warmstart \n",
    "    subword_encoder_zh.save_to_file(zh_vocab_file)\n",
    "\n",
    "print(f\"字典大小：{subword_encoder_zh.vocab_size}\")\n",
    "print(f\"前 10 個 subwords：{subword_encoder_zh.subwords[:10]}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "这种恐惧是真实而内在的。 忽视它的政治家们前途堪忧。\n",
      "[10, 151, 574, 1298, 6, 374, 55, 29, 193, 5, 1, 3, 3981, 931, 431, 125, 1, 17, 124, 33, 20, 97, 1089, 1247, 861, 3]\n"
     ]
    }
   ],
   "source": [
    "sample_string = sample_examples[0][1]\n",
    "indices = subword_encoder_zh.encode(sample_string)\n",
    "print(sample_string)\n",
    "print(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[英中原文]（轉換前）\n",
      "The eurozone’s collapse forces a major realignment of European politics.\n",
      "欧元区的瓦解强迫欧洲政治进行一次重大改组。\n",
      "\n",
      "--------------------\n",
      "\n",
      "[英中序列]（轉換後）\n",
      "[16, 900, 11, 6, 1527, 874, 8, 230, 2259, 2728, 239, 3, 89, 1236, 7903]\n",
      "[44, 202, 168, 1, 852, 201, 231, 592, 44, 87, 17, 124, 106, 38, 7, 279, 86, 18, 212, 265, 3]\n"
     ]
    }
   ],
   "source": [
    "en = \"The eurozone’s collapse forces a major realignment of European politics.\"\n",
    "zh = \"欧元区的瓦解强迫欧洲政治进行一次重大改组。\"\n",
    "\n",
    "# 將文字轉成為 subword indices\n",
    "en_indices = subword_encoder_en.encode(en)\n",
    "zh_indices = subword_encoder_zh.encode(zh)\n",
    "\n",
    "print(\"[英中原文]（轉換前）\")\n",
    "print(en)\n",
    "print(zh)\n",
    "print()\n",
    "print('-' * 20)\n",
    "print()\n",
    "print(\"[英中序列]（轉換後）\")\n",
    "print(en_indices)\n",
    "print(zh_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def encode(en_t, zh_t):\n",
    "  # 因為字典的索引從 0 開始，\n",
    "  # 我們可以使用 subword_encoder_en.vocab_size 這個值作為 BOS 的索引值\n",
    "  # 用 subword_encoder_en.vocab_size + 1 作為 EOS 的索引值\n",
    "  en_indices = [subword_encoder_en.vocab_size] + subword_encoder_en.encode(\n",
    "      en_t.numpy()) + [subword_encoder_en.vocab_size + 1]\n",
    "  # 同理，不過是使用中文字典的最後一個索引 + 1\n",
    "  zh_indices = [subword_encoder_zh.vocab_size] + subword_encoder_zh.encode(\n",
    "      zh_t.numpy()) + [subword_encoder_zh.vocab_size + 1]\n",
    "  \n",
    "  return en_indices, zh_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "英文 BOS 的 index： 8113\n",
      "英文 EOS 的 index： 8114\n",
      "中文 BOS 的 index： 4205\n",
      "中文 EOS 的 index： 4206\n",
      "\n",
      "輸入為 2 個 Tensors：\n",
      "(<tf.Tensor: shape=(), dtype=string, numpy=b'The fear is real and visceral, and politicians ignore it at their peril.'>,\n",
      " <tf.Tensor: shape=(), dtype=string, numpy=b'\\xe8\\xbf\\x99\\xe7\\xa7\\x8d\\xe6\\x81\\x90\\xe6\\x83\\xa7\\xe6\\x98\\xaf\\xe7\\x9c\\x9f\\xe5\\xae\\x9e\\xe8\\x80\\x8c\\xe5\\x86\\x85\\xe5\\x9c\\xa8\\xe7\\x9a\\x84\\xe3\\x80\\x82 \\xe5\\xbf\\xbd\\xe8\\xa7\\x86\\xe5\\xae\\x83\\xe7\\x9a\\x84\\xe6\\x94\\xbf\\xe6\\xb2\\xbb\\xe5\\xae\\xb6\\xe4\\xbb\\xac\\xe5\\x89\\x8d\\xe9\\x80\\x94\\xe5\\xa0\\xaa\\xe5\\xbf\\xa7\\xe3\\x80\\x82'>)\n",
      "---------------\n",
      "輸出為 2 個索引序列：\n",
      "([8113,\n",
      "  16,\n",
      "  1284,\n",
      "  9,\n",
      "  243,\n",
      "  5,\n",
      "  1275,\n",
      "  1756,\n",
      "  156,\n",
      "  1,\n",
      "  5,\n",
      "  1016,\n",
      "  5566,\n",
      "  21,\n",
      "  38,\n",
      "  33,\n",
      "  2982,\n",
      "  7965,\n",
      "  7903,\n",
      "  8114],\n",
      " [4205,\n",
      "  10,\n",
      "  151,\n",
      "  574,\n",
      "  1298,\n",
      "  6,\n",
      "  374,\n",
      "  55,\n",
      "  29,\n",
      "  193,\n",
      "  5,\n",
      "  1,\n",
      "  3,\n",
      "  3981,\n",
      "  931,\n",
      "  431,\n",
      "  125,\n",
      "  1,\n",
      "  17,\n",
      "  124,\n",
      "  33,\n",
      "  20,\n",
      "  97,\n",
      "  1089,\n",
      "  1247,\n",
      "  861,\n",
      "  3,\n",
      "  4206])\n"
     ]
    }
   ],
   "source": [
    "en_t, zh_t = next(iter(train_examples))\n",
    "en_indices, zh_indices = encode(en_t, zh_t)\n",
    "print('英文 BOS 的 index：', subword_encoder_en.vocab_size)\n",
    "print('英文 EOS 的 index：', subword_encoder_en.vocab_size + 1)\n",
    "print('中文 BOS 的 index：', subword_encoder_zh.vocab_size)\n",
    "print('中文 EOS 的 index：', subword_encoder_zh.vocab_size + 1)\n",
    "\n",
    "print('\\n輸入為 2 個 Tensors：')\n",
    "pprint((en_t, zh_t))\n",
    "print('-' * 15)\n",
    "print('輸出為 2 個索引序列：')\n",
    "pprint((en_indices, zh_indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[8113   16 1284    9  243    5 1275 1756  156    1    5 1016 5566   21\n",
      "   38   33 2982 7965 7903 8114], shape=(20,), dtype=int64)\n",
      "tf.Tensor(\n",
      "[4205   10  151  574 1298    6  374   55   29  193    5    1    3 3981\n",
      "  931  431  125    1   17  124   33   20   97 1089 1247  861    3 4206], shape=(28,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "def tf_encode(en_t, zh_t):\n",
    "  # 在 `tf_encode` 函式裡頭的 `en_t` 與 `zh_t` 都不是 Eager Tensors\n",
    "  # 要到 `tf.py_funtion` 裡頭才是\n",
    "  # 另外因為索引都是整數，所以使用 `tf.int64`\n",
    "  return tf.py_function(encode, [en_t, zh_t], [tf.int64, tf.int64])\n",
    "\n",
    "# `tmp_dataset` 為說明用資料集，說明完所有重要的 func，\n",
    "# 我們會從頭建立一個正式的 `train_dataset`\n",
    "tmp_dataset = train_examples.map(tf_encode)\n",
    "en_indices, zh_indices = next(iter(tmp_dataset))\n",
    "print(en_indices)\n",
    "print(zh_indices)\n",
    "## tmp_dataset 的輸出已經是兩個索引序列，而非原文字串"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 40\n",
    "\n",
    "def filter_max_length(en, zh, max_length=MAX_LENGTH):\n",
    "  # en, zh 分別代表英文與中文的索引序列\n",
    "  return tf.logical_and(tf.size(en) <= max_length,\n",
    "                        tf.size(zh) <= max_length)\n",
    "\n",
    "# tf.data.Dataset.filter(func) 只會回傳 func 為真的例子\n",
    "tmp_dataset = tmp_dataset.filter(filter_max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "所有英文與中文序列長度都不超過 40 個 tokens\n",
      "訓練資料集裡總共有 29784 筆數據\n"
     ]
    }
   ],
   "source": [
    "# 因為我們數據量小可以這樣 count\n",
    "num_examples = 0\n",
    "for en_indices, zh_indices in tmp_dataset:\n",
    "    cond1 = len(en_indices) <= MAX_LENGTH\n",
    "    cond2 = len(zh_indices) <= MAX_LENGTH\n",
    "    assert cond1 and cond2\n",
    "    num_examples += 1\n",
    "\n",
    "print(f\"所有英文與中文序列長度都不超過 {MAX_LENGTH} 個 tokens\")\n",
    "print(f\"訓練資料集裡總共有 {num_examples} 筆數據\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "英文索引序列的 batch\n",
      "tf.Tensor(\n",
      "[[8113   16 1284 ...    0    0    0]\n",
      " [8113 1894 1302 ...    0    0    0]\n",
      " [8113   44   40 ...    0    0    0]\n",
      " ...\n",
      " [8113  122  506 ...    0    0    0]\n",
      " [8113   16  215 ...    0    0    0]\n",
      " [8113 7443 7889 ...    0    0    0]], shape=(64, 39), dtype=int64)\n",
      "--------------------\n",
      "中文索引序列的 batch\n",
      "tf.Tensor(\n",
      "[[4205   10  151 ...    0    0    0]\n",
      " [4205  206  275 ...    0    0    0]\n",
      " [4205    5   10 ...    0    0    0]\n",
      " ...\n",
      " [4205   34    6 ...    0    0    0]\n",
      " [4205  317  256 ...    0    0    0]\n",
      " [4205  167  326 ...    0    0    0]], shape=(64, 40), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 64\n",
    "# 將 batch 裡的所有序列都 pad 到同樣長度\n",
    "tmp_dataset = tmp_dataset.padded_batch(BATCH_SIZE, padded_shapes=([-1], [-1]))\n",
    "en_batch, zh_batch = next(iter(tmp_dataset))\n",
    "print(\"英文索引序列的 batch\")\n",
    "print(en_batch)\n",
    "print('-' * 20)\n",
    "print(\"中文索引序列的 batch\")\n",
    "print(zh_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 40\n",
    "BATCH_SIZE = 128\n",
    "BUFFER_SIZE = 15000\n",
    "\n",
    "# 訓練集\n",
    "train_dataset = (train_examples  # 輸出：(英文句子, 中文句子)\n",
    "                 .map(tf_encode) # 輸出：(英文索引序列, 中文索引序列)\n",
    "                 .filter(filter_max_length) # 同上，且序列長度都不超過 40\n",
    "                 .cache() # 加快讀取數據\n",
    "                 .shuffle(BUFFER_SIZE) # 將例子洗牌確保隨機性\n",
    "                 .padded_batch(BATCH_SIZE, # 將 batch 裡的序列都 pad 到一樣長度\n",
    "                               padded_shapes=([-1], [-1]))\n",
    "                 .prefetch(tf.data.experimental.AUTOTUNE)) # 加速\n",
    "# 驗證集\n",
    "val_dataset = (val_examples\n",
    "               .map(tf_encode)\n",
    "               .filter(filter_max_length)\n",
    "               .padded_batch(BATCH_SIZE, \n",
    "                             padded_shapes=([-1], [-1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "英文索引序列的 batch\n",
      "tf.Tensor(\n",
      "[[8113   87 5599 ...    0    0    0]\n",
      " [8113  998    5 ...    0    0    0]\n",
      " [8113 6266 1606 ...    0    0    0]\n",
      " ...\n",
      " [8113   16 6445 ...    0    0    0]\n",
      " [8113 5363   39 ...    0    0    0]\n",
      " [8113 5150 1662 ...    0    0    0]], shape=(128, 40), dtype=int64)\n",
      "--------------------\n",
      "中文索引序列的 batch\n",
      "tf.Tensor(\n",
      "[[4205   10  126 ...    0    0    0]\n",
      " [4205    4   33 ...    0    0    0]\n",
      " [4205   52   11 ...    0    0    0]\n",
      " ...\n",
      " [4205  526  538 ...    0    0    0]\n",
      " [4205  266 1380 ...    0    0    0]\n",
      " [4205   45  116 ...    0    0    0]], shape=(128, 40), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "en_batch, zh_batch = next(iter(train_dataset))\n",
    "print(\"英文索引序列的 batch\")\n",
    "print(en_batch)\n",
    "print('-' * 20)\n",
    "print(\"中文索引序列的 batch\")\n",
    "print(zh_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('It is important.', '这很重要。'),\n",
      " ('The numbers speak for themselves.', '数字证明了一切。')]\n"
     ]
    }
   ],
   "source": [
    "demo_examples = [\n",
    "    (\"It is important.\", \"这很重要。\"),\n",
    "    (\"The numbers speak for themselves.\", \"数字证明了一切。\"),\n",
    "]\n",
    "pprint(demo_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inp: tf.Tensor(\n",
      "[[8113  103    9 1066 7903 8114    0    0]\n",
      " [8113   16 4111 6735   12 2750 7903 8114]], shape=(2, 8), dtype=int64)\n",
      "\n",
      "tar: tf.Tensor(\n",
      "[[4205   10  241   86   27    3 4206    0    0    0]\n",
      " [4205  165  489  398  191   14    7  560    3 4206]], shape=(2, 10), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 2\n",
    "demo_examples = tf.data.Dataset.from_tensor_slices((\n",
    "    [en for en, _ in demo_examples], [zh for _, zh in demo_examples]\n",
    "))\n",
    "\n",
    "# 將兩個句子透過之前定義的字典轉換成子詞的序列（sequence of subwords）\n",
    "# 並添加 padding token: <pad> 來確保 batch 裡的句子有一樣長度\n",
    "demo_dataset = demo_examples.map(tf_encode)\\\n",
    "  .padded_batch(batch_size, padded_shapes=([-1], [-1]))\n",
    "\n",
    "# 取出這個 demo dataset 裡唯一一個 batch\n",
    "inp, tar = next(iter(demo_dataset))\n",
    "print('inp:', inp)\n",
    "print('' * 10)\n",
    "print('tar:', tar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(2, 8, 4), dtype=float32, numpy=\n",
       " array([[[-0.02940199, -0.02748928,  0.009587  ,  0.02695702],\n",
       "         [ 0.02781546, -0.00051839, -0.04208876, -0.03737368],\n",
       "         [ 0.04085249, -0.01280526, -0.02142819,  0.00932728],\n",
       "         [-0.04287729,  0.00559031, -0.01610124,  0.02254481],\n",
       "         [ 0.02117098, -0.03711741, -0.01554632,  0.00051199],\n",
       "         [-0.00089258,  0.03358604,  0.04734136, -0.02150711],\n",
       "         [-0.01283282,  0.03373785,  0.0044058 , -0.04340933],\n",
       "         [-0.01283282,  0.03373785,  0.0044058 , -0.04340933]],\n",
       " \n",
       "        [[-0.02940199, -0.02748928,  0.009587  ,  0.02695702],\n",
       "         [-0.02589555, -0.00853635,  0.03899905,  0.02993281],\n",
       "         [ 0.02303552, -0.0043821 ,  0.03425056,  0.02410031],\n",
       "         [-0.02790775,  0.0453908 , -0.00815301, -0.01390294],\n",
       "         [ 0.02987817,  0.02534869,  0.04416693, -0.03023093],\n",
       "         [ 0.03675361,  0.04292214,  0.0072703 , -0.03425226],\n",
       "         [ 0.02117098, -0.03711741, -0.01554632,  0.00051199],\n",
       "         [-0.00089258,  0.03358604,  0.04734136, -0.02150711]]],\n",
       "       dtype=float32)>,\n",
       " <tf.Tensor: shape=(2, 10, 4), dtype=float32, numpy=\n",
       " array([[[ 0.02309941,  0.01578486, -0.03650398,  0.04221227],\n",
       "         [ 0.02180279, -0.03836104, -0.0052525 , -0.01320656],\n",
       "         [-0.0190685 , -0.03266476, -0.03334127,  0.04303731],\n",
       "         [ 0.03618959, -0.00743835,  0.01616282,  0.04101736],\n",
       "         [ 0.00099171,  0.03080915, -0.03851545,  0.03447074],\n",
       "         [-0.02579685,  0.02527661,  0.04145398, -0.04307295],\n",
       "         [-0.00221508,  0.03029467,  0.02829352,  0.04788197],\n",
       "         [-0.02839622,  0.04139889, -0.00255186, -0.03752811],\n",
       "         [-0.02839622,  0.04139889, -0.00255186, -0.03752811],\n",
       "         [-0.02839622,  0.04139889, -0.00255186, -0.03752811]],\n",
       " \n",
       "        [[ 0.02309941,  0.01578486, -0.03650398,  0.04221227],\n",
       "         [-0.00205498,  0.04796953, -0.00836701,  0.03551394],\n",
       "         [-0.0049132 , -0.03055991,  0.03992986, -0.02458232],\n",
       "         [ 0.02458249, -0.02872031,  0.01778464, -0.02678457],\n",
       "         [ 0.03532792,  0.02878738, -0.00509523, -0.0492527 ],\n",
       "         [-0.00865723, -0.02110739, -0.03051426, -0.02639371],\n",
       "         [ 0.02155342, -0.00126004,  0.00749636, -0.00912453],\n",
       "         [-0.01634679,  0.00527266,  0.01074455,  0.02497052],\n",
       "         [-0.02579685,  0.02527661,  0.04145398, -0.04307295],\n",
       "         [-0.00221508,  0.03029467,  0.02829352,  0.04788197]]],\n",
       "       dtype=float32)>)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# + 2 是因為我們額外加了 <start> 以及 <end> tokens\n",
    "vocab_size_en = subword_encoder_en.vocab_size + 2\n",
    "vocab_size_zh = subword_encoder_zh.vocab_size + 2\n",
    "\n",
    "# 為了方便 demo, 將詞彙轉換到一個 4 維的詞嵌入空間\n",
    "d_model = 4\n",
    "embedding_layer_en = tf.keras.layers.Embedding(vocab_size_en, d_model)\n",
    "embedding_layer_zh = tf.keras.layers.Embedding(vocab_size_zh, d_model)\n",
    "\n",
    "emb_inp = embedding_layer_en(inp)\n",
    "emb_tar = embedding_layer_zh(tar)\n",
    "emb_inp, emb_tar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tar[0]: tf.Tensor([0 0 0], shape=(3,), dtype=int64)\n",
      "--------------------\n",
      "emb_tar[0]: tf.Tensor(\n",
      "[[-0.02839622  0.04139889 -0.00255186 -0.03752811]\n",
      " [-0.02839622  0.04139889 -0.00255186 -0.03752811]\n",
      " [-0.02839622  0.04139889 -0.00255186 -0.03752811]], shape=(3, 4), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(\"tar[0]:\", tar[0][-3:])\n",
    "print(\"-\" * 20)\n",
    "print(\"emb_tar[0]:\", emb_tar[0][-3:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 1, 1, 8), dtype=float32, numpy=\n",
       "array([[[[0., 0., 0., 0., 0., 0., 1., 1.]]],\n",
       "\n",
       "\n",
       "       [[[0., 0., 0., 0., 0., 0., 0., 0.]]]], dtype=float32)>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_padding_mask(seq):\n",
    "    # padding mask 的工作就是把索引序列中為 0 的位置設為 1\n",
    "    mask = tf.cast(tf.equal(seq, 0), tf.float32)\n",
    "    return mask[:, tf.newaxis, tf.newaxis, :] #　broadcasting\n",
    "\n",
    "inp_mask = create_padding_mask(inp)\n",
    "inp_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inp: tf.Tensor(\n",
      "[[8113  103    9 1066 7903 8114    0    0]\n",
      " [8113   16 4111 6735   12 2750 7903 8114]], shape=(2, 8), dtype=int64)\n",
      "--------------------\n",
      "tf.squeeze(inp_mask): tf.Tensor(\n",
      "[[0. 0. 0. 0. 0. 0. 1. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]], shape=(2, 8), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"inp:\", inp)\n",
    "print(\"-\" * 20)\n",
    "print(\"tf.squeeze(inp_mask):\", tf.squeeze(inp_mask))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 8, 4), dtype=float32, numpy=\n",
       "array([[[1., 0., 0., 0.],\n",
       "        [0., 1., 0., 1.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [1., 0., 1., 0.],\n",
       "        [1., 0., 1., 0.],\n",
       "        [0., 1., 0., 1.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [0., 1., 0., 1.]],\n",
       "\n",
       "       [[1., 0., 1., 1.],\n",
       "        [1., 0., 1., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [1., 0., 1., 0.],\n",
       "        [0., 1., 0., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 1., 0.]]], dtype=float32)>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 設定一個 seed 確保我們每次都拿到一樣的隨機結果\n",
    "tf.random.set_seed(9527)\n",
    "\n",
    "# 自注意力機制：查詢 `q` 跟鍵值 `k` 都是 `emb_inp`\n",
    "q = emb_inp\n",
    "k = emb_inp\n",
    "# 簡單產生一個跟 `emb_inp` 同樣 shape 的 binary vector\n",
    "v = tf.cast(tf.math.greater(tf.random.uniform(shape=emb_inp.shape), 0.5), tf.float32)\n",
    "v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(q, k, v, mask):\n",
    "    # 將 `q`、 `k` 做點積再 scale\n",
    "    matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n",
    "\n",
    "    dk = tf.cast(tf.shape(k)[-1], tf.float32)  # 取得 seq_k 的序列長度\n",
    "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)  # scale by sqrt(dk)\n",
    "\n",
    "    # 將遮罩「加」到被丟入 softmax 前的 logits\n",
    "    if mask is not None:\n",
    "        scaled_attention_logits += (mask * -1e9)\n",
    "\n",
    "    # 取 softmax 是為了得到總和為 1 的比例之後對 `v` 做加權平均\n",
    "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n",
    "\n",
    "    # 以注意權重對 v 做加權平均（weighted average）\n",
    "    output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n",
    "\n",
    "    return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output: tf.Tensor(\n",
      "[[[0.3753399  0.3747662  0.37508252 0.49974045]\n",
      "  [0.37475583 0.37513748 0.37496054 0.50021064]\n",
      "  [0.37499675 0.3749313  0.37495598 0.50008714]\n",
      "  [0.3752308  0.3748534  0.37511384 0.49978137]\n",
      "  [0.37511206 0.37487078 0.37499475 0.49998224]\n",
      "  [0.3747127  0.37527195 0.3749196  0.5001556 ]\n",
      "  [0.3746645  0.37529942 0.3749698  0.5001717 ]\n",
      "  [0.3746645  0.37529942 0.3749698  0.5001717 ]]\n",
      "\n",
      " [[0.6251311  0.24970885 0.62504077 0.37487447]\n",
      "  [0.6251049  0.24979301 0.6250699  0.37489533]\n",
      "  [0.624972   0.25000775 0.62488645 0.37499285]\n",
      "  [0.6250523  0.25008556 0.62523466 0.3750197 ]\n",
      "  [0.6248019  0.25033873 0.62493974 0.375137  ]\n",
      "  [0.624817   0.25040787 0.6249676  0.37516844]\n",
      "  [0.62495446 0.24994777 0.62482053 0.37499923]\n",
      "  [0.624902   0.25021926 0.6250832  0.37507576]]], shape=(2, 8, 4), dtype=float32)\n",
      "--------------------\n",
      "attention_weights: tf.Tensor(\n",
      "[[[0.125177   0.12488609 0.12497426 0.12512204 0.12504087 0.12496053\n",
      "   0.1249196  0.1249196 ]\n",
      "  [0.12482883 0.1252137  0.12507316 0.12488215 0.12504484 0.12489024\n",
      "   0.12503353 0.12503353]\n",
      "  [0.12495689 0.12511314 0.12515587 0.12492786 0.12511201 0.12490202\n",
      "   0.12491614 0.12491614]\n",
      "  [0.12510477 0.1249222  0.12492798 0.12517215 0.12495389 0.12494341\n",
      "   0.12498779 0.12498779]\n",
      "  [0.12502298 0.12508428 0.12511148 0.12495323 0.12513587 0.12488083\n",
      "   0.12490567 0.12490567]\n",
      "  [0.12492479 0.1249118  0.12488367 0.12492491 0.12486301 0.12522845\n",
      "   0.1251317  0.1251317 ]\n",
      "  [0.12485838 0.1250296  0.12487227 0.12494376 0.12486234 0.12510614\n",
      "   0.1251637  0.1251637 ]\n",
      "  [0.12485838 0.1250296  0.12487227 0.12494376 0.12486234 0.12510614\n",
      "   0.1251637  0.1251637 ]]\n",
      "\n",
      " [[0.12516564 0.12514925 0.12503944 0.12495811 0.12489024 0.12481861\n",
      "   0.12502952 0.12494919]\n",
      "  [0.12510231 0.12516384 0.12505984 0.1249413  0.12495543 0.12483758\n",
      "   0.12491484 0.12502488]\n",
      "  [0.12498508 0.12505239 0.12510279 0.12486783 0.12504385 0.12496389\n",
      "   0.1249669  0.12501723]\n",
      "  [0.12493413 0.1249642  0.12489816 0.12518294 0.1250127  0.12507287\n",
      "   0.12485447 0.12508056]\n",
      "  [0.12479828 0.1249103  0.12500612 0.12494461 0.12519619 0.12514254\n",
      "   0.12485797 0.12514399]\n",
      "  [0.12476055 0.12482633 0.12496009 0.12503868 0.1251765  0.12523137\n",
      "   0.1248959  0.12511061]\n",
      "  [0.12505144 0.12498362 0.12504315 0.1249003  0.12497186 0.12497591\n",
      "   0.12516436 0.12490926]\n",
      "  [0.12485649 0.12497903 0.12497881 0.12501174 0.12514329 0.12507597\n",
      "   0.12479473 0.12515998]]], shape=(2, 8, 8), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "mask = None\n",
    "output, attention_weights = scaled_dot_product_attention(q, k, v, mask)\n",
    "print(\"output:\", output)\n",
    "print(\"-\" * 20)\n",
    "print(\"attention_weights:\", attention_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inp: tf.Tensor(\n",
      "[[8113  103    9 1066 7903 8114    0    0]\n",
      " [8113   16 4111 6735   12 2750 7903 8114]], shape=(2, 8), dtype=int64)\n",
      "--------------------\n",
      "inp_mask: tf.Tensor(\n",
      "[[[[0. 0. 0. 0. 0. 0. 1. 1.]]]\n",
      "\n",
      "\n",
      " [[[0. 0. 0. 0. 0. 0. 0. 0.]]]], shape=(2, 1, 1, 8), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "def create_padding_mask(seq):\n",
    "    # padding mask 的工作就是把索引序列中為 0 的位置設為 1\n",
    "    mask = tf.cast(tf.equal(seq, 0), tf.float32)\n",
    "    return mask[:, tf.newaxis, tf.newaxis, :] #　broadcasting\n",
    "\n",
    "print(\"inp:\", inp)\n",
    "inp_mask = create_padding_mask(inp)\n",
    "print(\"-\" * 20)\n",
    "print(\"inp_mask:\", inp_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention_weights: tf.Tensor(\n",
      "[[[0.16686688 0.16647908 0.16659662 0.16679361 0.16668542 0.16657832\n",
      "   0.         0.        ]\n",
      "  [0.16645332 0.16696653 0.16677913 0.16652443 0.16674137 0.16653521\n",
      "   0.         0.        ]\n",
      "  [0.16657193 0.16678022 0.16683717 0.16653322 0.16677871 0.16649878\n",
      "   0.         0.        ]\n",
      "  [0.16680093 0.1665575  0.16656522 0.16689077 0.16659975 0.16658579\n",
      "   0.         0.        ]\n",
      "  [0.16665538 0.1667371  0.16677335 0.16656241 0.16680586 0.1664659\n",
      "   0.         0.        ]\n",
      "  [0.1666249  0.16660757 0.16657004 0.16662505 0.16654249 0.16702992\n",
      "   0.         0.        ]\n",
      "  [0.16655058 0.16677895 0.1665691  0.16666447 0.16655585 0.16688107\n",
      "   0.         0.        ]\n",
      "  [0.16655058 0.16677895 0.1665691  0.16666447 0.16655585 0.16688107\n",
      "   0.         0.        ]]\n",
      "\n",
      " [[0.12516564 0.12514925 0.12503944 0.12495811 0.12489024 0.12481861\n",
      "   0.12502952 0.12494919]\n",
      "  [0.12510231 0.12516384 0.12505984 0.1249413  0.12495543 0.12483758\n",
      "   0.12491484 0.12502488]\n",
      "  [0.12498508 0.12505239 0.12510279 0.12486783 0.12504385 0.12496389\n",
      "   0.1249669  0.12501723]\n",
      "  [0.12493413 0.1249642  0.12489816 0.12518294 0.1250127  0.12507287\n",
      "   0.12485447 0.12508056]\n",
      "  [0.12479828 0.1249103  0.12500612 0.12494461 0.12519619 0.12514254\n",
      "   0.12485797 0.12514399]\n",
      "  [0.12476055 0.12482633 0.12496009 0.12503868 0.1251765  0.12523137\n",
      "   0.1248959  0.12511061]\n",
      "  [0.12505144 0.12498362 0.12504315 0.1249003  0.12497186 0.12497591\n",
      "   0.12516436 0.12490926]\n",
      "  [0.12485649 0.12497903 0.12497881 0.12501174 0.12514329 0.12507597\n",
      "   0.12479473 0.12515998]]], shape=(2, 8, 8), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# 這次讓我們將 padding mask 放入注意函式並觀察\n",
    "# 注意權重的變化\n",
    "mask = tf.squeeze(inp_mask, axis=1) # (batch_size, 1, seq_len_q)\n",
    "_, attention_weights = scaled_dot_product_attention(q, k, v, mask)\n",
    "print(\"attention_weights:\", attention_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 8, 2), dtype=float32, numpy=\n",
       "array([[[0.        , 0.        ],\n",
       "        [0.        , 0.        ],\n",
       "        [0.        , 0.        ],\n",
       "        [0.        , 0.        ],\n",
       "        [0.        , 0.        ],\n",
       "        [0.        , 0.        ],\n",
       "        [0.        , 0.        ],\n",
       "        [0.        , 0.        ]],\n",
       "\n",
       "       [[0.12502952, 0.12494919],\n",
       "        [0.12491484, 0.12502488],\n",
       "        [0.1249669 , 0.12501723],\n",
       "        [0.12485447, 0.12508056],\n",
       "        [0.12485797, 0.12514399],\n",
       "        [0.1248959 , 0.12511061],\n",
       "        [0.12516436, 0.12490926],\n",
       "        [0.12479473, 0.12515998]]], dtype=float32)>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 事實上也不完全是上句話的翻譯，\n",
    "# 因為我們在第一個維度還是把兩個句子都拿出來方便你比較\n",
    "attention_weights[:, :, -2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emb_tar: tf.Tensor(\n",
      "[[[ 0.02309941  0.01578486 -0.03650398  0.04221227]\n",
      "  [ 0.02180279 -0.03836104 -0.0052525  -0.01320656]\n",
      "  [-0.0190685  -0.03266476 -0.03334127  0.04303731]\n",
      "  [ 0.03618959 -0.00743835  0.01616282  0.04101736]\n",
      "  [ 0.00099171  0.03080915 -0.03851545  0.03447074]\n",
      "  [-0.02579685  0.02527661  0.04145398 -0.04307295]\n",
      "  [-0.00221508  0.03029467  0.02829352  0.04788197]\n",
      "  [-0.02839622  0.04139889 -0.00255186 -0.03752811]\n",
      "  [-0.02839622  0.04139889 -0.00255186 -0.03752811]\n",
      "  [-0.02839622  0.04139889 -0.00255186 -0.03752811]]\n",
      "\n",
      " [[ 0.02309941  0.01578486 -0.03650398  0.04221227]\n",
      "  [-0.00205498  0.04796953 -0.00836701  0.03551394]\n",
      "  [-0.0049132  -0.03055991  0.03992986 -0.02458232]\n",
      "  [ 0.02458249 -0.02872031  0.01778464 -0.02678457]\n",
      "  [ 0.03532792  0.02878738 -0.00509523 -0.0492527 ]\n",
      "  [-0.00865723 -0.02110739 -0.03051426 -0.02639371]\n",
      "  [ 0.02155342 -0.00126004  0.00749636 -0.00912453]\n",
      "  [-0.01634679  0.00527266  0.01074455  0.02497052]\n",
      "  [-0.02579685  0.02527661  0.04145398 -0.04307295]\n",
      "  [-0.00221508  0.03029467  0.02829352  0.04788197]]], shape=(2, 10, 4), dtype=float32)\n",
      "--------------------\n",
      "look_ahead_mask tf.Tensor(\n",
      "[[0. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [0. 0. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [0. 0. 0. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [0. 0. 0. 0. 1. 1. 1. 1. 1. 1.]\n",
      " [0. 0. 0. 0. 0. 1. 1. 1. 1. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 1. 1. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 1. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]], shape=(10, 10), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# 建立一個 2 維矩陣，維度為 (size, size)，\n",
    "# 其遮罩為一個右上角的三角形\n",
    "def create_look_ahead_mask(size):\n",
    "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "    return mask  # (seq_len, seq_len)\n",
    "\n",
    "seq_len = emb_tar.shape[1] # 注意這次我們用中文的詞嵌入張量 `emb_tar`\n",
    "look_ahead_mask = create_look_ahead_mask(seq_len)\n",
    "print(\"emb_tar:\", emb_tar)\n",
    "print(\"-\" * 20)\n",
    "print(\"look_ahead_mask\", look_ahead_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention_weights: tf.Tensor(\n",
      "[[[1.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.        ]\n",
      "  [0.49967298 0.5003271  0.         0.         0.         0.\n",
      "   0.         0.         0.         0.        ]\n",
      "  [0.3332953  0.33302316 0.33368158 0.         0.         0.\n",
      "   0.         0.         0.         0.        ]\n",
      "  [0.25003257 0.2498561  0.24989752 0.25021377 0.         0.\n",
      "   0.         0.         0.         0.        ]\n",
      "  [0.20017864 0.19970043 0.20001577 0.19990137 0.20020382 0.\n",
      "   0.         0.         0.         0.        ]\n",
      "  [0.16648294 0.16667846 0.1664795  0.16659202 0.16658287 0.16718417\n",
      "   0.         0.         0.         0.        ]\n",
      "  [0.14288522 0.14264204 0.14279628 0.14293528 0.14289069 0.14277938\n",
      "   0.14307113 0.         0.         0.        ]\n",
      "  [0.12492007 0.12490728 0.12486716 0.12483124 0.12501663 0.12521917\n",
      "   0.12497888 0.1252596  0.         0.        ]\n",
      "  [0.11101444 0.11100308 0.11096742 0.1109355  0.11110026 0.11128026\n",
      "   0.11106671 0.11131617 0.11131617 0.        ]\n",
      "  [0.09989457 0.09988434 0.09985226 0.09982353 0.09997179 0.10013375\n",
      "   0.0999416  0.10016607 0.10016607 0.10016607]]\n",
      "\n",
      " [[1.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.        ]\n",
      "  [0.49985975 0.5001403  0.         0.         0.         0.\n",
      "   0.         0.         0.         0.        ]\n",
      "  [0.3329623  0.3330336  0.33400407 0.         0.         0.\n",
      "   0.         0.         0.         0.        ]\n",
      "  [0.24977945 0.24967171 0.25025332 0.25029555 0.         0.\n",
      "   0.         0.         0.         0.        ]\n",
      "  [0.1998431  0.1998655  0.19990072 0.20003231 0.20035847 0.\n",
      "   0.         0.         0.         0.        ]\n",
      "  [0.16660614 0.16651092 0.16666041 0.16669704 0.16669574 0.16682968\n",
      "   0.         0.         0.         0.        ]\n",
      "  [0.14282052 0.14279835 0.14286603 0.14290085 0.14291462 0.14282288\n",
      "   0.1428767  0.         0.         0.        ]\n",
      "  [0.12503847 0.12508324 0.12499889 0.12495106 0.12490863 0.12495571\n",
      "   0.12498385 0.12508014 0.         0.        ]\n",
      "  [0.11088645 0.11104831 0.11119718 0.11111174 0.11117818 0.1110579\n",
      "   0.11108869 0.11107807 0.11135351 0.        ]\n",
      "  [0.10005799 0.10013337 0.09993912 0.09990206 0.09990182 0.09984995\n",
      "   0.09997167 0.100072   0.09998387 0.10018817]]], shape=(2, 10, 10), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 讓我們用目標語言（中文）的 batch\n",
    "# 來模擬 Decoder 處理的情況\n",
    "temp_q = temp_k = emb_tar\n",
    "temp_v = tf.cast(tf.math.greater(\n",
    "    tf.random.uniform(shape=emb_tar.shape), 0.5), tf.float32)\n",
    "\n",
    "# 將 look_ahead_mask 放入注意函式\n",
    "_, attention_weights = scaled_dot_product_attention(\n",
    "    temp_q, temp_k, temp_v, look_ahead_mask)\n",
    "\n",
    "print(\"attention_weights:\", attention_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 10), dtype=float32, numpy=\n",
       "array([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]], dtype=float32)>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_weights[:, 0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: tf.Tensor(\n",
      "[[[-0.02940199 -0.02748928  0.009587    0.02695702]\n",
      "  [ 0.02781546 -0.00051839 -0.04208876 -0.03737368]\n",
      "  [ 0.04085249 -0.01280526 -0.02142819  0.00932728]\n",
      "  [-0.04287729  0.00559031 -0.01610124  0.02254481]\n",
      "  [ 0.02117098 -0.03711741 -0.01554632  0.00051199]\n",
      "  [-0.00089258  0.03358604  0.04734136 -0.02150711]\n",
      "  [-0.01283282  0.03373785  0.0044058  -0.04340933]\n",
      "  [-0.01283282  0.03373785  0.0044058  -0.04340933]]\n",
      "\n",
      " [[-0.02940199 -0.02748928  0.009587    0.02695702]\n",
      "  [-0.02589555 -0.00853635  0.03899905  0.02993281]\n",
      "  [ 0.02303552 -0.0043821   0.03425056  0.02410031]\n",
      "  [-0.02790775  0.0453908  -0.00815301 -0.01390294]\n",
      "  [ 0.02987817  0.02534869  0.04416693 -0.03023093]\n",
      "  [ 0.03675361  0.04292214  0.0072703  -0.03425226]\n",
      "  [ 0.02117098 -0.03711741 -0.01554632  0.00051199]\n",
      "  [-0.00089258  0.03358604  0.04734136 -0.02150711]]], shape=(2, 8, 4), dtype=float32)\n",
      "output: tf.Tensor(\n",
      "[[[[-0.02940199 -0.02748928]\n",
      "   [ 0.02781546 -0.00051839]\n",
      "   [ 0.04085249 -0.01280526]\n",
      "   [-0.04287729  0.00559031]\n",
      "   [ 0.02117098 -0.03711741]\n",
      "   [-0.00089258  0.03358604]\n",
      "   [-0.01283282  0.03373785]\n",
      "   [-0.01283282  0.03373785]]\n",
      "\n",
      "  [[ 0.009587    0.02695702]\n",
      "   [-0.04208876 -0.03737368]\n",
      "   [-0.02142819  0.00932728]\n",
      "   [-0.01610124  0.02254481]\n",
      "   [-0.01554632  0.00051199]\n",
      "   [ 0.04734136 -0.02150711]\n",
      "   [ 0.0044058  -0.04340933]\n",
      "   [ 0.0044058  -0.04340933]]]\n",
      "\n",
      "\n",
      " [[[-0.02940199 -0.02748928]\n",
      "   [-0.02589555 -0.00853635]\n",
      "   [ 0.02303552 -0.0043821 ]\n",
      "   [-0.02790775  0.0453908 ]\n",
      "   [ 0.02987817  0.02534869]\n",
      "   [ 0.03675361  0.04292214]\n",
      "   [ 0.02117098 -0.03711741]\n",
      "   [-0.00089258  0.03358604]]\n",
      "\n",
      "  [[ 0.009587    0.02695702]\n",
      "   [ 0.03899905  0.02993281]\n",
      "   [ 0.03425056  0.02410031]\n",
      "   [-0.00815301 -0.01390294]\n",
      "   [ 0.04416693 -0.03023093]\n",
      "   [ 0.0072703  -0.03425226]\n",
      "   [-0.01554632  0.00051199]\n",
      "   [ 0.04734136 -0.02150711]]]], shape=(2, 2, 8, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "def split_heads(x, d_model, num_heads):\n",
    "    # x.shape: (batch_size, seq_len, d_model)\n",
    "    batch_size = tf.shape(x)[0]\n",
    "\n",
    "    # 我們要確保維度 `d_model` 可以被平分成 `num_heads` 個 `depth` 維度\n",
    "    assert d_model % num_heads == 0\n",
    "    depth = d_model // num_heads  # 這是分成多頭以後每個向量的維度 \n",
    "\n",
    "    # 將最後一個 d_model 維度分成 num_heads 個 depth 維度。\n",
    "    # 最後一個維度變成兩個維度，張量 x 從 3 維到 4 維\n",
    "    # (batch_size, seq_len, num_heads, depth)\n",
    "    reshaped_x = tf.reshape(x, shape=(batch_size, -1, num_heads, depth))\n",
    "\n",
    "    # 將 head 的維度拉前使得最後兩個維度為子詞以及其對應的 depth 向量\n",
    "    # (batch_size, num_heads, seq_len, depth)\n",
    "    output = tf.transpose(reshaped_x, perm=[0, 2, 1, 3])\n",
    "\n",
    "    return output\n",
    "\n",
    "# 我們的 `emb_inp` 裡頭的子詞本來就是 4 維的詞嵌入向量\n",
    "d_model = 4\n",
    "# 將 4 維詞嵌入向量分為 2 個 head 的 2 維矩陣\n",
    "num_heads = 2\n",
    "x = emb_inp\n",
    "\n",
    "output = split_heads(x, d_model, num_heads)  \n",
    "print(\"x:\", x)\n",
    "print(\"output:\", output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 實作一個執行多頭注意力機制的 keras layer\n",
    "# 在初始的時候指定輸出維度 `d_model` & `num_heads，\n",
    "# 在呼叫的時候輸入 `v`, `k`, `q` 以及 `mask`\n",
    "# 輸出跟 scaled_dot_product_attention 函式一樣有兩個：\n",
    "# output.shape            == (batch_size, seq_len_q, d_model)\n",
    "# attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "  # 在初始的時候建立一些必要參數\n",
    "  def __init__(self, d_model, num_heads):\n",
    "    super(MultiHeadAttention, self).__init__()\n",
    "    self.num_heads = num_heads # 指定要將 `d_model` 拆成幾個 heads\n",
    "    self.d_model = d_model # 在 split_heads 之前的基底維度\n",
    "    \n",
    "    assert d_model % self.num_heads == 0  # 前面看過，要確保可以平分\n",
    "    \n",
    "    self.depth = d_model // self.num_heads  # 每個 head 裡子詞的新的 repr. 維度\n",
    "    \n",
    "    self.wq = tf.keras.layers.Dense(d_model)  # 分別給 q, k, v 的 3 個線性轉換 \n",
    "    self.wk = tf.keras.layers.Dense(d_model)  # 注意我們並沒有指定 activation func\n",
    "    self.wv = tf.keras.layers.Dense(d_model)\n",
    "    \n",
    "    self.dense = tf.keras.layers.Dense(d_model)  # 多 heads 串接後通過的線性轉換\n",
    "    \n",
    "    \n",
    "  # 這跟我們前面看過的函式有 87% 相似\n",
    "  def split_heads(self, x, batch_size):\n",
    "    \"\"\"Split the last dimension into (num_heads, depth).\n",
    "    Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
    "    \"\"\"\n",
    "    x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "    return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "  \n",
    "  # multi-head attention 的實際執行流程，注意參數順序（這邊跟論文以及 TensorFlow 官方教學一致）\n",
    "  def call(self, v, k, q, mask):\n",
    "    batch_size = tf.shape(q)[0]\n",
    "    \n",
    "    # 將輸入的 q, k, v 都各自做一次線性轉換到 `d_model` 維空間\n",
    "    q = self.wq(q)  # (batch_size, seq_len, d_model)\n",
    "    k = self.wk(k)  # (batch_size, seq_len, d_model)\n",
    "    v = self.wv(v)  # (batch_size, seq_len, d_model)\n",
    "    \n",
    "    # 前面看過的，將最後一個 `d_model` 維度分成 `num_heads` 個 `depth` 維度\n",
    "    q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n",
    "    k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n",
    "    v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n",
    "    \n",
    "    # 利用 broadcasting 讓每個句子的每個 head 的 qi, ki, vi 都各自進行注意力機制\n",
    "    # 輸出會多一個 head 維度\n",
    "    scaled_attention, attention_weights = scaled_dot_product_attention(\n",
    "        q, k, v, mask)\n",
    "    # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n",
    "    # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "    \n",
    "    \n",
    "    # 跟我們在 `split_heads` 函式做的事情剛好相反，先做 transpose 再做 reshape\n",
    "    # 將 `num_heads` 個 `depth` 維度串接回原來的 `d_model` 維度\n",
    "    scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n",
    "    # (batch_size, seq_len_q, num_heads, depth)\n",
    "    concat_attention = tf.reshape(scaled_attention, \n",
    "                                  (batch_size, -1, self.d_model)) \n",
    "    # (batch_size, seq_len_q, d_model)\n",
    "\n",
    "    # 通過最後一個線性轉換\n",
    "    output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n",
    "        \n",
    "    return output, attention_weights    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d_model: 4\n",
      "num_heads: 2\n",
      "\n",
      "q.shape: (2, 8, 4)\n",
      "k.shape: (2, 8, 4)\n",
      "v.shape: (2, 8, 4)\n",
      "padding_mask.shape: (2, 1, 1, 8)\n",
      "output.shape: (2, 8, 4)\n",
      "attention_weights.shape: (2, 2, 8, 8)\n",
      "\n",
      "output: tf.Tensor(\n",
      "[[[ 0.00708912 -0.00861574 -0.00824888 -0.00125531]\n",
      "  [ 0.00704044 -0.00856255 -0.00824743 -0.00130908]\n",
      "  [ 0.00706525 -0.00858905 -0.00825153 -0.00128642]\n",
      "  [ 0.00707757 -0.00860305 -0.00824692 -0.00126657]\n",
      "  [ 0.00706903 -0.00859383 -0.00825078 -0.00128115]\n",
      "  [ 0.00706301 -0.00858597 -0.00824349 -0.00127383]\n",
      "  [ 0.00704599 -0.00856815 -0.00824224 -0.00129322]\n",
      "  [ 0.00704599 -0.00856815 -0.00824224 -0.00129322]]\n",
      "\n",
      " [[-0.0084104   0.00985345  0.0218877   0.01573933]\n",
      "  [-0.00840437  0.00984869  0.02188719  0.01574546]\n",
      "  [-0.00840402  0.00984824  0.02187178  0.01572827]\n",
      "  [-0.00845825  0.0099027   0.02189481  0.01570285]\n",
      "  [-0.00844073  0.00988418  0.02187815  0.01570553]\n",
      "  [-0.00845728  0.00990096  0.02187634  0.01568546]\n",
      "  [-0.00842486  0.00986613  0.02187385  0.01571093]\n",
      "  [-0.0084405   0.0098847   0.02188747  0.0157156 ]]], shape=(2, 8, 4), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# emb_inp.shape == (batch_size, seq_len, d_model)\n",
    "#               == (2, 8, 4)\n",
    "assert d_model == emb_inp.shape[-1]  == 4\n",
    "num_heads = 2\n",
    "\n",
    "print(f\"d_model: {d_model}\")\n",
    "print(f\"num_heads: {num_heads}\\n\")\n",
    "\n",
    "# 初始化一個 multi-head attention layer\n",
    "mha = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "# 簡單將 v, k, q 都設置為 `emb_inp`\n",
    "# 順便看看 padding mask 的作用。\n",
    "# 別忘記，第一個英文序列的最後兩個 tokens 是 <pad>\n",
    "v = k = q = emb_inp\n",
    "padding_mask = create_padding_mask(inp)\n",
    "print(\"q.shape:\", q.shape)\n",
    "print(\"k.shape:\", k.shape)\n",
    "print(\"v.shape:\", v.shape)\n",
    "print(\"padding_mask.shape:\", padding_mask.shape)\n",
    "\n",
    "output, attention_weights = mha(v, k, q, mask)\n",
    "print(\"output.shape:\", output.shape)\n",
    "print(\"attention_weights.shape:\", attention_weights.shape)\n",
    "\n",
    "print(\"\\noutput:\", output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 建立 Transformer 裡 Encoder / Decoder layer 都有使用到的 Feed Forward 元件\n",
    "def point_wise_feed_forward_network(d_model, dff):\n",
    "  \n",
    "  # 此 FFN 對輸入做兩個線性轉換，中間加了一個 ReLU activation func\n",
    "    return tf.keras.Sequential([\n",
    "      tf.keras.layers.Dense(dff, activation='relu'),  # (batch_size, seq_len, dff)\n",
    "      tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.shape: (64, 10, 512)\n",
      "out.shape: (64, 10, 512)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "batch_size = 64\n",
    "seq_len = 10\n",
    "d_model = 512\n",
    "dff = 2048\n",
    "\n",
    "x = tf.random.uniform((batch_size, seq_len, d_model))\n",
    "ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "out = ffn(x)\n",
    "print(\"x.shape:\", x.shape)\n",
    "print(\"out.shape:\", out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5, 4), dtype=float32, numpy=\n",
       "array([[ 2.8674245 , -2.174698  , -1.3073453 , -6.4233937 ],\n",
       "       [ 2.8674245 , -2.174698  , -1.3073453 , -6.4233937 ],\n",
       "       [ 3.6502066 , -0.97325826, -2.4126563 , -6.509499  ],\n",
       "       [ 3.6502066 , -0.97325826, -2.4126563 , -6.509499  ],\n",
       "       [ 3.6502066 , -0.97325826, -2.4126563 , -6.509499  ]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_model = 4 # FFN 的輸入輸出張量的最後一維皆為 `d_model`\n",
    "dff = 6\n",
    "\n",
    "# 建立一個小 FFN\n",
    "small_ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "# 懂子詞梗的站出來\n",
    "dummy_sentence = tf.constant([[5, 5, 6, 6], \n",
    "                              [5, 5, 6, 6], \n",
    "                              [9, 5, 2, 7], \n",
    "                              [9, 5, 2, 7],\n",
    "                              [9, 5, 2, 7]], dtype=tf.float32)\n",
    "small_ffn(dummy_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Encoder 裡頭會有 N 個 EncoderLayers，而每個 EncoderLayer 裡又有兩個 sub-layers: MHA & FFN\n",
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "  # Transformer 論文內預設 dropout rate 為 0.1\n",
    "  def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "    super(EncoderLayer, self).__init__()\n",
    "\n",
    "    self.mha = MultiHeadAttention(d_model, num_heads)\n",
    "    self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "    # layer norm 很常在 RNN-based 的模型被使用。一個 sub-layer 一個 layer norm\n",
    "    self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "    self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "    \n",
    "    # 一樣，一個 sub-layer 一個 dropout layer\n",
    "    self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "    self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "    \n",
    "  # 需要丟入 `training` 參數是因為 dropout 在訓練以及測試的行為有所不同\n",
    "  def call(self, x, training, mask):\n",
    "    # 除了 `attn`，其他張量的 shape 皆為 (batch_size, input_seq_len, d_model)\n",
    "    # attn.shape == (batch_size, num_heads, input_seq_len, input_seq_len)\n",
    "    \n",
    "    # sub-layer 1: MHA\n",
    "    # Encoder 利用注意機制關注自己當前的序列，因此 v, k, q 全部都是自己\n",
    "    # 另外別忘了我們還需要 padding mask 來遮住輸入序列中的 <pad> token\n",
    "    attn_output, attn = self.mha(x, x, x, mask)  \n",
    "    attn_output = self.dropout1(attn_output, training=training) \n",
    "    out1 = self.layernorm1(x + attn_output)  \n",
    "    \n",
    "    # sub-layer 2: FFN\n",
    "    ffn_output = self.ffn(out1) \n",
    "    ffn_output = self.dropout2(ffn_output, training=training)  # 記得 training\n",
    "    out2 = self.layernorm2(out1 + ffn_output)\n",
    "    \n",
    "    return out2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inp: tf.Tensor(\n",
      "[[8113  103    9 1066 7903 8114    0    0]\n",
      " [8113   16 4111 6735   12 2750 7903 8114]], shape=(2, 8), dtype=int64)\n",
      "--------------------\n",
      "padding_mask: tf.Tensor(\n",
      "[[[[0. 0. 0. 0. 0. 0. 1. 1.]]]\n",
      "\n",
      "\n",
      " [[[0. 0. 0. 0. 0. 0. 0. 0.]]]], shape=(2, 1, 1, 8), dtype=float32)\n",
      "--------------------\n",
      "emb_inp: tf.Tensor(\n",
      "[[[-0.02940199 -0.02748928  0.009587    0.02695702]\n",
      "  [ 0.02781546 -0.00051839 -0.04208876 -0.03737368]\n",
      "  [ 0.04085249 -0.01280526 -0.02142819  0.00932728]\n",
      "  [-0.04287729  0.00559031 -0.01610124  0.02254481]\n",
      "  [ 0.02117098 -0.03711741 -0.01554632  0.00051199]\n",
      "  [-0.00089258  0.03358604  0.04734136 -0.02150711]\n",
      "  [-0.01283282  0.03373785  0.0044058  -0.04340933]\n",
      "  [-0.01283282  0.03373785  0.0044058  -0.04340933]]\n",
      "\n",
      " [[-0.02940199 -0.02748928  0.009587    0.02695702]\n",
      "  [-0.02589555 -0.00853635  0.03899905  0.02993281]\n",
      "  [ 0.02303552 -0.0043821   0.03425056  0.02410031]\n",
      "  [-0.02790775  0.0453908  -0.00815301 -0.01390294]\n",
      "  [ 0.02987817  0.02534869  0.04416693 -0.03023093]\n",
      "  [ 0.03675361  0.04292214  0.0072703  -0.03425226]\n",
      "  [ 0.02117098 -0.03711741 -0.01554632  0.00051199]\n",
      "  [-0.00089258  0.03358604  0.04734136 -0.02150711]]], shape=(2, 8, 4), dtype=float32)\n",
      "--------------------\n",
      "enc_out: tf.Tensor(\n",
      "[[[-1.2790084  -0.4484179   0.2844689   1.4429574 ]\n",
      "  [ 0.8189746   0.89949    -1.5816016  -0.13686289]\n",
      "  [ 1.2944572  -0.2120691  -1.4614167   0.3790287 ]\n",
      "  [-1.4679327   1.1489065  -0.32746476  0.6464911 ]\n",
      "  [ 1.3575203  -1.3017013  -0.50809157  0.45227253]\n",
      "  [-0.4725885   1.4475989   0.2889553  -1.2639656 ]\n",
      "  [-0.2845264   1.6905513  -0.8935656  -0.5124593 ]\n",
      "  [-0.2845264   1.6905513  -0.8935656  -0.5124593 ]]\n",
      "\n",
      " [[-0.09594733 -1.3889819   0.05000272  1.4349266 ]\n",
      "  [-0.80072045 -1.177281    0.897902    1.0800993 ]\n",
      "  [ 1.1408303  -1.6084566   0.2656777   0.20194861]\n",
      "  [-0.73713     1.6235303  -0.90576535  0.01936519]\n",
      "  [ 1.3079005   0.28855962 -0.11566927 -1.4807909 ]\n",
      "  [ 1.1174244   0.8156742  -1.2963963  -0.63670236]\n",
      "  [ 1.3715899  -1.3202223  -0.45836774  0.40700006]\n",
      "  [ 0.38809723  0.7998333   0.524973   -1.7129036 ]]], shape=(2, 8, 4), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# 之後可以調的超參數。這邊為了 demo 設小一點\n",
    "d_model = 4\n",
    "num_heads = 2\n",
    "dff = 8\n",
    "\n",
    "# 新建一個使用上述參數的 Encoder Layer\n",
    "enc_layer = EncoderLayer(d_model, num_heads, dff)\n",
    "padding_mask = create_padding_mask(inp)  # 建立一個當前輸入 batch 使用的 padding mask\n",
    "enc_out = enc_layer(emb_inp, training=False, mask=padding_mask)  # (batch_size, seq_len, d_model)\n",
    "\n",
    "print(\"inp:\", inp)\n",
    "print(\"-\" * 20)\n",
    "print(\"padding_mask:\", padding_mask)\n",
    "print(\"-\" * 20)\n",
    "print(\"emb_inp:\", emb_inp)\n",
    "print(\"-\" * 20)\n",
    "print(\"enc_out:\", enc_out)\n",
    "assert emb_inp.shape == enc_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoder 裡頭會有 N 個 DecoderLayer，\n",
    "# 而 DecoderLayer 又有三個 sub-layers: 自注意的 MHA, 關注 Encoder 輸出的 MHA & FFN\n",
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        # 3 個 sub-layers 的主角們\n",
    "        self.mha1 = MultiHeadAttention(d_model, num_heads)\n",
    "        self.mha2 = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "        # 定義每個 sub-layer 用的 LayerNorm\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        # 定義每個 sub-layer 用的 Dropout\n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout3 = tf.keras.layers.Dropout(rate)\n",
    "    \n",
    "    \n",
    "    def call(self, x, enc_output, training, combined_mask, inp_padding_mask):\n",
    "        # 所有 sub-layers 的主要輸出皆為 (batch_size, target_seq_len, d_model)\n",
    "        # enc_output 為 Encoder 輸出序列，shape 為 (batch_size, input_seq_len, d_model)\n",
    "        # attn_weights_block_1 則為 (batch_size, num_heads, target_seq_len, target_seq_len)\n",
    "        # attn_weights_block_2 則為 (batch_size, num_heads, target_seq_len, input_seq_len)\n",
    "\n",
    "\n",
    "        # sub-layer 1: Decoder layer 自己對輸出序列做注意力。\n",
    "        # 我們同時需要 look ahead mask 以及輸出序列的 padding mask \n",
    "        # 來避免前面已生成的子詞關注到未來的子詞以及 <pad>\n",
    "        attn1, attn_weights_block1 = self.mha1(x, x, x, combined_mask)\n",
    "        attn1 = self.dropout1(attn1, training=training)\n",
    "        out1 = self.layernorm1(attn1 + x)\n",
    "\n",
    "        # sub-layer 2: Decoder layer 關注 Encoder 的最後輸出\n",
    "        # 記得我們一樣需要對 Encoder 的輸出套用 padding mask 避免關注到 <pad>\n",
    "        attn2, attn_weights_block2 = self.mha2(\n",
    "            enc_output, enc_output, out1, inp_padding_mask)  # (batch_size, target_seq_len, d_model)\n",
    "        attn2 = self.dropout2(attn2, training=training)\n",
    "        out2 = self.layernorm2(attn2 + out1)  # (batch_size, target_seq_len, d_model)\n",
    "\n",
    "        # sub-layer 3: FFN 部分跟 Encoder layer 完全一樣\n",
    "        ffn_output = self.ffn(out2)  # (batch_size, target_seq_len, d_model)\n",
    "\n",
    "        ffn_output = self.dropout3(ffn_output, training=training)\n",
    "        out3 = self.layernorm3(ffn_output + out2)  # (batch_size, target_seq_len, d_model)\n",
    "\n",
    "        # 除了主要輸出 `out3` 以外，輸出 multi-head 注意權重方便之後理解模型內部狀況\n",
    "        return out3, attn_weights_block1, attn_weights_block2    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tar: tf.Tensor(\n",
      "[[4205   10  241   86   27    3 4206    0    0    0]\n",
      " [4205  165  489  398  191   14    7  560    3 4206]], shape=(2, 10), dtype=int64)\n",
      "--------------------\n",
      "tar_padding_mask: tf.Tensor(\n",
      "[[[[0. 0. 0. 0. 0. 0. 0. 1. 1. 1.]]]\n",
      "\n",
      "\n",
      " [[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]]], shape=(2, 1, 1, 10), dtype=float32)\n",
      "--------------------\n",
      "look_ahead_mask: tf.Tensor(\n",
      "[[0. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [0. 0. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [0. 0. 0. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [0. 0. 0. 0. 1. 1. 1. 1. 1. 1.]\n",
      " [0. 0. 0. 0. 0. 1. 1. 1. 1. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 1. 1. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 1. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]], shape=(10, 10), dtype=float32)\n",
      "--------------------\n",
      "combined_mask: tf.Tensor(\n",
      "[[[[0. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "   [0. 0. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "   [0. 0. 0. 1. 1. 1. 1. 1. 1. 1.]\n",
      "   [0. 0. 0. 0. 1. 1. 1. 1. 1. 1.]\n",
      "   [0. 0. 0. 0. 0. 1. 1. 1. 1. 1.]\n",
      "   [0. 0. 0. 0. 0. 0. 1. 1. 1. 1.]\n",
      "   [0. 0. 0. 0. 0. 0. 0. 1. 1. 1.]\n",
      "   [0. 0. 0. 0. 0. 0. 0. 1. 1. 1.]\n",
      "   [0. 0. 0. 0. 0. 0. 0. 1. 1. 1.]\n",
      "   [0. 0. 0. 0. 0. 0. 0. 1. 1. 1.]]]\n",
      "\n",
      "\n",
      " [[[0. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "   [0. 0. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "   [0. 0. 0. 1. 1. 1. 1. 1. 1. 1.]\n",
      "   [0. 0. 0. 0. 1. 1. 1. 1. 1. 1.]\n",
      "   [0. 0. 0. 0. 0. 1. 1. 1. 1. 1.]\n",
      "   [0. 0. 0. 0. 0. 0. 1. 1. 1. 1.]\n",
      "   [0. 0. 0. 0. 0. 0. 0. 1. 1. 1.]\n",
      "   [0. 0. 0. 0. 0. 0. 0. 0. 1. 1.]\n",
      "   [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "   [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]]], shape=(2, 1, 10, 10), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "tar_padding_mask = create_padding_mask(tar)\n",
    "look_ahead_mask = create_look_ahead_mask(tar.shape[-1])\n",
    "combined_mask = tf.maximum(tar_padding_mask, look_ahead_mask)\n",
    "\n",
    "print(\"tar:\", tar)\n",
    "print(\"-\" * 20)\n",
    "print(\"tar_padding_mask:\", tar_padding_mask)\n",
    "print(\"-\" * 20)\n",
    "print(\"look_ahead_mask:\", look_ahead_mask)\n",
    "print(\"-\" * 20)\n",
    "print(\"combined_mask:\", combined_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emb_tar: tf.Tensor(\n",
      "[[[ 0.02309941  0.01578486 -0.03650398  0.04221227]\n",
      "  [ 0.02180279 -0.03836104 -0.0052525  -0.01320656]\n",
      "  [-0.0190685  -0.03266476 -0.03334127  0.04303731]\n",
      "  [ 0.03618959 -0.00743835  0.01616282  0.04101736]\n",
      "  [ 0.00099171  0.03080915 -0.03851545  0.03447074]\n",
      "  [-0.02579685  0.02527661  0.04145398 -0.04307295]\n",
      "  [-0.00221508  0.03029467  0.02829352  0.04788197]\n",
      "  [-0.02839622  0.04139889 -0.00255186 -0.03752811]\n",
      "  [-0.02839622  0.04139889 -0.00255186 -0.03752811]\n",
      "  [-0.02839622  0.04139889 -0.00255186 -0.03752811]]\n",
      "\n",
      " [[ 0.02309941  0.01578486 -0.03650398  0.04221227]\n",
      "  [-0.00205498  0.04796953 -0.00836701  0.03551394]\n",
      "  [-0.0049132  -0.03055991  0.03992986 -0.02458232]\n",
      "  [ 0.02458249 -0.02872031  0.01778464 -0.02678457]\n",
      "  [ 0.03532792  0.02878738 -0.00509523 -0.0492527 ]\n",
      "  [-0.00865723 -0.02110739 -0.03051426 -0.02639371]\n",
      "  [ 0.02155342 -0.00126004  0.00749636 -0.00912453]\n",
      "  [-0.01634679  0.00527266  0.01074455  0.02497052]\n",
      "  [-0.02579685  0.02527661  0.04145398 -0.04307295]\n",
      "  [-0.00221508  0.03029467  0.02829352  0.04788197]]], shape=(2, 10, 4), dtype=float32)\n",
      "--------------------\n",
      "enc_out: tf.Tensor(\n",
      "[[[-1.2790084  -0.4484179   0.2844689   1.4429574 ]\n",
      "  [ 0.8189746   0.89949    -1.5816016  -0.13686289]\n",
      "  [ 1.2944572  -0.2120691  -1.4614167   0.3790287 ]\n",
      "  [-1.4679327   1.1489065  -0.32746476  0.6464911 ]\n",
      "  [ 1.3575203  -1.3017013  -0.50809157  0.45227253]\n",
      "  [-0.4725885   1.4475989   0.2889553  -1.2639656 ]\n",
      "  [-0.2845264   1.6905513  -0.8935656  -0.5124593 ]\n",
      "  [-0.2845264   1.6905513  -0.8935656  -0.5124593 ]]\n",
      "\n",
      " [[-0.09594733 -1.3889819   0.05000272  1.4349266 ]\n",
      "  [-0.80072045 -1.177281    0.897902    1.0800993 ]\n",
      "  [ 1.1408303  -1.6084566   0.2656777   0.20194861]\n",
      "  [-0.73713     1.6235303  -0.90576535  0.01936519]\n",
      "  [ 1.3079005   0.28855962 -0.11566927 -1.4807909 ]\n",
      "  [ 1.1174244   0.8156742  -1.2963963  -0.63670236]\n",
      "  [ 1.3715899  -1.3202223  -0.45836774  0.40700006]\n",
      "  [ 0.38809723  0.7998333   0.524973   -1.7129036 ]]], shape=(2, 8, 4), dtype=float32)\n",
      "--------------------\n",
      "dec_out: tf.Tensor(\n",
      "[[[ 0.24520996  0.9124286  -1.6827396   0.52510095]\n",
      "  [ 1.4490802  -1.3585964  -0.20380807  0.11332427]\n",
      "  [-0.4028599  -0.530555   -0.7824085   1.7158233 ]\n",
      "  [ 0.8677076  -1.2873157  -0.65675914  1.0763671 ]\n",
      "  [-0.12391096  1.1748888  -1.5384061   0.48742837]\n",
      "  [-0.24596532  0.5464132   1.1906333  -1.4910812 ]\n",
      "  [-1.7202684   0.7570852   0.5242093   0.4389739 ]\n",
      "  [-0.02801281  1.3993088   0.05628167 -1.4275779 ]\n",
      "  [-0.02801275  1.3993089   0.05628166 -1.427578  ]\n",
      "  [-0.02801275  1.3993089   0.05628166 -1.427578  ]]\n",
      "\n",
      " [[ 0.11055017  1.2710681  -1.5327494   0.15113118]\n",
      "  [-0.36196986  1.7159778  -0.7391547  -0.614853  ]\n",
      "  [ 0.1960543  -1.0118155   1.5491637  -0.7334023 ]\n",
      "  [ 1.3901527  -1.410459    0.20747498 -0.18716866]\n",
      "  [ 0.93018496  0.9748236  -0.52242076 -1.3825879 ]\n",
      "  [ 1.5940356  -0.124051   -1.1611568  -0.30882782]\n",
      "  [ 1.412476   -1.2407097   0.388878   -0.5606442 ]\n",
      "  [-1.4752018  -0.31004715  1.151554    0.633695  ]\n",
      "  [-0.50473964  0.42634052  1.37344    -1.2950408 ]\n",
      "  [-1.6073794  -0.07197207  0.86243844  0.816913  ]]], shape=(2, 10, 4), dtype=float32)\n",
      "--------------------\n",
      "dec_self_attn_weights.shape: (2, 2, 10, 10)\n",
      "dec_enc_attn_weights: (2, 2, 10, 8)\n"
     ]
    }
   ],
   "source": [
    "# 超參數\n",
    "d_model = 4\n",
    "num_heads = 2\n",
    "dff = 8\n",
    "dec_layer = DecoderLayer(d_model, num_heads, dff)\n",
    "\n",
    "# 來源、目標語言的序列都需要 padding mask\n",
    "inp_padding_mask = create_padding_mask(inp)\n",
    "tar_padding_mask = create_padding_mask(tar)\n",
    "\n",
    "# masked MHA 用的遮罩，把 padding 跟未來子詞都蓋住\n",
    "look_ahead_mask = create_look_ahead_mask(tar.shape[-1])\n",
    "combined_mask = tf.maximum(tar_padding_mask, look_ahead_mask)\n",
    "\n",
    "# 實際初始一個 decoder layer 並做 3 個 sub-layers 的計算\n",
    "dec_out, dec_self_attn_weights, dec_enc_attn_weights = dec_layer(\n",
    "    emb_tar, enc_out, False, combined_mask, inp_padding_mask)\n",
    "\n",
    "print(\"emb_tar:\", emb_tar)\n",
    "print(\"-\" * 20)\n",
    "print(\"enc_out:\", enc_out)\n",
    "print(\"-\" * 20)\n",
    "print(\"dec_out:\", dec_out)\n",
    "assert emb_tar.shape == dec_out.shape\n",
    "print(\"-\" * 20)\n",
    "print(\"dec_self_attn_weights.shape:\", dec_self_attn_weights.shape)\n",
    "print(\"dec_enc_attn_weights:\", dec_enc_attn_weights.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 50, 512), dtype=float32, numpy=\n",
       "array([[[ 0.        ,  0.        ,  0.        , ...,  1.        ,\n",
       "          1.        ,  1.        ],\n",
       "        [ 0.84147096,  0.8218562 ,  0.8019618 , ...,  1.        ,\n",
       "          1.        ,  1.        ],\n",
       "        [ 0.9092974 ,  0.9364147 ,  0.95814437, ...,  1.        ,\n",
       "          1.        ,  1.        ],\n",
       "        ...,\n",
       "        [ 0.12357312,  0.97718984, -0.24295525, ...,  0.9999863 ,\n",
       "          0.99998724,  0.99998814],\n",
       "        [-0.76825464,  0.7312359 ,  0.63279754, ...,  0.9999857 ,\n",
       "          0.9999867 ,  0.9999876 ],\n",
       "        [-0.95375264, -0.14402692,  0.99899054, ...,  0.9999851 ,\n",
       "          0.9999861 ,  0.9999871 ]]], dtype=float32)>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 以下直接參考 TensorFlow 官方 tutorial \n",
    "def get_angles(pos, i, d_model):\n",
    "    angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
    "    return pos * angle_rates\n",
    "\n",
    "def positional_encoding(position, d_model):\n",
    "    angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
    "                          np.arange(d_model)[np.newaxis, :],\n",
    "                          d_model)\n",
    "\n",
    "    # apply sin to even indices in the array; 2i\n",
    "    sines = np.sin(angle_rads[:, 0::2])\n",
    "\n",
    "    # apply cos to odd indices in the array; 2i+1\n",
    "    cosines = np.cos(angle_rads[:, 1::2])\n",
    "\n",
    "    pos_encoding = np.concatenate([sines, cosines], axis=-1)\n",
    "\n",
    "    pos_encoding = pos_encoding[np.newaxis, ...]\n",
    "\n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)\n",
    "\n",
    "\n",
    "seq_len = 50\n",
    "d_model = 512\n",
    "\n",
    "pos_encoding = positional_encoding(seq_len, d_model)\n",
    "pos_encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.pcolormesh(pos_encoding[0], cmap='RdBu')\n",
    "plt.xlabel('d_model')\n",
    "plt.xlim((0, 512))\n",
    "plt.ylabel('Position')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "  # Encoder 的初始參數除了本來就要給 EncoderLayer 的參數還多了：\n",
    "  # - num_layers: 決定要有幾個 EncoderLayers, 前面影片中的 `N`\n",
    "  # - input_vocab_size: 用來把索引轉成詞嵌入向量\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, rate=0.1):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "\n",
    "        self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n",
    "        self.pos_encoding = positional_encoding(input_vocab_size, self.d_model)\n",
    "\n",
    "        # 建立 `num_layers` 個 EncoderLayers\n",
    "        self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) \n",
    "                           for _ in range(num_layers)]\n",
    "\n",
    "        self.dropout = tf.keras.layers.Dropout(rate)\n",
    "        \n",
    "\n",
    "    def call(self, x, training, mask):\n",
    "        # 輸入的 x.shape == (batch_size, input_seq_len)\n",
    "        # 以下各 layer 的輸出皆為 (batch_size, input_seq_len, d_model)\n",
    "        input_seq_len = tf.shape(x)[1]\n",
    "\n",
    "        # 將 2 維的索引序列轉成 3 維的詞嵌入張量，並依照論文乘上 sqrt(d_model)\n",
    "        # 再加上對應長度的位置編碼\n",
    "        x = self.embedding(x)\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        x += self.pos_encoding[:, :input_seq_len, :]\n",
    "\n",
    "        # 對 embedding 跟位置編碼的總合做 regularization\n",
    "        # 這在 Decoder 也會做\n",
    "        x = self.dropout(x, training=training)\n",
    "    \n",
    "        # 通過 N 個 EncoderLayer 做編碼\n",
    "        for i, enc_layer in enumerate(self.enc_layers):\n",
    "            x = enc_layer(x, training, mask)\n",
    "            # 以下只是用來 demo EncoderLayer outputs\n",
    "            #print('-' * 20)\n",
    "            #print(f\"EncoderLayer {i + 1}'s output:\", x)\n",
    "\n",
    "\n",
    "        return x        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inp: tf.Tensor(\n",
      "[[8113  103    9 1066 7903 8114    0    0]\n",
      " [8113   16 4111 6735   12 2750 7903 8114]], shape=(2, 8), dtype=int64)\n",
      "--------------------\n",
      "enc_out: tf.Tensor(\n",
      "[[[-0.7849332  -0.591968   -0.33270508  1.7096064 ]\n",
      "  [-0.50706536 -0.5110137  -0.7082318   1.7263108 ]\n",
      "  [-0.39270186 -0.03102632 -1.1583618   1.5820901 ]\n",
      "  [-0.5561631   0.38050288 -1.2407897   1.4164499 ]\n",
      "  [-0.90431994  0.19381054 -0.847289    1.5577984 ]\n",
      "  [-0.9732156  -0.22992761 -0.46524626  1.6683894 ]\n",
      "  [-0.84681964 -0.5434473  -0.31013626  1.7004031 ]\n",
      "  [-0.6243278  -0.5679047  -0.539001    1.7312335 ]]\n",
      "\n",
      " [[-0.7742376  -0.6076475  -0.32800585  1.7098908 ]\n",
      "  [-0.47978237 -0.56156063 -0.6860291   1.7273722 ]\n",
      "  [-0.30068296 -0.07366994 -1.1973958   1.5717487 ]\n",
      "  [-0.5147843   0.27872467 -1.229085    1.4651445 ]\n",
      "  [-0.8963448   0.26754576 -0.895411    1.52421   ]\n",
      "  [-0.9755362  -0.22618702 -0.4656963   1.6674196 ]\n",
      "  [-0.87600446 -0.54483974 -0.27099538  1.6918396 ]\n",
      "  [-0.6013047  -0.59936655 -0.5306772   1.7313485 ]]], shape=(2, 8, 4), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# 超參數\n",
    "num_layers = 2 # 2 層的 Encoder\n",
    "d_model = 4\n",
    "num_heads = 2\n",
    "dff = 8\n",
    "input_vocab_size = subword_encoder_en.vocab_size + 2 # 記得加上 <start>, <end>\n",
    "\n",
    "# 初始化一個 Encoder\n",
    "encoder = Encoder(num_layers, d_model, num_heads, dff, input_vocab_size)\n",
    "\n",
    "# 將 2 維的索引序列丟入 Encoder 做編碼\n",
    "enc_out = encoder(inp, training=False, mask=None)\n",
    "print(\"inp:\", inp)\n",
    "print(\"-\" * 20)\n",
    "print(\"enc_out:\", enc_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.layers.Layer):\n",
    "    # 初始參數跟 Encoder 只差在用 `target_vocab_size` 而非 `inp_vocab_size`\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size, rate=0.1):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "\n",
    "        # 為中文（目標語言）建立詞嵌入層\n",
    "        self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n",
    "        self.pos_encoding = positional_encoding(target_vocab_size, self.d_model)\n",
    "\n",
    "        self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate) \n",
    "                           for _ in range(num_layers)]\n",
    "        self.dropout = tf.keras.layers.Dropout(rate)\n",
    "        \n",
    "        \n",
    "    # 呼叫時的參數跟 DecoderLayer 一模一樣\n",
    "    def call(self, x, enc_output, training, combined_mask, inp_padding_mask):\n",
    "    \n",
    "        tar_seq_len = tf.shape(x)[1]\n",
    "        attention_weights = {}  # 用來存放每個 Decoder layer 的注意權重\n",
    "\n",
    "        # 這邊跟 Encoder 做的事情完全一樣\n",
    "        x = self.embedding(x)  # (batch_size, tar_seq_len, d_model)\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        x += self.pos_encoding[:, :tar_seq_len, :]\n",
    "        x = self.dropout(x, training=training)\n",
    "\n",
    "\n",
    "        for i, dec_layer in enumerate(self.dec_layers):\n",
    "            x, block1, block2 = dec_layer(x, enc_output, training,combined_mask, inp_padding_mask)\n",
    "\n",
    "            # 將從每個 Decoder layer 取得的注意權重全部存下來回傳，方便我們觀察\n",
    "            attention_weights['decoder_layer{}_block1'.format(i + 1)] = block1\n",
    "            attention_weights['decoder_layer{}_block2'.format(i + 1)] = block2\n",
    "\n",
    "        # x.shape == (batch_size, tar_seq_len, d_model)\n",
    "        return x, attention_weights        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tar: tf.Tensor(\n",
      "[[4205   10  241   86   27    3 4206    0    0    0]\n",
      " [4205  165  489  398  191   14    7  560    3 4206]], shape=(2, 10), dtype=int64)\n",
      "--------------------\n",
      "combined_mask: tf.Tensor(\n",
      "[[[[0. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "   [0. 0. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "   [0. 0. 0. 1. 1. 1. 1. 1. 1. 1.]\n",
      "   [0. 0. 0. 0. 1. 1. 1. 1. 1. 1.]\n",
      "   [0. 0. 0. 0. 0. 1. 1. 1. 1. 1.]\n",
      "   [0. 0. 0. 0. 0. 0. 1. 1. 1. 1.]\n",
      "   [0. 0. 0. 0. 0. 0. 0. 1. 1. 1.]\n",
      "   [0. 0. 0. 0. 0. 0. 0. 1. 1. 1.]\n",
      "   [0. 0. 0. 0. 0. 0. 0. 1. 1. 1.]\n",
      "   [0. 0. 0. 0. 0. 0. 0. 1. 1. 1.]]]\n",
      "\n",
      "\n",
      " [[[0. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "   [0. 0. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "   [0. 0. 0. 1. 1. 1. 1. 1. 1. 1.]\n",
      "   [0. 0. 0. 0. 1. 1. 1. 1. 1. 1.]\n",
      "   [0. 0. 0. 0. 0. 1. 1. 1. 1. 1.]\n",
      "   [0. 0. 0. 0. 0. 0. 1. 1. 1. 1.]\n",
      "   [0. 0. 0. 0. 0. 0. 0. 1. 1. 1.]\n",
      "   [0. 0. 0. 0. 0. 0. 0. 0. 1. 1.]\n",
      "   [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "   [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]]], shape=(2, 1, 10, 10), dtype=float32)\n",
      "--------------------\n",
      "enc_out: tf.Tensor(\n",
      "[[[-0.7849332  -0.591968   -0.33270508  1.7096064 ]\n",
      "  [-0.50706536 -0.5110137  -0.7082318   1.7263108 ]\n",
      "  [-0.39270186 -0.03102632 -1.1583618   1.5820901 ]\n",
      "  [-0.5561631   0.38050288 -1.2407897   1.4164499 ]\n",
      "  [-0.90431994  0.19381054 -0.847289    1.5577984 ]\n",
      "  [-0.9732156  -0.22992761 -0.46524626  1.6683894 ]\n",
      "  [-0.84681964 -0.5434473  -0.31013626  1.7004031 ]\n",
      "  [-0.6243278  -0.5679047  -0.539001    1.7312335 ]]\n",
      "\n",
      " [[-0.7742376  -0.6076475  -0.32800585  1.7098908 ]\n",
      "  [-0.47978237 -0.56156063 -0.6860291   1.7273722 ]\n",
      "  [-0.30068296 -0.07366994 -1.1973958   1.5717487 ]\n",
      "  [-0.5147843   0.27872467 -1.229085    1.4651445 ]\n",
      "  [-0.8963448   0.26754576 -0.895411    1.52421   ]\n",
      "  [-0.9755362  -0.22618702 -0.4656963   1.6674196 ]\n",
      "  [-0.87600446 -0.54483974 -0.27099538  1.6918396 ]\n",
      "  [-0.6013047  -0.59936655 -0.5306772   1.7313485 ]]], shape=(2, 8, 4), dtype=float32)\n",
      "--------------------\n",
      "inp_padding_mask: tf.Tensor(\n",
      "[[[[0. 0. 0. 0. 0. 0. 1. 1.]]]\n",
      "\n",
      "\n",
      " [[[0. 0. 0. 0. 0. 0. 0. 0.]]]], shape=(2, 1, 1, 8), dtype=float32)\n",
      "--------------------\n",
      "dec_out: tf.Tensor(\n",
      "[[[-0.56521416 -1.0581812   1.6000751   0.02332021]\n",
      "  [-0.340198   -1.2377603   1.5330343   0.04492389]\n",
      "  [ 0.3675253  -1.4228351   1.3287866  -0.27347657]\n",
      "  [ 0.09472068 -1.353683    1.455942   -0.19697979]\n",
      "  [-0.3839205  -1.0940721   1.6231283  -0.14513561]\n",
      "  [-0.41729766 -1.0276326   1.6514215  -0.20649134]\n",
      "  [-0.33023405 -1.0454822   1.6500466  -0.2743302 ]\n",
      "  [-0.19232097 -1.1254803   1.6149355  -0.29713422]\n",
      "  [ 0.4082282  -1.3586452   1.3515031  -0.40108618]\n",
      "  [ 0.19979605 -1.4183373   1.3857938  -0.16725263]]\n",
      "\n",
      " [[-0.5650454  -1.0544488   1.602678    0.01681651]\n",
      "  [-0.36043385 -1.2348609   1.5300139   0.06528082]\n",
      "  [ 0.24521802 -1.4295446   1.3651297  -0.18080297]\n",
      "  [-0.06483471 -1.3449187   1.4773033  -0.06755004]\n",
      "  [-0.41885287 -1.0775514   1.6267892  -0.13038504]\n",
      "  [-0.400182   -1.0338532   1.650498   -0.21646306]\n",
      "  [-0.35319278 -1.0375832   1.652348   -0.26157212]\n",
      "  [-0.24463181 -1.1371143   1.6107953  -0.22904916]\n",
      "  [ 0.19615427 -1.3627281   1.4271016  -0.26052776]\n",
      "  [ 0.08419981 -1.3687493   1.4467621  -0.16221273]]], shape=(2, 10, 4), dtype=float32)\n",
      "--------------------\n",
      "decoder_layer1_block1.shape: (2, 2, 10, 10)\n",
      "decoder_layer1_block2.shape: (2, 2, 10, 8)\n",
      "decoder_layer2_block1.shape: (2, 2, 10, 10)\n",
      "decoder_layer2_block2.shape: (2, 2, 10, 8)\n"
     ]
    }
   ],
   "source": [
    "# 超參數\n",
    "num_layers = 2 # 2 層的 Decoder\n",
    "d_model = 4\n",
    "num_heads = 2\n",
    "dff = 8\n",
    "target_vocab_size = subword_encoder_zh.vocab_size + 2 # 記得加上 <start>, <end>\n",
    "\n",
    "# 遮罩\n",
    "inp_padding_mask = create_padding_mask(inp)\n",
    "tar_padding_mask = create_padding_mask(tar)\n",
    "look_ahead_mask = create_look_ahead_mask(tar.shape[1])\n",
    "combined_mask = tf.math.maximum(tar_padding_mask, look_ahead_mask)\n",
    "\n",
    "# 初始化一個 Decoder\n",
    "decoder = Decoder(num_layers, d_model, num_heads, dff, target_vocab_size)\n",
    "\n",
    "# 將 2 維的索引序列以及遮罩丟入 Decoder\n",
    "print(\"tar:\", tar)\n",
    "print(\"-\" * 20)\n",
    "print(\"combined_mask:\", combined_mask)\n",
    "print(\"-\" * 20)\n",
    "print(\"enc_out:\", enc_out)\n",
    "print(\"-\" * 20)\n",
    "print(\"inp_padding_mask:\", inp_padding_mask)\n",
    "print(\"-\" * 20)\n",
    "dec_out, attn = decoder(tar, enc_out, training=False, \n",
    "                        combined_mask=combined_mask,\n",
    "                        inp_padding_mask=inp_padding_mask)\n",
    "print(\"dec_out:\", dec_out)\n",
    "print(\"-\" * 20)\n",
    "for block_name, attn_weights in attn.items():\n",
    "    print(f\"{block_name}.shape: {attn_weights.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer 之上已經沒有其他 layers 了，我們使用 tf.keras.Model 建立一個模型\n",
    "class Transformer(tf.keras.Model):\n",
    "    # 初始參數包含 Encoder & Decoder 都需要超參數以及中英字典數目\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, target_vocab_size, rate=0.1):\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        self.encoder = Encoder(num_layers, d_model, num_heads, dff, \n",
    "                               input_vocab_size, rate)\n",
    "\n",
    "        self.decoder = Decoder(num_layers, d_model, num_heads, dff, \n",
    "                               target_vocab_size, rate)\n",
    "        # 這個 FFN 輸出跟中文字典一樣大的 logits 數，等通過 softmax 就代表每個中文字的出現機率\n",
    "        self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
    "  \n",
    "    # enc_padding_mask 跟 dec_padding_mask 都是英文序列的 padding mask，\n",
    "    # 只是一個給 Encoder layer 的 MHA 用，一個是給 Decoder layer 的 MHA 2 使用\n",
    "    def call(self, inp, tar, training, enc_padding_mask, combined_mask, dec_padding_mask):\n",
    "\n",
    "        enc_output = self.encoder(inp, training, enc_padding_mask)  # (batch_size, inp_seq_len, d_model)\n",
    "\n",
    "        # dec_output.shape == (batch_size, tar_seq_len, d_model)\n",
    "        dec_output, attention_weights = self.decoder(\n",
    "            tar, enc_output, training, combined_mask, dec_padding_mask)\n",
    "\n",
    "        # 將 Decoder 輸出通過最後一個 linear layer\n",
    "        final_output = self.final_layer(dec_output)  # (batch_size, tar_seq_len, target_vocab_size)\n",
    "\n",
    "        return final_output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tar: tf.Tensor(\n",
      "[[4205   10  241   86   27    3 4206    0    0    0]\n",
      " [4205  165  489  398  191   14    7  560    3 4206]], shape=(2, 10), dtype=int64)\n",
      "--------------------\n",
      "tar_inp: tf.Tensor(\n",
      "[[4205   10  241   86   27    3 4206    0    0]\n",
      " [4205  165  489  398  191   14    7  560    3]], shape=(2, 9), dtype=int64)\n",
      "--------------------\n",
      "tar_real: tf.Tensor(\n",
      "[[  10  241   86   27    3 4206    0    0    0]\n",
      " [ 165  489  398  191   14    7  560    3 4206]], shape=(2, 9), dtype=int64)\n",
      "--------------------\n",
      "predictions: tf.Tensor(\n",
      "[[[ 0.01349578 -0.00199539 -0.00217387 ... -0.03862738 -0.03212878\n",
      "   -0.07692746]\n",
      "  [ 0.037483    0.01585472 -0.02548709 ... -0.04276202 -0.02495992\n",
      "   -0.05491883]\n",
      "  [ 0.05718527  0.0288353  -0.04577482 ... -0.0450176  -0.01315335\n",
      "   -0.03639909]\n",
      "  ...\n",
      "  [ 0.01202047 -0.00400385 -0.00099438 ... -0.03859971 -0.03085512\n",
      "   -0.07979749]\n",
      "  [ 0.02357969  0.00501019 -0.0119309  ... -0.04091505 -0.02892826\n",
      "   -0.06939011]\n",
      "  [ 0.04867784  0.02382022 -0.03683803 ... -0.04392422 -0.01941058\n",
      "   -0.04347048]]\n",
      "\n",
      " [[ 0.01676658 -0.00080312 -0.00556348 ... -0.03981712 -0.0293731\n",
      "   -0.07665334]\n",
      "  [ 0.03873826  0.01607162 -0.02685272 ... -0.04328423 -0.0234593\n",
      "   -0.0552263 ]\n",
      "  [ 0.0564083   0.02865588 -0.04492006 ... -0.04475704 -0.014088\n",
      "   -0.03639094]\n",
      "  ...\n",
      "  [ 0.01514175 -0.00298802 -0.0042616  ... -0.0397689  -0.02800198\n",
      "   -0.0797462 ]\n",
      "  [ 0.02867932  0.00800282 -0.01704067 ... -0.04215823 -0.02618419\n",
      "   -0.06638923]\n",
      "  [ 0.0505631   0.02489875 -0.03880979 ... -0.04421616 -0.01803543\n",
      "   -0.04204436]]], shape=(2, 9, 4207), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# 超參數\n",
    "num_layers = 1\n",
    "d_model = 4\n",
    "num_heads = 2\n",
    "dff = 8\n",
    "\n",
    "# + 2 是為了 <start> & <end> token\n",
    "input_vocab_size = subword_encoder_en.vocab_size + 2\n",
    "output_vocab_size = subword_encoder_zh.vocab_size + 2\n",
    "\n",
    "# 重點中的重點。訓練時用前一個字來預測下一個中文字\n",
    "tar_inp = tar[:, :-1]\n",
    "tar_real = tar[:, 1:]\n",
    "\n",
    "# 來源 / 目標語言用的遮罩。注意 `comined_mask` 已經將目標語言的兩種遮罩合而為一\n",
    "inp_padding_mask = create_padding_mask(inp)\n",
    "tar_padding_mask = create_padding_mask(tar_inp)\n",
    "look_ahead_mask = create_look_ahead_mask(tar_inp.shape[1])\n",
    "combined_mask = tf.math.maximum(tar_padding_mask, look_ahead_mask)\n",
    "\n",
    "# 初始化我們的第一個 transformer\n",
    "transformer = Transformer(num_layers, d_model, num_heads, dff, \n",
    "                          input_vocab_size, output_vocab_size)\n",
    "\n",
    "# 將英文、中文序列丟入取得 Transformer 預測下個中文字的結果\n",
    "predictions, attn_weights = transformer(inp, tar_inp, False, inp_padding_mask, \n",
    "                                        combined_mask, inp_padding_mask)\n",
    "print(\"tar:\", tar)\n",
    "print(\"-\" * 20)\n",
    "print(\"tar_inp:\", tar_inp)\n",
    "print(\"-\" * 20)\n",
    "print(\"tar_real:\", tar_real)\n",
    "print(\"-\" * 20)\n",
    "print(\"predictions:\", predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3,), dtype=float32, numpy=array([0.31326166, 0.31326166, 1.3132616 ], dtype=float32)>"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "# 假設我們要解的是一個 binary classifcation， 0 跟 1 個代表一個 label\n",
    "real = tf.constant([1, 1, 0], shape=(1, 3), dtype=tf.float32)\n",
    "pred = tf.constant([[0, 1], [0, 1], [0, 1]], dtype=tf.float32)\n",
    "loss_object(real, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions: tf.Tensor(\n",
      "[[[ 0.01349578 -0.00199539 -0.00217387 ... -0.03862738 -0.03212878\n",
      "   -0.07692746]\n",
      "  [ 0.037483    0.01585472 -0.02548709 ... -0.04276202 -0.02495992\n",
      "   -0.05491883]\n",
      "  [ 0.05718527  0.0288353  -0.04577482 ... -0.0450176  -0.01315335\n",
      "   -0.03639909]\n",
      "  ...\n",
      "  [ 0.01202047 -0.00400385 -0.00099438 ... -0.03859971 -0.03085512\n",
      "   -0.07979749]\n",
      "  [ 0.02357969  0.00501019 -0.0119309  ... -0.04091505 -0.02892826\n",
      "   -0.06939011]\n",
      "  [ 0.04867784  0.02382022 -0.03683803 ... -0.04392422 -0.01941058\n",
      "   -0.04347048]]\n",
      "\n",
      " [[ 0.01676658 -0.00080312 -0.00556348 ... -0.03981712 -0.0293731\n",
      "   -0.07665334]\n",
      "  [ 0.03873826  0.01607162 -0.02685272 ... -0.04328423 -0.0234593\n",
      "   -0.0552263 ]\n",
      "  [ 0.0564083   0.02865588 -0.04492006 ... -0.04475704 -0.014088\n",
      "   -0.03639094]\n",
      "  ...\n",
      "  [ 0.01514175 -0.00298802 -0.0042616  ... -0.0397689  -0.02800198\n",
      "   -0.0797462 ]\n",
      "  [ 0.02867932  0.00800282 -0.01704067 ... -0.04215823 -0.02618419\n",
      "   -0.06638923]\n",
      "  [ 0.0505631   0.02489875 -0.03880979 ... -0.04421616 -0.01803543\n",
      "   -0.04204436]]], shape=(2, 9, 4207), dtype=float32)\n",
      "--------------------\n",
      "tf.Tensor(\n",
      "[[1.376189  2.9352083 3.868732  3.4191108 2.608355  1.5664341 1.1489887\n",
      "  1.9882673 3.5525477]\n",
      " [1.4309777 2.921915  3.873899  3.5009158 2.649916  1.6611692 1.1839204\n",
      "  2.2150593 3.6206648]], shape=(2, 9), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(\"predictions:\", predictions)\n",
    "print(\"-\" * 20)\n",
    "print(tf.reduce_sum(predictions, axis=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(real, pred):\n",
    "    # 這次的 mask 將序列中不等於 0 的位置視為 1，其餘為 0 \n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    # 照樣計算所有位置的 cross entropy 但不加總\n",
    "    loss_ = loss_object(real, pred)\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask  # 只計算非 <pad> 位置的損失 \n",
    "\n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(\n",
    "    name='train_accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_vocab_size: 8115\n",
      "target_vocab_size: 4207\n"
     ]
    }
   ],
   "source": [
    "num_layers = 4 \n",
    "d_model = 128\n",
    "dff = 512\n",
    "num_heads = 8\n",
    "\n",
    "input_vocab_size = subword_encoder_en.vocab_size + 2\n",
    "target_vocab_size = subword_encoder_zh.vocab_size + 2\n",
    "dropout_rate = 0.1  # 預設值\n",
    "\n",
    "print(\"input_vocab_size:\", input_vocab_size)\n",
    "print(\"target_vocab_size:\", target_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    # 論文預設 `warmup_steps` = 4000\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super(CustomSchedule, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "\n",
    "        self.warmup_steps = warmup_steps\n",
    "    \n",
    "    def __call__(self, step):\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "\n",
    "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
    "  \n",
    "# 將客製化 learning rate schdeule 丟入 Adam opt.\n",
    "# Adam opt. 的參數都跟論文相同\n",
    "learning_rate = CustomSchedule(d_model)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, \n",
    "                                     epsilon=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'Train Step')"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "d_models = [128, 256, 512]\n",
    "warmup_steps = [1000 * i for i in range(1, 4)]\n",
    "\n",
    "schedules = []\n",
    "labels = []\n",
    "colors = [\"blue\", \"red\", \"black\"]\n",
    "for d in d_models:\n",
    "    schedules += [CustomSchedule(d, s) for s in warmup_steps]\n",
    "    labels += [f\"d_model: {d}, warm: {s}\" for s in warmup_steps]\n",
    "\n",
    "for i, (schedule, label) in enumerate(zip(schedules, labels)):\n",
    "    plt.plot(schedule(tf.range(10000, dtype=tf.float32)), \n",
    "           label=label, color=colors[i // 3])\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.ylabel(\"Learning Rate\")\n",
    "plt.xlabel(\"Train Step\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "這個 Transformer 有 4 層 Encoder / Decoder layers\n",
      "d_model: 128\n",
      "num_heads: 8\n",
      "dff: 512\n",
      "input_vocab_size: 8115\n",
      "target_vocab_size: 4207\n",
      "dropout_rate: 0.1\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transformer = Transformer(num_layers, d_model, num_heads, dff,\n",
    "                          input_vocab_size, target_vocab_size, dropout_rate)\n",
    "\n",
    "print(f\"\"\"這個 Transformer 有 {num_layers} 層 Encoder / Decoder layers\n",
    "d_model: {d_model}\n",
    "num_heads: {num_heads}\n",
    "dff: {dff}\n",
    "input_vocab_size: {input_vocab_size}\n",
    "target_vocab_size: {target_vocab_size}\n",
    "dropout_rate: {dropout_rate}\n",
    "\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "沒找到 checkpoint，從頭訓練。\n"
     ]
    }
   ],
   "source": [
    "train_perc = 20\n",
    "val_prec = 1\n",
    "drop_prec = 100 - train_perc - val_prec\n",
    "\n",
    "# 方便比較不同實驗/ 不同超參數設定的結果\n",
    "run_id = f\"{num_layers}layers_{d_model}d_{num_heads}heads_{dff}dff_{train_perc}train_perc\"\n",
    "checkpoint_path = os.path.join(checkpoint_path, run_id)\n",
    "log_dir = os.path.join(log_dir, run_id)\n",
    "\n",
    "# tf.train.Checkpoint 可以幫我們把想要存下來的東西整合起來，方便儲存與讀取\n",
    "# 一般來說你會想存下模型以及 optimizer 的狀態\n",
    "ckpt = tf.train.Checkpoint(transformer=transformer,\n",
    "                           optimizer=optimizer)\n",
    "\n",
    "# ckpt_manager 會去 checkpoint_path 看有沒有符合 ckpt 裡頭定義的東西\n",
    "# 存檔的時候只保留最近 5 次 checkpoints，其他自動刪除\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
    "\n",
    "# 如果在 checkpoint 路徑上有發現檔案就讀進來\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "\n",
    "    # 用來確認之前訓練多少 epochs 了\n",
    "    last_epoch = int(ckpt_manager.latest_checkpoint.split(\"-\")[-1])\n",
    "    print(f'已讀取最新的 checkpoint，模型已訓練 {last_epoch} epochs。')\n",
    "else:\n",
    "    last_epoch = 0\n",
    "    print(\"沒找到 checkpoint，從頭訓練。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 為 Transformer 的 Encoder / Decoder 準備遮罩\n",
    "def create_masks(inp, tar):\n",
    "    # 英文句子的 padding mask，要交給 Encoder layer 自注意力機制用的\n",
    "    enc_padding_mask = create_padding_mask(inp)\n",
    "\n",
    "    # 同樣也是英文句子的 padding mask，但是是要交給 Decoder layer 的 MHA 2 \n",
    "    # 關注 Encoder 輸出序列用的\n",
    "    dec_padding_mask = create_padding_mask(inp)\n",
    "\n",
    "    # Decoder layer 的 MHA1 在做自注意力機制用的\n",
    "    # `combined_mask` 是中文句子的 padding mask 跟 look ahead mask 的疊加\n",
    "    look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n",
    "    dec_target_padding_mask = create_padding_mask(tar)\n",
    "    combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
    "\n",
    "    return enc_padding_mask, combined_mask, dec_padding_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_step_signature = [\n",
    "    tf.TensorSpec(shape=(None, None), dtype=tf.int64),\n",
    "    tf.TensorSpec(shape=(None, None), dtype=tf.int64),\n",
    "]\n",
    "\n",
    "@tf.function  # 讓 TensorFlow 幫我們將 eager code 優化並加快運算\n",
    "def train_step(inp, tar):\n",
    "    # 前面說過的，用去尾的原始序列去預測下一個字的序列\n",
    "    tar_inp = tar[:, :-1]\n",
    "    tar_real = tar[:, 1:]\n",
    "\n",
    "    # 建立 3 個遮罩\n",
    "    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)\n",
    "\n",
    "    # 紀錄 Transformer 的所有運算過程以方便之後做梯度下降\n",
    "    with tf.GradientTape() as tape:\n",
    "        # 注意是丟入 `tar_inp` 而非 `tar`。記得將 `training` 參數設定為 True\n",
    "        predictions, _ = transformer(inp, tar_inp, \n",
    "                                     True, \n",
    "                                     enc_padding_mask, \n",
    "                                     combined_mask, \n",
    "                                     dec_padding_mask)\n",
    "        # 跟影片中顯示的相同，計算左移一個字的序列跟模型預測分佈之間的差異，當作 loss\n",
    "        loss = loss_function(tar_real, predictions)\n",
    "\n",
    "    # 取出梯度並呼叫前面定義的 Adam optimizer 幫我們更新 Transformer 裡頭可訓練的參數\n",
    "    gradients = tape.gradient(loss, transformer.trainable_variables)    \n",
    "    optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
    "\n",
    "    # 將 loss 以及訓練 acc 記錄到 TensorBoard 上，非必要\n",
    "    train_loss(loss)\n",
    "    train_accuracy(tar_real, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "此超參數組合的 Transformer 已經訓練 0 epochs。\n",
      "剩餘 epochs：-50\n",
      "Saving checkpoint for epoch 1 at nmt\\checkpoints\\4layers_128d_8heads_512dff_20train_perc\\ckpt-82\n",
      "Epoch 1 Loss 0.6401 Accuracy 0.4965\n",
      "Time taken for 1 epoch: 469.93735122680664 secs\n",
      "\n",
      "Saving checkpoint for epoch 2 at nmt\\checkpoints\\4layers_128d_8heads_512dff_20train_perc\\ckpt-83\n",
      "Epoch 2 Loss 0.6379 Accuracy 0.4971\n",
      "Time taken for 1 epoch: 542.9662888050079 secs\n",
      "\n",
      "Saving checkpoint for epoch 3 at nmt\\checkpoints\\4layers_128d_8heads_512dff_20train_perc\\ckpt-84\n",
      "Epoch 3 Loss 0.6317 Accuracy 0.4982\n",
      "Time taken for 1 epoch: 558.0755507946014 secs\n",
      "\n",
      "Saving checkpoint for epoch 4 at nmt\\checkpoints\\4layers_128d_8heads_512dff_20train_perc\\ckpt-85\n",
      "Epoch 4 Loss 0.6294 Accuracy 0.4989\n",
      "Time taken for 1 epoch: 557.2021055221558 secs\n",
      "\n",
      "Saving checkpoint for epoch 5 at nmt\\checkpoints\\4layers_128d_8heads_512dff_20train_perc\\ckpt-86\n",
      "Epoch 5 Loss 0.6244 Accuracy 0.4994\n",
      "Time taken for 1 epoch: 565.8318014144897 secs\n",
      "\n",
      "Saving checkpoint for epoch 6 at nmt\\checkpoints\\4layers_128d_8heads_512dff_20train_perc\\ckpt-87\n",
      "Epoch 6 Loss 0.6213 Accuracy 0.4999\n",
      "Time taken for 1 epoch: 581.837304353714 secs\n",
      "\n",
      "Saving checkpoint for epoch 7 at nmt\\checkpoints\\4layers_128d_8heads_512dff_20train_perc\\ckpt-88\n",
      "Epoch 7 Loss 0.6179 Accuracy 0.5006\n",
      "Time taken for 1 epoch: 597.5467672348022 secs\n",
      "\n",
      "Saving checkpoint for epoch 8 at nmt\\checkpoints\\4layers_128d_8heads_512dff_20train_perc\\ckpt-89\n",
      "Epoch 8 Loss 0.6135 Accuracy 0.5023\n",
      "Time taken for 1 epoch: 597.5941359996796 secs\n",
      "\n",
      "Saving checkpoint for epoch 9 at nmt\\checkpoints\\4layers_128d_8heads_512dff_20train_perc\\ckpt-90\n",
      "Epoch 9 Loss 0.6098 Accuracy 0.5029\n",
      "Time taken for 1 epoch: 591.1441025733948 secs\n",
      "\n",
      "Saving checkpoint for epoch 10 at nmt\\checkpoints\\4layers_128d_8heads_512dff_20train_perc\\ckpt-91\n",
      "Epoch 10 Loss 0.6072 Accuracy 0.5029\n",
      "Time taken for 1 epoch: 578.5670299530029 secs\n",
      "\n",
      "Saving checkpoint for epoch 11 at nmt\\checkpoints\\4layers_128d_8heads_512dff_20train_perc\\ckpt-92\n",
      "Epoch 11 Loss 0.6032 Accuracy 0.5035\n",
      "Time taken for 1 epoch: 534.515017747879 secs\n",
      "\n",
      "Saving checkpoint for epoch 12 at nmt\\checkpoints\\4layers_128d_8heads_512dff_20train_perc\\ckpt-93\n",
      "Epoch 12 Loss 0.6001 Accuracy 0.5043\n",
      "Time taken for 1 epoch: 520.2795543670654 secs\n",
      "\n",
      "Saving checkpoint for epoch 13 at nmt\\checkpoints\\4layers_128d_8heads_512dff_20train_perc\\ckpt-94\n",
      "Epoch 13 Loss 0.5972 Accuracy 0.5046\n",
      "Time taken for 1 epoch: 516.9343898296356 secs\n",
      "\n",
      "Saving checkpoint for epoch 14 at nmt\\checkpoints\\4layers_128d_8heads_512dff_20train_perc\\ckpt-95\n",
      "Epoch 14 Loss 0.5935 Accuracy 0.5060\n",
      "Time taken for 1 epoch: 512.2538754940033 secs\n",
      "\n",
      "Saving checkpoint for epoch 15 at nmt\\checkpoints\\4layers_128d_8heads_512dff_20train_perc\\ckpt-96\n",
      "Epoch 15 Loss 0.5911 Accuracy 0.5062\n",
      "Time taken for 1 epoch: 508.87002301216125 secs\n",
      "\n",
      "Saving checkpoint for epoch 16 at nmt\\checkpoints\\4layers_128d_8heads_512dff_20train_perc\\ckpt-97\n",
      "Epoch 16 Loss 0.5875 Accuracy 0.5071\n",
      "Time taken for 1 epoch: 497.962867975235 secs\n",
      "\n",
      "Saving checkpoint for epoch 17 at nmt\\checkpoints\\4layers_128d_8heads_512dff_20train_perc\\ckpt-98\n",
      "Epoch 17 Loss 0.5856 Accuracy 0.5071\n",
      "Time taken for 1 epoch: 549.788143157959 secs\n",
      "\n",
      "Saving checkpoint for epoch 18 at nmt\\checkpoints\\4layers_128d_8heads_512dff_20train_perc\\ckpt-99\n",
      "Epoch 18 Loss 0.5812 Accuracy 0.5081\n",
      "Time taken for 1 epoch: 562.9428415298462 secs\n",
      "\n",
      "Saving checkpoint for epoch 19 at nmt\\checkpoints\\4layers_128d_8heads_512dff_20train_perc\\ckpt-100\n",
      "Epoch 19 Loss 0.5794 Accuracy 0.5084\n",
      "Time taken for 1 epoch: 580.0667653083801 secs\n",
      "\n",
      "Saving checkpoint for epoch 20 at nmt\\checkpoints\\4layers_128d_8heads_512dff_20train_perc\\ckpt-101\n",
      "Epoch 20 Loss 0.5762 Accuracy 0.5092\n",
      "Time taken for 1 epoch: 572.4685475826263 secs\n",
      "\n",
      "Saving checkpoint for epoch 21 at nmt\\checkpoints\\4layers_128d_8heads_512dff_20train_perc\\ckpt-102\n",
      "Epoch 21 Loss 0.5738 Accuracy 0.5096\n",
      "Time taken for 1 epoch: 572.9128725528717 secs\n",
      "\n",
      "Saving checkpoint for epoch 22 at nmt\\checkpoints\\4layers_128d_8heads_512dff_20train_perc\\ckpt-103\n",
      "Epoch 22 Loss 0.5714 Accuracy 0.5104\n",
      "Time taken for 1 epoch: 576.2663559913635 secs\n",
      "\n",
      "Saving checkpoint for epoch 23 at nmt\\checkpoints\\4layers_128d_8heads_512dff_20train_perc\\ckpt-104\n",
      "Epoch 23 Loss 0.5688 Accuracy 0.5104\n",
      "Time taken for 1 epoch: 567.9275867938995 secs\n",
      "\n",
      "Saving checkpoint for epoch 24 at nmt\\checkpoints\\4layers_128d_8heads_512dff_20train_perc\\ckpt-105\n",
      "Epoch 24 Loss 0.5647 Accuracy 0.5113\n",
      "Time taken for 1 epoch: 568.2100372314453 secs\n",
      "\n",
      "Saving checkpoint for epoch 25 at nmt\\checkpoints\\4layers_128d_8heads_512dff_20train_perc\\ckpt-106\n",
      "Epoch 25 Loss 0.5622 Accuracy 0.5118\n",
      "Time taken for 1 epoch: 557.6671347618103 secs\n",
      "\n",
      "Saving checkpoint for epoch 26 at nmt\\checkpoints\\4layers_128d_8heads_512dff_20train_perc\\ckpt-107\n",
      "Epoch 26 Loss 0.5605 Accuracy 0.5122\n",
      "Time taken for 1 epoch: 547.9945809841156 secs\n",
      "\n",
      "Saving checkpoint for epoch 27 at nmt\\checkpoints\\4layers_128d_8heads_512dff_20train_perc\\ckpt-108\n",
      "Epoch 27 Loss 0.5571 Accuracy 0.5128\n",
      "Time taken for 1 epoch: 525.0303177833557 secs\n",
      "\n",
      "Saving checkpoint for epoch 28 at nmt\\checkpoints\\4layers_128d_8heads_512dff_20train_perc\\ckpt-109\n",
      "Epoch 28 Loss 0.5546 Accuracy 0.5132\n",
      "Time taken for 1 epoch: 521.5731670856476 secs\n",
      "\n",
      "Saving checkpoint for epoch 29 at nmt\\checkpoints\\4layers_128d_8heads_512dff_20train_perc\\ckpt-110\n",
      "Epoch 29 Loss 0.5533 Accuracy 0.5134\n",
      "Time taken for 1 epoch: 519.9080538749695 secs\n",
      "\n",
      "Saving checkpoint for epoch 30 at nmt\\checkpoints\\4layers_128d_8heads_512dff_20train_perc\\ckpt-111\n",
      "Epoch 30 Loss 0.5504 Accuracy 0.5145\n",
      "Time taken for 1 epoch: 520.6402144432068 secs\n",
      "\n",
      "Saving checkpoint for epoch 31 at nmt\\checkpoints\\4layers_128d_8heads_512dff_20train_perc\\ckpt-112\n",
      "Epoch 31 Loss 0.5478 Accuracy 0.5147\n",
      "Time taken for 1 epoch: 511.9010262489319 secs\n",
      "\n",
      "Saving checkpoint for epoch 32 at nmt\\checkpoints\\4layers_128d_8heads_512dff_20train_perc\\ckpt-113\n",
      "Epoch 32 Loss 0.5460 Accuracy 0.5155\n",
      "Time taken for 1 epoch: 516.7470238208771 secs\n",
      "\n",
      "Saving checkpoint for epoch 33 at nmt\\checkpoints\\4layers_128d_8heads_512dff_20train_perc\\ckpt-114\n",
      "Epoch 33 Loss 0.5448 Accuracy 0.5153\n",
      "Time taken for 1 epoch: 506.47868967056274 secs\n",
      "\n",
      "Saving checkpoint for epoch 34 at nmt\\checkpoints\\4layers_128d_8heads_512dff_20train_perc\\ckpt-115\n",
      "Epoch 34 Loss 0.5417 Accuracy 0.5160\n",
      "Time taken for 1 epoch: 513.3499746322632 secs\n",
      "\n",
      "Saving checkpoint for epoch 35 at nmt\\checkpoints\\4layers_128d_8heads_512dff_20train_perc\\ckpt-116\n",
      "Epoch 35 Loss 0.5390 Accuracy 0.5165\n",
      "Time taken for 1 epoch: 527.5496146678925 secs\n",
      "\n",
      "Saving checkpoint for epoch 36 at nmt\\checkpoints\\4layers_128d_8heads_512dff_20train_perc\\ckpt-117\n",
      "Epoch 36 Loss 0.5369 Accuracy 0.5172\n",
      "Time taken for 1 epoch: 526.9078667163849 secs\n",
      "\n",
      "Saving checkpoint for epoch 37 at nmt\\checkpoints\\4layers_128d_8heads_512dff_20train_perc\\ckpt-118\n",
      "Epoch 37 Loss 0.5344 Accuracy 0.5177\n",
      "Time taken for 1 epoch: 521.3848025798798 secs\n",
      "\n",
      "Saving checkpoint for epoch 38 at nmt\\checkpoints\\4layers_128d_8heads_512dff_20train_perc\\ckpt-119\n",
      "Epoch 38 Loss 0.5311 Accuracy 0.5184\n",
      "Time taken for 1 epoch: 551.1381568908691 secs\n",
      "\n",
      "Saving checkpoint for epoch 39 at nmt\\checkpoints\\4layers_128d_8heads_512dff_20train_perc\\ckpt-120\n",
      "Epoch 39 Loss 0.5277 Accuracy 0.5194\n",
      "Time taken for 1 epoch: 562.349057674408 secs\n",
      "\n",
      "Saving checkpoint for epoch 40 at nmt\\checkpoints\\4layers_128d_8heads_512dff_20train_perc\\ckpt-121\n",
      "Epoch 40 Loss 0.5280 Accuracy 0.5188\n",
      "Time taken for 1 epoch: 569.2324764728546 secs\n",
      "\n",
      "Saving checkpoint for epoch 41 at nmt\\checkpoints\\4layers_128d_8heads_512dff_20train_perc\\ckpt-122\n",
      "Epoch 41 Loss 0.5242 Accuracy 0.5200\n",
      "Time taken for 1 epoch: 529.7696323394775 secs\n",
      "\n",
      "Saving checkpoint for epoch 42 at nmt\\checkpoints\\4layers_128d_8heads_512dff_20train_perc\\ckpt-123\n",
      "Epoch 42 Loss 0.5233 Accuracy 0.5200\n",
      "Time taken for 1 epoch: 510.1077344417572 secs\n",
      "\n",
      "Saving checkpoint for epoch 43 at nmt\\checkpoints\\4layers_128d_8heads_512dff_20train_perc\\ckpt-124\n",
      "Epoch 43 Loss 0.5218 Accuracy 0.5203\n",
      "Time taken for 1 epoch: 519.3099150657654 secs\n",
      "\n",
      "Saving checkpoint for epoch 44 at nmt\\checkpoints\\4layers_128d_8heads_512dff_20train_perc\\ckpt-125\n",
      "Epoch 44 Loss 0.5179 Accuracy 0.5213\n",
      "Time taken for 1 epoch: 515.6606500148773 secs\n",
      "\n",
      "Saving checkpoint for epoch 45 at nmt\\checkpoints\\4layers_128d_8heads_512dff_20train_perc\\ckpt-126\n",
      "Epoch 45 Loss 0.5169 Accuracy 0.5215\n",
      "Time taken for 1 epoch: 510.85252618789673 secs\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving checkpoint for epoch 46 at nmt\\checkpoints\\4layers_128d_8heads_512dff_20train_perc\\ckpt-127\n",
      "Epoch 46 Loss 0.5159 Accuracy 0.5216\n",
      "Time taken for 1 epoch: 512.1797797679901 secs\n",
      "\n",
      "Saving checkpoint for epoch 47 at nmt\\checkpoints\\4layers_128d_8heads_512dff_20train_perc\\ckpt-128\n",
      "Epoch 47 Loss 0.5131 Accuracy 0.5222\n",
      "Time taken for 1 epoch: 527.4822580814362 secs\n",
      "\n",
      "Saving checkpoint for epoch 48 at nmt\\checkpoints\\4layers_128d_8heads_512dff_20train_perc\\ckpt-129\n",
      "Epoch 48 Loss 0.5117 Accuracy 0.5223\n",
      "Time taken for 1 epoch: 555.3475494384766 secs\n",
      "\n",
      "Saving checkpoint for epoch 49 at nmt\\checkpoints\\4layers_128d_8heads_512dff_20train_perc\\ckpt-130\n",
      "Epoch 49 Loss 0.5097 Accuracy 0.5227\n",
      "Time taken for 1 epoch: 578.9111795425415 secs\n",
      "\n",
      "Saving checkpoint for epoch 50 at nmt\\checkpoints\\4layers_128d_8heads_512dff_20train_perc\\ckpt-131\n",
      "Epoch 50 Loss 0.5083 Accuracy 0.5232\n",
      "Time taken for 1 epoch: 579.1058614253998 secs\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 定義我們要看幾遍數據集\n",
    "EPOCHS = 50\n",
    "print(f\"此超參數組合的 Transformer 已經訓練 {last_epoch} epochs。\")\n",
    "print(f\"剩餘 epochs：{min(0, last_epoch - EPOCHS)}\")\n",
    "\n",
    "\n",
    "# 用來寫資訊到 TensorBoard，非必要但十分推薦\n",
    "summary_writer = tf.summary.create_file_writer(log_dir)\n",
    "\n",
    "# 比對設定的 `EPOCHS` 以及已訓練的 `last_epoch` 來決定還要訓練多少 epochs\n",
    "for epoch in range(last_epoch, EPOCHS):\n",
    "    start = time.time()\n",
    "\n",
    "    # 重置紀錄 TensorBoard 的 metrics\n",
    "    train_loss.reset_states()\n",
    "    train_accuracy.reset_states()\n",
    "\n",
    "    # 一個 epoch 就是把我們定義的訓練資料集一個一個 batch 拿出來處理，直到看完整個數據集 \n",
    "    for (step_idx, (inp, tar)) in enumerate(train_dataset):\n",
    "    \n",
    "        # 每次 step 就是將數據丟入 Transformer，讓它生預測結果並計算梯度最小化 loss\n",
    "        train_step(inp, tar)  \n",
    "\n",
    "    # 每個 epoch 完成就存一次檔    \n",
    "    if (epoch + 1) % 1 == 0:\n",
    "        ckpt_save_path = ckpt_manager.save()\n",
    "        print ('Saving checkpoint for epoch {} at {}'.format(epoch+1,ckpt_save_path))\n",
    "    \n",
    "    \n",
    "    # 將 loss 以及 accuracy 寫到 TensorBoard 上\n",
    "    with summary_writer.as_default():\n",
    "        tf.summary.scalar(\"train_loss\", train_loss.result(), step=epoch + 1)\n",
    "        tf.summary.scalar(\"train_acc\", train_accuracy.result(), step=epoch + 1)\n",
    "  \n",
    "    print('Epoch {} Loss {:.4f} Accuracy {:.4f}'.format(epoch + 1, \n",
    "                                                train_loss.result(), \n",
    "                                                train_accuracy.result()))\n",
    "    print('Time taken for 1 epoch: {} secs\\n'.format(time.time() - start))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 給定一個英文句子，輸出預測的中文索引數字序列以及注意權重 dict\n",
    "def evaluate(inp_sentence):\n",
    "  \n",
    "    # 準備英文句子前後會加上的 <start>, <end>\n",
    "    start_token = [subword_encoder_en.vocab_size]\n",
    "    end_token = [subword_encoder_en.vocab_size + 1]\n",
    "\n",
    "    # inp_sentence 是字串，我們用 Subword Tokenizer 將其變成子詞的索引序列\n",
    "    # 並在前後加上 BOS / EOS\n",
    "    inp_sentence = start_token + subword_encoder_en.encode(inp_sentence) + end_token\n",
    "    encoder_input = tf.expand_dims(inp_sentence, 0)\n",
    "\n",
    "    # 跟我們在影片裡看到的一樣，Decoder 在第一個時間點吃進去的輸入\n",
    "    # 是一個只包含一個中文 <start> token 的序列\n",
    "    decoder_input = [subword_encoder_zh.vocab_size]\n",
    "    output = tf.expand_dims(decoder_input, 0)  # 增加 batch 維度\n",
    "\n",
    "    # auto-regressive，一次生成一個中文字並將預測加到輸入再度餵進 Transformer\n",
    "    for i in range(MAX_LENGTH):\n",
    "        # 每多一個生成的字就得產生新的遮罩\n",
    "        enc_padding_mask, combined_mask, dec_padding_mask = create_masks(\n",
    "            encoder_input, output)\n",
    "  \n",
    "        # predictions.shape == (batch_size, seq_len, vocab_size)\n",
    "        predictions, attention_weights = transformer(encoder_input, \n",
    "                                                     output,\n",
    "                                                     False,\n",
    "                                                     enc_padding_mask,\n",
    "                                                     combined_mask,\n",
    "                                                     dec_padding_mask)\n",
    "        \n",
    "        \n",
    "        # 將序列中最後一個 distribution 取出，並將裡頭值最大的當作模型最新的預測字\n",
    "        predictions = predictions[: , -1:, :]  # (batch_size, 1, vocab_size)\n",
    "\n",
    "        predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
    "\n",
    "        # 遇到 <end> token 就停止回傳，代表模型已經產生完結果\n",
    "        if tf.equal(predicted_id, subword_encoder_zh.vocab_size + 1):\n",
    "              return tf.squeeze(output, axis=0), attention_weights\n",
    "\n",
    "        #將 Transformer 新預測的中文索引加到輸出序列中，讓 Decoder 可以在產生\n",
    "        # 下個中文字的時候關注到最新的 `predicted_id`\n",
    "        output = tf.concat([output, predicted_id], axis=-1)  \n",
    "        \n",
    "    # 將 batch 的維度去掉後回傳預測的中文索引序列\n",
    "    return tf.squeeze(output, axis=0), attention_weights        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence: China, India, and others have enjoyed continuing economic growth.\n",
      "predicted_sentence: 中國、印度和其他國家享受了經濟增長。\n"
     ]
    }
   ],
   "source": [
    "from opencc import OpenCC\n",
    "cc = OpenCC('s2t')\n",
    "# 要被翻譯的英文句子\n",
    "sentence = \"China, India, and others have enjoyed continuing economic growth.\"\n",
    "\n",
    "# 取得預測的中文索引序列\n",
    "predicted_seq, _ = evaluate(sentence)\n",
    "\n",
    "# 過濾掉 <start> & <end> tokens 並用中文的 subword tokenizer 幫我們將索引序列還原回中文句子\n",
    "target_vocab_size = subword_encoder_zh.vocab_size\n",
    "predicted_seq_without_bos_eos = [idx for idx in predicted_seq if idx < target_vocab_size]\n",
    "predicted_sentence = subword_encoder_zh.decode(predicted_seq_without_bos_eos)\n",
    "\n",
    "print(\"sentence:\", sentence)\n",
    "print(\"predicted_sentence:\", cc.convert(predicted_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence: Slowly and not without struggle, America began to listen.\n",
      "predicted_sentence: 沒有緩慢而不是，美國還在長期以來又開始聆聽。\n",
      "------------------------------\n",
      "sentence: I didn't own a Thesaurus until four years ago and I use a small Webster's dictionary that I'd bought at K-Mart for 89 cents.\n",
      "predicted_sentence: 我在第四年前並沒有擁有自己的那些叫停KP的小牛津，我並在父母上安排了。\n",
      "------------------------------\n",
      "sentence: portlet, you must write three short deployment descriptors: web.xml, portlet.xml, and geronimo-web.xml. (Some of these may have been generated by your IDE.)\n",
      "predicted_sentence: 你們必須寫三個泡沫，用純屬廉價的卡賣蟲：食用電池。 ）\n",
      "------------------------------\n",
      "sentence: Dithering is a technique that blends your colors together, making them look smoother, or just creating interesting textures.\n",
      "predicted_sentence: 診斷是你的技術和金融裝活動常常常常專注地，並把握自科學帶一個干擾甚至死。\n",
      "------------------------------\n",
      "sentence: This paper discusses the petrologic characteristics of the coal-bearing strata under the geologic structural background of the Tertiary coal basin in Hunchun.\n",
      "predicted_sentence: 這一文章發現了特徵性研究對煤炭的結構性中間產生了一清晰的結構性錯誤。\n",
      "------------------------------\n",
      "sentence: Women over 55 are pickier about their partners than at any other time in their lives.\n",
      "predicted_sentence: 555名女性在他們身業夥伴中的作用越大。\n",
      "------------------------------\n",
      "sentence: Ruben: So, to heal (with capital letters) you need to have no predilections.\n",
      "predicted_sentence: 總的：解決之道（和資本 ） ， 也都沒有人需要信心的行爲。\n",
      "------------------------------\n",
      "sentence: The second encounter relates to my grandfather's treasure box.\n",
      "predicted_sentence: 第二個輿論遭到了祖父子的恐懼。\n",
      "------------------------------\n",
      "sentence: Change the value for the <ejb-link> tag to MyEJB, which is the name of the EJB as defined in the ejb-jar.xml file.\n",
      "predicted_sentence: 畢竟，那些博茨基所稱的戀吉亞州總稱呼籲提出MRCDR的定價。\n",
      "------------------------------\n",
      "sentence: One way to address these challenges would be to establish a Truth and Reconciliation Commission modeled on the experience of Muggle South Africa.\n",
      "predicted_sentence: 解決這些挑戰之一是建立三個挑戰，並指出歐盟委員會建立安全的經驗。\n",
      "------------------------------\n",
      "sentence: Brain: If you don't mind, Jonathan, while you and Mr. Sun get acquainted, I'd like to check the arrangements for the meeting.\n",
      "predicted_sentence: 調查你不是：你想象，也就像是己孩子們一樣，誘惑巧防網上和努力打寫道。\n",
      "------------------------------\n",
      "sentence: Bailee Madison plays Sally, a young girl who goes to live with her father and his girlfriend.\n",
      "predicted_sentence: 無錢在禁止薩爾科齊採取薩科齊，一個孩子很多女孩子招募和友善人的年輕人。\n",
      "------------------------------\n",
      "sentence: Reduce blood fat, prevent thrombosis, arteriosclerosis, apoplexy and heart disease. Improve memory, nourish the brain and improve the intelligence.\n",
      "predicted_sentence: 破壞流血、僵化的努力防止創新先驅動、藝術和質量等腦海外。\n",
      "------------------------------\n",
      "sentence: Toomay said signs of community intolerance, including bumper stickers opposing same-sex marriage, also made him feel down, and he sought guidance from a school counselor after contemplating suicide.\n",
      "predicted_sentence: 社區的表示反對象都被視爲性別反複製性、激進和兒童自殺後。\n",
      "------------------------------\n",
      "sentence: When you eat dinner out, reduce the temptation to clean your plate by setting aside one-third of your meal.\n",
      "predicted_sentence: 當你的滅絕下降時，降低了自己設計的指控，將本國兒童佔總量指責。\n",
      "------------------------------\n",
      "sentence: Sang Lan is one of the best athletes in our country.\n",
      "predicted_sentence: 桑德斯在我們的運動之一是我們的最佳電。\n",
      "------------------------------\n",
      "sentence: They are able to show that active peroxiredoxin 1, Prx1, an enzyme that breaks down harmful hydrogen peroxide in the cells, is required for caloric restriction to work effectively.\n",
      "predicted_sentence: 它們能夠表明，治療童的正統性（Puxuson）限制了有效地消息的一個細胞。\n",
      "------------------------------\n",
      "sentence: He went to slide upon the ice Before the ice would bear; Then he plunged in above his knees, Which made poor Simon stare.\n",
      "predicted_sentence: 在冰蓋面前，居家們就將向冰蓋面；謀生一個居國，從而受到貧窮猛進了翻戴街頭。\n",
      "------------------------------\n",
      "sentence: Miaoxiang son know the devil tricks, white three quick deployment.\n",
      "predicted_sentence: 棉蘭教院警告訴他們，有三顆白宮防部署了快速軍事停滯。\n",
      "------------------------------\n",
      "sentence: “My daughter has been banned from watching the show, ” supermodel Cindy Crawford told ShowbizSpy.\n",
      "predicted_sentence: “我的女兒會被看到，大肆意察PPP的超級別緩衝擊所有的庫爾德人。\n",
      "------------------------------\n",
      "sentence: \"The Chinese Super League starts in a couple of weeks and Sheffield United have asked me to go over and have a look at the coaching set-up, \" said McKinna.\n",
      "predicted_sentence: \"1國內的文明星部開始，還有一週年精明星部還要求美國國國內的審護。\n",
      "------------------------------\n",
      "sentence: On the fifteenth day of that month the Lord 's Feast of Unleavened Bread begins; for seven days you must eat bread made without yeast.\n",
      "predicted_sentence: 第五天，德·馬謝瓦兒童逐漸降臨時，就必須小兒童天真，至七個月底重小。\n",
      "------------------------------\n",
      "sentence: This paper consists of 3 parts:the monitorial system, the pupil-teacher system, and the historic role of the monitorial system & the pupil-teacher system.\n",
      "predicted_sentence: 這一文層工作者在監督教師系統、肺結果和監控系統的典型研究。\n",
      "------------------------------\n",
      "sentence: Further development of the central cell mainly involved changes in the orientation of the polar nuclei and the distribution of the cytoplasm.\n",
      "predicted_sentence: 未來中央神聖通牒博弈，目的發射和分析業務之間的變化與右翼居高度。\n",
      "------------------------------\n",
      "sentence: The first parameter to the script is saved to a variable called $IP.\n",
      "predicted_sentence: 第一條關於綁架的條件是儲蓄的所謂的可比性變化。\n",
      "------------------------------\n",
      "sentence: Effective microbicides mean women can act independently to protect themselves against HIV/AIDS.\n",
      "predicted_sentence: 效果導致細於女性獨立的行動可以獨立於檢查自身艾滋病毒/艾滋病的保護。\n",
      "------------------------------\n",
      "sentence: 当我把消息告诉她时，她简直目瞪口呆。\n",
      "predicted_sentence: 民族滅亡公約爲獨裁者呼籲公約、加壓壓壓壓迫性公約、宣稱推廣支持度推廣支行魯塞爾。\n",
      "------------------------------\n",
      "sentence: KEY LARGO, Fla. - The newborn calf of a deaf bottlenose dolphin that was found stranded last fall off a Florida beach died Friday at a marine mammal rehabilitation center.\n",
      "predicted_sentence: 東京—在弗羅德波索四個星期一出生，新聞噴子已經被提供。\n",
      "------------------------------\n",
      "sentence: Despite some real scandals at Fannie and Freddie, they played little role in causing the crisis: most of the really bad lending came from private loan originators.\n",
      "predicted_sentence: 儘管有些醜聞醜聞人和法國人都在危機中發揮作用：大部分借貸者的真正貸款權。\n",
      "------------------------------\n",
      "sentence: His time of 13 seconds is his best result since his injury four years ago.\n",
      "predicted_sentence: 他的13歲以來最好的時候是他善意的，四周前就是第四年前的不公平。\n",
      "------------------------------\n",
      "sentence: They walked in a stooped posture, the shoulders well forward, the head still farther forward, the eyes bent upon the ground.\n",
      "predicted_sentence: 在一個症候時期，它們的注意力最近也從眼中開始，清晰浴室也就清醒了。\n",
      "------------------------------\n",
      "sentence: In the East, Manichaeism survived until the 13th century.\n",
      "predicted_sentence: 在13世紀以前，一波離開了死衚衕。\n",
      "------------------------------\n",
      "sentence: I am clearly warning against that, because what you get is liable to be mediocrity.\n",
      "predicted_sentence: 我很顯然警告此警告，那就是因爲應該低了總統的。\n",
      "------------------------------\n",
      "sentence: He could see I meant what I said. So he took his fur coat and left.\n",
      "predicted_sentence: 他可以認爲我在左翼和左翼獲得的情況下曾經人所言出的了。\n",
      "------------------------------\n",
      "sentence: But the main problem hasn't been solved which emphasizes on the system of curriculum and the integrity of the teaching content.\n",
      "predicted_sentence: 但主要問題並沒有解決武裝制統治理問題的強調並完整根據內容。\n",
      "------------------------------\n",
      "sentence: Mr Blake:This is my son, Timmy.\n",
      "predicted_sentence: 布拉格特·布魯納，時間的時候是我的。\n",
      "------------------------------\n",
      "sentence: Few people studying Gauge Field Theory need to be convinced of the importance of the work of 't Hooft.\n",
      "predicted_sentence: 沒有人研究難民研究人員研究，沒有決定他們的重要工作者的重要性。\n",
      "------------------------------\n",
      "sentence: Shanghai is a varicolored world.\n",
      "predicted_sentence: 上海是氯喹的發展趨勢。\n",
      "------------------------------\n",
      "sentence: Using this in concert with the exec() command and dumping the results to an array allows you to build an HTML table or form that then allows you to run other commands.\n",
      "predicted_sentence: 感覺歡宣稱是在外交官簽證訴還指揮官，施壓將結果引發列-HTMt-卡病毒。\n",
      "------------------------------\n",
      "sentence: As a result, the equivalent model of contactless smart card and the interrogator was accomplished, which was verified by the simulation of Hspice software.\n",
      "predicted_sentence: 因此，此，相當於黃金和軟件相當濃縮，並且只是活動本國的冷漠化。\n",
      "------------------------------\n",
      "sentence: Because, by all accounts, that would be fostering a climate of lying and dishonesty.\n",
      "predicted_sentence: 任何構成，由於所有的原因都可以促進一位氣候變化和誠實的氣候問題。\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence: Therefore, laminoplasty with its ability to address multiple levels, and limited short- and long-term morbidity, despite neck pain, is the author's procedure of choice.\n",
      "predicted_sentence: 因此，拉雅評估多層的少數層能夠通過多管、有限的測試驗和長期作用。\n",
      "------------------------------\n",
      "sentence: It will actively foster volunteer teams.\n",
      "predicted_sentence: 它將積極地致力於志願者隊伍。\n",
      "------------------------------\n",
      "sentence: Frank Miller's dispiriting take on The Spirit a couple of years ago could easily have laid the subgenre to rest for a generation.\n",
      "predicted_sentence: 弗拉克父母親在封閉之前很可能很快將快將折磨一段落爲其他人。\n",
      "------------------------------\n",
      "sentence: E. g. Dalian is one of the most beautiful cities in China.\n",
      "predicted_sentence: 那麼，達到了最多的中國手段，其中大多數都是一座城市的城市。\n",
      "------------------------------\n",
      "sentence: Truth, good and beauty have always been considered as the three top pursuits of human beings.\n",
      "predicted_sentence: 真正的是好的美軍和奧林肯定總是以人類爲目標付出的三個追求。\n",
      "------------------------------\n",
      "sentence: Li Shizhen own invention, he is the main drug observe, study and practical application of the new discovery, new experiences, thus greatly enriched and improved the knowledge of herbal medicine.\n",
      "predicted_sentence: 他寫入自杭州、研究和服用於新發現的研究都是研究的主要研究、實驗促進的教育。\n",
      "------------------------------\n",
      "sentence: Even though vitamin C-rich foods are probably the first thing you think of when you feel a cold coming, the illness-preventing power of the antioxidant is debatable.\n",
      "predicted_sentence: 雖然汽車稍有着街頭的死亡，但你們也許首先認爲你會在一個欺騙網上進行獨善情。\n",
      "------------------------------\n",
      "sentence: Further perfecting and developing shareholding system economy is still disrupted by the problem, that shareholding system belongs to capitalism or socialism, in people's ideology.\n",
      "predicted_sentence: 未來完美有完美的政府和發展業體系仍然是經濟體，因爲人民主義者和社會主義的主義依賴\n",
      "------------------------------\n",
      "sentence: At last, the design method of widened pavement was proposed with considering tension strength as a controlling index.\n",
      "predicted_sentence: 此外，還算計了一條截止森林規範，並且擴大了一條截然不斷加強限的爭限。\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "file = open('translation2019zh_valid.json', 'r', encoding='utf-8') \n",
    "datas = []\n",
    "for line in file.readlines():\n",
    "    dic = json.loads(line)\n",
    "    datas.append(dic)\n",
    "    \n",
    "for index,data in enumerate(datas):\n",
    "    if index<50 :\n",
    "        # 要被翻譯的英文句子\n",
    "        sentence = data['english']\n",
    "\n",
    "        # 取得預測的中文索引序列\n",
    "        predicted_seq, _ = evaluate(sentence)\n",
    "\n",
    "        # 過濾掉 <start> & <end> tokens 並用中文的 subword tokenizer 幫我們將索引序列還原回中文句子\n",
    "        target_vocab_size = subword_encoder_zh.vocab_size\n",
    "        predicted_seq_without_bos_eos = [idx for idx in predicted_seq if idx < target_vocab_size]\n",
    "        predicted_sentence = subword_encoder_zh.decode(predicted_seq_without_bos_eos)\n",
    "\n",
    "        print(\"sentence:\", sentence)\n",
    "        print(\"predicted_sentence:\", cc.convert(predicted_sentence))\n",
    "        print(\"-\"*30)\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
